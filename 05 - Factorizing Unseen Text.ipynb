{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone: Philosophical Factors for NLP\n",
    "**_Measuring Similarity to Philosophical Concepts in Text Data_**\n",
    "\n",
    "## Thomas W. Ludlow, Jr.\n",
    "**General Assembly Data Science Immersive DSI-NY-6**\n",
    "\n",
    "**February 12, 2019**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5 - Factorizing Unseen Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**5.1 Prepare Models for Prediction**](#5.1-Prepare-Models-for-Prediction)\n",
    "\n",
    "[**5.2 Factorizing**](#5.2-Factorizing)\n",
    "- [5.2.1 Preprocess Unseen Text for Factorizing](#5.2.1-Preprocess-Unseen-Text-for-Factorizing)\n",
    "- [5.2.2 Factorize Text](#5.2.2-Factorize-Text)\n",
    "- [5.2.3 Display Results](#5.2.3-Display-Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Python Data Science\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Natural Language Processing\n",
    "import spacy\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel, ldamulticore, CoherenceModel\n",
    "\n",
    "# Modeling Prep\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Neural Net\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Override deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Prepare Models and Input Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "rnn = keras.models.load_model('../models/rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = joblib.load('../models/logreg')\n",
    "# mnb = joblib.load('./models/mnb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../data_eda/sw.pkl','rb')\n",
    "sw = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using medium English library which does not include vectors\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_s_file = open('../models/d2v_s.pkl','rb')\n",
    "d2v_s = pickle.load(d2v_s_file)\n",
    "d2v_s_file.close()\n",
    "\n",
    "d2v_p_file = open('../models/d2v_p.pkl','rb')\n",
    "d2v_p = pickle.load(d2v_p_file)\n",
    "d2v_p_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_multi_s = Dictionary.load('../models/lda_multi_s')\n",
    "lda_multi_p = Dictionary.load('../models/lda_multi_p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 New Text Preprocessing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert Block String to List of Paragraphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = \"\"\"\n",
    "Why give a robot an order to obey orders—why aren't the original orders enough? \n",
    "Why command a robot not to do harm—wouldn't it be easier never to command it to \n",
    "do harm in the first place? Does the universe contain a mysterious force pulling \n",
    "entities toward malevolence, so that a positronic brain must be programmed to \n",
    "withstand it? Do intelligent beings inevitably develop an attitude problem?\n",
    "\n",
    "Now that computers really have become smarter and more powerful, the anxiety has \n",
    "waned. Today's ubiquitous, networked computers have an unprecedented ability to \n",
    "do mischief should they ever go to the bad. But the only mayhem comes from \n",
    "unpredictable chaos or from human malice in the form of viruses. We no longer \n",
    "worry about electronic serial killers or subversive silicon cabals because we \n",
    "are beginning to appreciate that malevolence—like vision, motor coordination, \n",
    "and common sense—does not come free with computation but has to be programmed in.\n",
    "\n",
    "Aggression, like every other part of human behavior we take for granted, is a \n",
    "challenging engineering problem!\n",
    "\n",
    "Steven Pinker\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWhy give a robot an order to obey orders—why aren't the original orders enough? \\nWhy command a robot not to do harm—wouldn't it be easier never to command it to \\ndo harm in the first place? Does the universe contain a mysterious force pulling \\nentities toward malevolence, so that a positronic brain must be programmed to \\nwithstand it? Do intelligent beings inevitably develop an attitude problem?\\n\\nNow that computers really have become smarter and more powerful, the anxiety has \\nwaned. Today's ubiquitous, networked computers have an unprecedented ability to \\ndo mischief should they ever go to the bad. But the only mayhem comes from \\nunpredictable chaos or from human malice in the form of viruses. We no longer \\nworry about electronic serial killers or subversive silicon cabals because we \\nare beginning to appreciate that malevolence—like vision, motor coordination, \\nand common sense—does not come free with computation but has to be programmed in.\\n\\nAggression, like every other part of human behavior we take for granted, is a \\nchallenging engineering problem!\\n\\nSteven Pinker\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_list = quote.strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def par_list(text_list, min_lines=0):\n",
    "    pars = []\n",
    "    count = 0\n",
    "    \n",
    "    # Check the longest single line to determine threshold for end of paragraph lines\n",
    "    line_lengths = [len(text_list[t].strip()) for t in range(len(text_list))]\n",
    "    line_check_length = max(line_lengths)\n",
    "    \n",
    "    for i, line in enumerate(text_list):\n",
    "        # If it reaches a blank line after too few lines, reset count\n",
    "        if line == '\\n' and count < min_lines: \n",
    "            count = 0\n",
    "        \n",
    "        # If it reaches the end of a paragraph after too few lines, reset count\n",
    "        elif len(line.strip()) < (line_check_length * .67) and count < min_lines:\n",
    "            count = 0\n",
    "            \n",
    "        # If it reaches a blank line after enough lines, save paragraph and reset count\n",
    "        elif line == '\\n' and count >= min_lines:\n",
    "            loop_par = ''\n",
    "            for j in range(count+1):\n",
    "                loop_par += text_list[(i-count)+j].replace('\\n','').strip() + ' '\n",
    "            pars.append(loop_par[:-1])\n",
    "            count = 0\n",
    "        \n",
    "        # If it sees the end of a paragraph after enough lines, save paragraph and reset count\n",
    "        elif len(line.strip()) < (line_check_length * .67) and count >= min_lines:\n",
    "            loop_par = ''\n",
    "            for j in range(count+1):\n",
    "                loop_par += text_list[(i-count)+j].replace('\\n','').strip() + ' '\n",
    "            pars.append(loop_par[:-1])\n",
    "            count = 0\n",
    "            \n",
    "        # Otherwise increase count\n",
    "        else:\n",
    "            count += 1\n",
    "    while '' in pars: pars.remove('')\n",
    "    return pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_pars = par_list(quote_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Why give a robot an order to obey orders—why aren't the original orders enough? Why command a robot not to do harm—wouldn't it be easier never to command it to do harm in the first place? Does the universe contain a mysterious force pulling entities toward malevolence, so that a positronic brain must be programmed to withstand it? Do intelligent beings inevitably develop an attitude problem? \",\n",
       " \"Now that computers really have become smarter and more powerful, the anxiety has waned. Today's ubiquitous, networked computers have an unprecedented ability to do mischief should they ever go to the bad. But the only mayhem comes from unpredictable chaos or from human malice in the form of viruses. We no longer worry about electronic serial killers or subversive silicon cabals because we are beginning to appreciate that malevolence—like vision, motor coordination, and common sense—does not come free with computation but has to be programmed in. \",\n",
       " 'Aggression, like every other part of human behavior we take for granted, is a challenging engineering problem!',\n",
       " 'Steven Pinker']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_pars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build DataFrame from Paragraph List**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 673.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why give a robot an order to obey orders—why a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now that computers really have become smarter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aggression, like every other part of human beh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Steven Pinker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           paragraph\n",
       "0  Why give a robot an order to obey orders—why a...\n",
       "1  Now that computers really have become smarter ...\n",
       "2  Aggression, like every other part of human beh...\n",
       "3                                      Steven Pinker"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par_df = pd.DataFrame(columns=['paragraph'])\n",
    "\n",
    "for i, book in enumerate(tqdm(c_pars)):\n",
    "    #print(book)\n",
    "    #temp_df = pd.DataFrame(columns=['paragraph'])\n",
    "    #temp_df.paragraph = book\n",
    "    #par_df = par_df.append(temp_df)\n",
    "    \n",
    "    par_df.loc[i, 'paragraph'] = book\n",
    "\n",
    "# par_df.index = range(par_df.shape[0])\n",
    "par_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess from DataFrame with `paragraph` Series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_to_df(par_file, nlp=nlp, sw=['the','a','but','like','for'], to_stem=False):\n",
    "    # Run spaCy process on each paragraph and store docs in list\n",
    "    print('1/8: nlp of paragraphs...')\n",
    "    par_nlp = []\n",
    "    for par in tqdm(par_file.paragraph):\n",
    "        par_nlp.append(nlp(par))\n",
    "    \n",
    "    # Store paragraph lemma from spaCy docs\n",
    "    print('2/8: nlp lemmatizing, part-of-speech, stopwords...')\n",
    "    par_lemma = []\n",
    "    for par in tqdm(par_nlp):\n",
    "        par_lemma.append([token.lemma_ for token in par     # List comprehension\n",
    "                           if token.lemma_ != '-PRON-'           # Pronouns are excluded\n",
    "                           and token.pos_ != 'PUNCT'             # Punctuation is excluded\n",
    "                           and token.is_alpha                    # Numbers are excluded\n",
    "                           and not token.is_stop                 # Stop words are excluded\n",
    "                          and len(token.lemma_) > 1])\n",
    "    par_lemma = [[pl[i].lower() for i in range(len(pl))] for pl in par_lemma]\n",
    "    \n",
    "    # Stem lemma with NLTK PorterStemmer and remove stop words\n",
    "    print('3/8: additional stopwords...')\n",
    "    if to_stem: ps = PorterStemmer()\n",
    "    par_lemma_sw = []\n",
    "    for vec_list in tqdm(par_lemma):    \n",
    "        update_list = []\n",
    "        for token in vec_list:\n",
    "            if token in sw: continue\n",
    "            if to_stem: update_list.append(ps.stem(token))\n",
    "            else: update_list.append(token)\n",
    "        par_lemma_sw.append(update_list)\n",
    "    \n",
    "    # Run spaCy on each sentence doc: Text\n",
    "    print('4/8: saving sentence text...')\n",
    "    sent_text = []\n",
    "    for par in tqdm(par_nlp):\n",
    "        sent_list = []\n",
    "        for s in par.sents:\n",
    "            sent_list.append(s.text)\n",
    "        sent_text.append(sent_list)\n",
    "    \n",
    "    # Run spaCy on each sentence doc: NLP\n",
    "    print('5/8: nlp of sentences...')\n",
    "    sent_nlp = []\n",
    "    for par in tqdm(par_nlp):\n",
    "        sent_list = []\n",
    "        for s in par.sents:\n",
    "            sent_list.append(nlp(s.text))\n",
    "        sent_nlp.append(sent_list)\n",
    "    \n",
    "    # Store lemma from spaCy docs\n",
    "    print('6/8: nlp lemmatizing, part-of-speech, stopwords...')\n",
    "    sent_lemma = []\n",
    "    for par in tqdm(sent_nlp):\n",
    "        for sent in par:\n",
    "            sent_lemma.append([token.lemma_ for token in sent     # List comprehension\n",
    "                               if token.lemma_ != '-PRON-'           # Pronouns are excluded\n",
    "                               and token.pos_ != 'PUNCT'             # Punctuation is excluded\n",
    "                               and token.is_alpha                    # Numbers are excluded\n",
    "                               and not token.is_stop                 # Stop words are excluded\n",
    "                              and len(token.lemma_) > 1])\n",
    "    sent_lemma = [[sl[j].lower() for j in range(len(sl))] for sl in sent_lemma]\n",
    "    \n",
    "    # Stem lemma with NLTK PorterStemmer and remove stop words\n",
    "    print('7/8: additional stopwords...')\n",
    "    if to_stem: ps = PorterStemmer()\n",
    "    sent_lemma_sw = []\n",
    "    for vec_list in tqdm(sent_lemma):    \n",
    "        update_list = []\n",
    "        for token in vec_list:\n",
    "            if token in sw: continue\n",
    "            if to_stem: update_list.append(ps.stem(token))\n",
    "            else: update_list.append(token)\n",
    "        sent_lemma_sw.append(update_list)\n",
    "    \n",
    "    print('8/8: constructing dataframe...')\n",
    "    nlp_df = pd.DataFrame(columns=['p_num','s_num','sent_text','sent_lemma',\n",
    "                                   'par_text','par_lemma'])\n",
    "\n",
    "    p_num = 0\n",
    "    skip = 0\n",
    "    s_lem_count = 0\n",
    "\n",
    "    for p, sents_in_par in enumerate(tqdm(sent_text)):\n",
    "        for s, sent in enumerate(sents_in_par):\n",
    "            if (len(sent) < 10)|(len(sent_lemma_sw[s_lem_count]) < 1):\n",
    "                skip += 1\n",
    "            else:\n",
    "                nlp_df = nlp_df.append({'p_num':p_num,\n",
    "                                        's_num':s - skip,\n",
    "                                        'sent_text':sent,\n",
    "                                        'sent_lemma':sent_lemma_sw[s_lem_count],\n",
    "                                        'par_text':par_file.loc[p, 'paragraph'],\n",
    "                                        'par_lemma':par_lemma_sw[p]\n",
    "                                        }, ignore_index=True)\n",
    "            s_lem_count += 1\n",
    "        p_num += 1\n",
    "        skip = 0\n",
    "        if p == par_file.shape[0]-1: continue\n",
    "\n",
    "    print('complete')\n",
    "    return nlp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 66.09it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 2792.48it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1059.37it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 5099.46it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 31.92it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/8: nlp of paragraphs...\n",
      "2/8: nlp lemmatizing, part-of-speech, stopwords...\n",
      "3/8: additional stopwords...\n",
      "4/8: saving sentence text...\n",
      "5/8: nlp of sentences...\n",
      "6/8: nlp lemmatizing, part-of-speech, stopwords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 2573.98it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 9565.96it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 106.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/8: additional stopwords...\n",
      "8/8: constructing dataframe...\n",
      "complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_num</th>\n",
       "      <th>s_num</th>\n",
       "      <th>sent_text</th>\n",
       "      <th>sent_lemma</th>\n",
       "      <th>par_text</th>\n",
       "      <th>par_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Why give a robot an order to obey orders—why a...</td>\n",
       "      <td>[robot, order, obey, order, original, order]</td>\n",
       "      <td>Why give a robot an order to obey orders—why a...</td>\n",
       "      <td>[robot, order, obey, order, original, order, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Why command a robot not to do harm</td>\n",
       "      <td>[command, robot, harm]</td>\n",
       "      <td>Why give a robot an order to obey orders—why a...</td>\n",
       "      <td>[robot, order, obey, order, original, order, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>it be easier never to command it to do harm in...</td>\n",
       "      <td>[easier, command, harm, place]</td>\n",
       "      <td>Why give a robot an order to obey orders—why a...</td>\n",
       "      <td>[robot, order, obey, order, original, order, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Does the universe contain a mysterious force p...</td>\n",
       "      <td>[universe, contain, mysterious, force, pull, e...</td>\n",
       "      <td>Why give a robot an order to obey orders—why a...</td>\n",
       "      <td>[robot, order, obey, order, original, order, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Do intelligent beings inevitably develop an at...</td>\n",
       "      <td>[intelligent, being, inevitably, develop, atti...</td>\n",
       "      <td>Why give a robot an order to obey orders—why a...</td>\n",
       "      <td>[robot, order, obey, order, original, order, c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  p_num s_num                                          sent_text  \\\n",
       "0     0     0  Why give a robot an order to obey orders—why a...   \n",
       "1     0     1                 Why command a robot not to do harm   \n",
       "2     0     2  it be easier never to command it to do harm in...   \n",
       "3     0     3  Does the universe contain a mysterious force p...   \n",
       "4     0     4  Do intelligent beings inevitably develop an at...   \n",
       "\n",
       "                                          sent_lemma  \\\n",
       "0       [robot, order, obey, order, original, order]   \n",
       "1                             [command, robot, harm]   \n",
       "2                     [easier, command, harm, place]   \n",
       "3  [universe, contain, mysterious, force, pull, e...   \n",
       "4  [intelligent, being, inevitably, develop, atti...   \n",
       "\n",
       "                                            par_text  \\\n",
       "0  Why give a robot an order to obey orders—why a...   \n",
       "1  Why give a robot an order to obey orders—why a...   \n",
       "2  Why give a robot an order to obey orders—why a...   \n",
       "3  Why give a robot an order to obey orders—why a...   \n",
       "4  Why give a robot an order to obey orders—why a...   \n",
       "\n",
       "                                           par_lemma  \n",
       "0  [robot, order, obey, order, original, order, c...  \n",
       "1  [robot, order, obey, order, original, order, c...  \n",
       "2  [robot, order, obey, order, original, order, c...  \n",
       "3  [robot, order, obey, order, original, order, c...  \n",
       "4  [robot, order, obey, order, original, order, c...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_df = preprocess_to_df(par_df, nlp, sw)\n",
    "unseen_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW Dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dict = Dictionary.load('../models/g_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus_s = [g_dict.doc2bow(sent) for sent in unseen_df.sent_lemma]\n",
    "bow_corpus_p = [g_dict.doc2bow(par) for par in unseen_df.par_lemma]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = Dictionary.load('../models/tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_s = tfidf[bow_corpus_s]\n",
    "corpus_p = tfidf[bow_corpus_p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 118.10it/s]\n"
     ]
    }
   ],
   "source": [
    "lda_df_s = pd.DataFrame(columns=[n for n in range(lda_multi_s.num_topics)])\n",
    "\n",
    "for i, doc in enumerate(lda_multi_s.get_document_topics(tqdm(corpus_s))):\n",
    "    for topic, proba in doc:\n",
    "        lda_df_s.loc[i, topic] = proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df_s.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.596593</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.638646</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.012011  0.012011  0.012011  0.012011  0.012011  0.012011  0.012011   \n",
       "1  0.013013  0.013013  0.013013  0.596593  0.013013  0.013013  0.013013   \n",
       "2  0.011657  0.011657  0.011657  0.638646  0.011657  0.011657  0.011657   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         7         8         9     ...           22        23        24  \\\n",
       "0  0.012011  0.012011  0.012011    ...     0.012011  0.012011  0.012011   \n",
       "1  0.013013  0.013013  0.013013    ...     0.013013  0.013013  0.013013   \n",
       "2  0.011657  0.011657  0.011657    ...     0.011657  0.011657  0.011657   \n",
       "3  0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "\n",
       "         25        26        27        28        29        30        31  \n",
       "0  0.012011  0.012011  0.012011  0.012011  0.012011  0.012011  0.012011  \n",
       "1  0.013013  0.013013  0.013013  0.013013  0.013013  0.013013  0.013013  \n",
       "2  0.011657  0.011657  0.011657  0.011657  0.011657  0.011657  0.011657  \n",
       "3  0.000000  0.147805  0.000000  0.153483  0.000000  0.000000  0.158270  \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_df_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 155.72it/s]\n"
     ]
    }
   ],
   "source": [
    "lda_df_p = pd.DataFrame(columns=[n for n in range(lda_multi_p.num_topics)])\n",
    "\n",
    "for i, doc in enumerate(lda_multi_p.get_document_topics(tqdm(corpus_p))):\n",
    "    for topic, proba in doc:\n",
    "        lda_df_p.loc[i, topic] = proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df_p.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 48)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_df = pd.merge(lda_df_s, lda_df_p, left_index=True, right_index=True)\n",
    "lda_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_train = pd.read_csv('../data_vec/lda_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df.columns = lda_train.columns[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_col_f = open('../models/lda_col.pkl','wb')\n",
    "pickle.dump(lda_train.columns[:-3].tolist(), lda_col_f)\n",
    "lda_col_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s0_lda_wrong',\n",
       " 's1_lda_note',\n",
       " 's2_lda_lives',\n",
       " 's3_lda_agenda',\n",
       " 's4_lda_reason']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_col = lda_train.columns[:-3].tolist()\n",
    "lda_col[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s0_lda_wrong</th>\n",
       "      <th>s1_lda_note</th>\n",
       "      <th>s2_lda_lives</th>\n",
       "      <th>s3_lda_agenda</th>\n",
       "      <th>s4_lda_reason</th>\n",
       "      <th>s5_lda_way</th>\n",
       "      <th>s6_lda_face</th>\n",
       "      <th>s7_lda_subjection</th>\n",
       "      <th>s8_lda_objection</th>\n",
       "      <th>s9_lda_evil</th>\n",
       "      <th>...</th>\n",
       "      <th>p6_lda_hospitality</th>\n",
       "      <th>p7_lda_accommodation</th>\n",
       "      <th>p8_lda_happiness</th>\n",
       "      <th>p9_lda_civil</th>\n",
       "      <th>p10_lda_deadites</th>\n",
       "      <th>p11_lda_idea</th>\n",
       "      <th>p12_lda_love</th>\n",
       "      <th>p13_lda_sense</th>\n",
       "      <th>p14_lda_biomolecular</th>\n",
       "      <th>p15_lda_thee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.355949</td>\n",
       "      <td>0.061488</td>\n",
       "      <td>0.011924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.596593</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.080901</td>\n",
       "      <td>0.363550</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.638646</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.360059</td>\n",
       "      <td>0.061597</td>\n",
       "      <td>0.011924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.078768</td>\n",
       "      <td>0.358053</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.356380</td>\n",
       "      <td>0.061498</td>\n",
       "      <td>0.011924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   s0_lda_wrong  s1_lda_note  s2_lda_lives  s3_lda_agenda  s4_lda_reason  \\\n",
       "0      0.012011     0.012011      0.012011       0.012011       0.012011   \n",
       "1      0.013013     0.013013      0.013013       0.596593       0.013013   \n",
       "2      0.011657     0.011657      0.011657       0.638646       0.011657   \n",
       "3      0.000000     0.000000      0.000000       0.000000       0.000000   \n",
       "4      0.000000     0.000000      0.000000       0.000000       0.000000   \n",
       "\n",
       "   s5_lda_way  s6_lda_face  s7_lda_subjection  s8_lda_objection  s9_lda_evil  \\\n",
       "0    0.012011     0.012011           0.012011          0.012011     0.012011   \n",
       "1    0.013013     0.013013           0.013013          0.013013     0.013013   \n",
       "2    0.011657     0.011657           0.011657          0.011657     0.011657   \n",
       "3    0.000000     0.000000           0.000000          0.000000     0.000000   \n",
       "4    0.000000     0.000000           0.000000          0.000000     0.000000   \n",
       "\n",
       "       ...       p6_lda_hospitality  p7_lda_accommodation  p8_lda_happiness  \\\n",
       "0      ...                 0.011924              0.011924          0.011924   \n",
       "1      ...                 0.011924              0.011924          0.011924   \n",
       "2      ...                 0.011924              0.011924          0.011924   \n",
       "3      ...                 0.011924              0.011924          0.011924   \n",
       "4      ...                 0.011924              0.011924          0.011924   \n",
       "\n",
       "   p9_lda_civil  p10_lda_deadites  p11_lda_idea  p12_lda_love  p13_lda_sense  \\\n",
       "0      0.011924          0.011924      0.011924      0.011924       0.355949   \n",
       "1      0.011924          0.011924      0.011924      0.080901       0.363550   \n",
       "2      0.011924          0.011924      0.011924      0.011924       0.360059   \n",
       "3      0.011924          0.011924      0.011924      0.078768       0.358053   \n",
       "4      0.011924          0.011924      0.011924      0.011924       0.356380   \n",
       "\n",
       "   p14_lda_biomolecular  p15_lda_thee  \n",
       "0              0.061488      0.011924  \n",
       "1              0.011924      0.011924  \n",
       "2              0.061597      0.011924  \n",
       "3              0.011924      0.011924  \n",
       "4              0.061498      0.011924  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Doc2Vec Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_s_vecs = []\n",
    "\n",
    "for _, sent_vec in unseen_df.sent_lemma.iteritems():\n",
    "    unseen_s_vecs.append(d2v_s.infer_vector(sent_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unseen_s_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_p_vecs = []\n",
    "\n",
    "for _, par_vec in unseen_df.par_lemma.iteritems():\n",
    "    unseen_p_vecs.append(d2v_p.infer_vector(par_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unseen_p_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine LDA and Doc2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature names\n",
    "s_vec_cols = ['s_vec_'+str(i) for i in range(len(unseen_s_vecs[0]))]\n",
    "p_vec_cols = ['p_vec_'+str(j) for j in range(len(unseen_p_vecs[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s_vec_0</th>\n",
       "      <th>s_vec_1</th>\n",
       "      <th>s_vec_2</th>\n",
       "      <th>s_vec_3</th>\n",
       "      <th>s_vec_4</th>\n",
       "      <th>s_vec_5</th>\n",
       "      <th>s_vec_6</th>\n",
       "      <th>s_vec_7</th>\n",
       "      <th>s_vec_8</th>\n",
       "      <th>s_vec_9</th>\n",
       "      <th>...</th>\n",
       "      <th>s_vec_22</th>\n",
       "      <th>s_vec_23</th>\n",
       "      <th>s_vec_24</th>\n",
       "      <th>s_vec_25</th>\n",
       "      <th>s_vec_26</th>\n",
       "      <th>s_vec_27</th>\n",
       "      <th>s_vec_28</th>\n",
       "      <th>s_vec_29</th>\n",
       "      <th>s_vec_30</th>\n",
       "      <th>s_vec_31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003744</td>\n",
       "      <td>0.008966</td>\n",
       "      <td>0.014755</td>\n",
       "      <td>0.007530</td>\n",
       "      <td>0.010414</td>\n",
       "      <td>0.013362</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.006343</td>\n",
       "      <td>-0.008635</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000615</td>\n",
       "      <td>0.010216</td>\n",
       "      <td>-0.013623</td>\n",
       "      <td>0.008943</td>\n",
       "      <td>-0.002946</td>\n",
       "      <td>0.004471</td>\n",
       "      <td>-0.011293</td>\n",
       "      <td>0.006642</td>\n",
       "      <td>0.010690</td>\n",
       "      <td>-0.000364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013981</td>\n",
       "      <td>-0.011683</td>\n",
       "      <td>-0.006053</td>\n",
       "      <td>-0.005907</td>\n",
       "      <td>0.006124</td>\n",
       "      <td>-0.007855</td>\n",
       "      <td>-0.009603</td>\n",
       "      <td>0.002044</td>\n",
       "      <td>0.005365</td>\n",
       "      <td>0.014382</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014127</td>\n",
       "      <td>-0.001812</td>\n",
       "      <td>-0.010935</td>\n",
       "      <td>-0.005800</td>\n",
       "      <td>-0.005018</td>\n",
       "      <td>-0.005067</td>\n",
       "      <td>0.012154</td>\n",
       "      <td>-0.006569</td>\n",
       "      <td>0.005070</td>\n",
       "      <td>0.006829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.008320</td>\n",
       "      <td>0.011190</td>\n",
       "      <td>-0.013644</td>\n",
       "      <td>-0.009686</td>\n",
       "      <td>0.013197</td>\n",
       "      <td>0.005827</td>\n",
       "      <td>-0.015350</td>\n",
       "      <td>-0.008278</td>\n",
       "      <td>0.009358</td>\n",
       "      <td>-0.002134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015343</td>\n",
       "      <td>-0.006496</td>\n",
       "      <td>-0.014522</td>\n",
       "      <td>-0.007119</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>-0.015175</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>-0.009038</td>\n",
       "      <td>-0.001843</td>\n",
       "      <td>0.013470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.005391</td>\n",
       "      <td>0.003979</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>-0.014013</td>\n",
       "      <td>-0.010442</td>\n",
       "      <td>-0.001955</td>\n",
       "      <td>-0.009408</td>\n",
       "      <td>0.012399</td>\n",
       "      <td>0.009340</td>\n",
       "      <td>-0.004419</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012620</td>\n",
       "      <td>0.003640</td>\n",
       "      <td>-0.006103</td>\n",
       "      <td>-0.011624</td>\n",
       "      <td>0.003740</td>\n",
       "      <td>0.010123</td>\n",
       "      <td>-0.015580</td>\n",
       "      <td>-0.013971</td>\n",
       "      <td>0.007665</td>\n",
       "      <td>0.006049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.004845</td>\n",
       "      <td>0.007972</td>\n",
       "      <td>-0.009591</td>\n",
       "      <td>-0.012286</td>\n",
       "      <td>-0.015615</td>\n",
       "      <td>-0.001116</td>\n",
       "      <td>0.014135</td>\n",
       "      <td>-0.014956</td>\n",
       "      <td>-0.009803</td>\n",
       "      <td>-0.000510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>-0.001767</td>\n",
       "      <td>0.003677</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.009254</td>\n",
       "      <td>0.005856</td>\n",
       "      <td>-0.010865</td>\n",
       "      <td>0.010693</td>\n",
       "      <td>-0.013434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    s_vec_0   s_vec_1   s_vec_2   s_vec_3   s_vec_4   s_vec_5   s_vec_6  \\\n",
       "0  0.003744  0.008966  0.014755  0.007530  0.010414  0.013362  0.000740   \n",
       "1  0.013981 -0.011683 -0.006053 -0.005907  0.006124 -0.007855 -0.009603   \n",
       "2 -0.008320  0.011190 -0.013644 -0.009686  0.013197  0.005827 -0.015350   \n",
       "3 -0.005391  0.003979  0.006634 -0.014013 -0.010442 -0.001955 -0.009408   \n",
       "4 -0.004845  0.007972 -0.009591 -0.012286 -0.015615 -0.001116  0.014135   \n",
       "\n",
       "    s_vec_7   s_vec_8   s_vec_9    ...     s_vec_22  s_vec_23  s_vec_24  \\\n",
       "0  0.006343 -0.008635  0.001410    ...    -0.000615  0.010216 -0.013623   \n",
       "1  0.002044  0.005365  0.014382    ...    -0.014127 -0.001812 -0.010935   \n",
       "2 -0.008278  0.009358 -0.002134    ...     0.015343 -0.006496 -0.014522   \n",
       "3  0.012399  0.009340 -0.004419    ...    -0.012620  0.003640 -0.006103   \n",
       "4 -0.014956 -0.009803 -0.000510    ...     0.001292 -0.001767  0.003677   \n",
       "\n",
       "   s_vec_25  s_vec_26  s_vec_27  s_vec_28  s_vec_29  s_vec_30  s_vec_31  \n",
       "0  0.008943 -0.002946  0.004471 -0.011293  0.006642  0.010690 -0.000364  \n",
       "1 -0.005800 -0.005018 -0.005067  0.012154 -0.006569  0.005070  0.006829  \n",
       "2 -0.007119  0.003241 -0.015175  0.002232 -0.009038 -0.001843  0.013470  \n",
       "3 -0.011624  0.003740  0.010123 -0.015580 -0.013971  0.007665  0.006049  \n",
       "4  0.000851  0.000275  0.009254  0.005856 -0.010865  0.010693 -0.013434  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_df = pd.DataFrame(unseen_s_vecs, columns=s_vec_cols)\n",
    "vec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_vec_0</th>\n",
       "      <th>p_vec_1</th>\n",
       "      <th>p_vec_2</th>\n",
       "      <th>p_vec_3</th>\n",
       "      <th>p_vec_4</th>\n",
       "      <th>p_vec_5</th>\n",
       "      <th>p_vec_6</th>\n",
       "      <th>p_vec_7</th>\n",
       "      <th>p_vec_8</th>\n",
       "      <th>p_vec_9</th>\n",
       "      <th>p_vec_10</th>\n",
       "      <th>p_vec_11</th>\n",
       "      <th>p_vec_12</th>\n",
       "      <th>p_vec_13</th>\n",
       "      <th>p_vec_14</th>\n",
       "      <th>p_vec_15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003275</td>\n",
       "      <td>-0.016758</td>\n",
       "      <td>0.011398</td>\n",
       "      <td>-0.019664</td>\n",
       "      <td>0.00178</td>\n",
       "      <td>0.017397</td>\n",
       "      <td>-0.026134</td>\n",
       "      <td>0.020437</td>\n",
       "      <td>0.029765</td>\n",
       "      <td>0.009829</td>\n",
       "      <td>0.01931</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.031064</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>-0.018649</td>\n",
       "      <td>-0.01217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.003275</td>\n",
       "      <td>-0.016758</td>\n",
       "      <td>0.011398</td>\n",
       "      <td>-0.019664</td>\n",
       "      <td>0.00178</td>\n",
       "      <td>0.017397</td>\n",
       "      <td>-0.026134</td>\n",
       "      <td>0.020437</td>\n",
       "      <td>0.029765</td>\n",
       "      <td>0.009829</td>\n",
       "      <td>0.01931</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.031064</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>-0.018649</td>\n",
       "      <td>-0.01217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.003275</td>\n",
       "      <td>-0.016758</td>\n",
       "      <td>0.011398</td>\n",
       "      <td>-0.019664</td>\n",
       "      <td>0.00178</td>\n",
       "      <td>0.017397</td>\n",
       "      <td>-0.026134</td>\n",
       "      <td>0.020437</td>\n",
       "      <td>0.029765</td>\n",
       "      <td>0.009829</td>\n",
       "      <td>0.01931</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.031064</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>-0.018649</td>\n",
       "      <td>-0.01217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.003275</td>\n",
       "      <td>-0.016758</td>\n",
       "      <td>0.011398</td>\n",
       "      <td>-0.019664</td>\n",
       "      <td>0.00178</td>\n",
       "      <td>0.017397</td>\n",
       "      <td>-0.026134</td>\n",
       "      <td>0.020437</td>\n",
       "      <td>0.029765</td>\n",
       "      <td>0.009829</td>\n",
       "      <td>0.01931</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.031064</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>-0.018649</td>\n",
       "      <td>-0.01217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.003275</td>\n",
       "      <td>-0.016758</td>\n",
       "      <td>0.011398</td>\n",
       "      <td>-0.019664</td>\n",
       "      <td>0.00178</td>\n",
       "      <td>0.017397</td>\n",
       "      <td>-0.026134</td>\n",
       "      <td>0.020437</td>\n",
       "      <td>0.029765</td>\n",
       "      <td>0.009829</td>\n",
       "      <td>0.01931</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.031064</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>-0.018649</td>\n",
       "      <td>-0.01217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    p_vec_0   p_vec_1   p_vec_2   p_vec_3  p_vec_4   p_vec_5   p_vec_6  \\\n",
       "0 -0.003275 -0.016758  0.011398 -0.019664  0.00178  0.017397 -0.026134   \n",
       "1 -0.003275 -0.016758  0.011398 -0.019664  0.00178  0.017397 -0.026134   \n",
       "2 -0.003275 -0.016758  0.011398 -0.019664  0.00178  0.017397 -0.026134   \n",
       "3 -0.003275 -0.016758  0.011398 -0.019664  0.00178  0.017397 -0.026134   \n",
       "4 -0.003275 -0.016758  0.011398 -0.019664  0.00178  0.017397 -0.026134   \n",
       "\n",
       "    p_vec_7   p_vec_8   p_vec_9  p_vec_10  p_vec_11  p_vec_12  p_vec_13  \\\n",
       "0  0.020437  0.029765  0.009829   0.01931  0.000084 -0.031064  0.014659   \n",
       "1  0.020437  0.029765  0.009829   0.01931  0.000084 -0.031064  0.014659   \n",
       "2  0.020437  0.029765  0.009829   0.01931  0.000084 -0.031064  0.014659   \n",
       "3  0.020437  0.029765  0.009829   0.01931  0.000084 -0.031064  0.014659   \n",
       "4  0.020437  0.029765  0.009829   0.01931  0.000084 -0.031064  0.014659   \n",
       "\n",
       "   p_vec_14  p_vec_15  \n",
       "0 -0.018649  -0.01217  \n",
       "1 -0.018649  -0.01217  \n",
       "2 -0.018649  -0.01217  \n",
       "3 -0.018649  -0.01217  \n",
       "4 -0.018649  -0.01217  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_vec_df = pd.DataFrame(unseen_p_vecs, columns=p_vec_cols)\n",
    "p_vec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in p_vec_cols:\n",
    "    vec_df[col_name] = p_vec_df[col_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s0_lda_wrong</th>\n",
       "      <th>s1_lda_note</th>\n",
       "      <th>s2_lda_lives</th>\n",
       "      <th>s3_lda_agenda</th>\n",
       "      <th>s4_lda_reason</th>\n",
       "      <th>s5_lda_way</th>\n",
       "      <th>s6_lda_face</th>\n",
       "      <th>s7_lda_subjection</th>\n",
       "      <th>s8_lda_objection</th>\n",
       "      <th>s9_lda_evil</th>\n",
       "      <th>...</th>\n",
       "      <th>p_vec_6</th>\n",
       "      <th>p_vec_7</th>\n",
       "      <th>p_vec_8</th>\n",
       "      <th>p_vec_9</th>\n",
       "      <th>p_vec_10</th>\n",
       "      <th>p_vec_11</th>\n",
       "      <th>p_vec_12</th>\n",
       "      <th>p_vec_13</th>\n",
       "      <th>p_vec_14</th>\n",
       "      <th>p_vec_15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026134</td>\n",
       "      <td>0.020437</td>\n",
       "      <td>0.029765</td>\n",
       "      <td>0.009829</td>\n",
       "      <td>0.01931</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.031064</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>-0.018649</td>\n",
       "      <td>-0.01217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.596593</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026134</td>\n",
       "      <td>0.020437</td>\n",
       "      <td>0.029765</td>\n",
       "      <td>0.009829</td>\n",
       "      <td>0.01931</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.031064</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>-0.018649</td>\n",
       "      <td>-0.01217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.638646</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026134</td>\n",
       "      <td>0.020437</td>\n",
       "      <td>0.029765</td>\n",
       "      <td>0.009829</td>\n",
       "      <td>0.01931</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.031064</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>-0.018649</td>\n",
       "      <td>-0.01217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026134</td>\n",
       "      <td>0.020437</td>\n",
       "      <td>0.029765</td>\n",
       "      <td>0.009829</td>\n",
       "      <td>0.01931</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.031064</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>-0.018649</td>\n",
       "      <td>-0.01217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026134</td>\n",
       "      <td>0.020437</td>\n",
       "      <td>0.029765</td>\n",
       "      <td>0.009829</td>\n",
       "      <td>0.01931</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.031064</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>-0.018649</td>\n",
       "      <td>-0.01217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   s0_lda_wrong  s1_lda_note  s2_lda_lives  s3_lda_agenda  s4_lda_reason  \\\n",
       "0      0.012011     0.012011      0.012011       0.012011       0.012011   \n",
       "1      0.013013     0.013013      0.013013       0.596593       0.013013   \n",
       "2      0.011657     0.011657      0.011657       0.638646       0.011657   \n",
       "3      0.000000     0.000000      0.000000       0.000000       0.000000   \n",
       "4      0.000000     0.000000      0.000000       0.000000       0.000000   \n",
       "\n",
       "   s5_lda_way  s6_lda_face  s7_lda_subjection  s8_lda_objection  s9_lda_evil  \\\n",
       "0    0.012011     0.012011           0.012011          0.012011     0.012011   \n",
       "1    0.013013     0.013013           0.013013          0.013013     0.013013   \n",
       "2    0.011657     0.011657           0.011657          0.011657     0.011657   \n",
       "3    0.000000     0.000000           0.000000          0.000000     0.000000   \n",
       "4    0.000000     0.000000           0.000000          0.000000     0.000000   \n",
       "\n",
       "     ...      p_vec_6   p_vec_7   p_vec_8   p_vec_9  p_vec_10  p_vec_11  \\\n",
       "0    ...    -0.026134  0.020437  0.029765  0.009829   0.01931  0.000084   \n",
       "1    ...    -0.026134  0.020437  0.029765  0.009829   0.01931  0.000084   \n",
       "2    ...    -0.026134  0.020437  0.029765  0.009829   0.01931  0.000084   \n",
       "3    ...    -0.026134  0.020437  0.029765  0.009829   0.01931  0.000084   \n",
       "4    ...    -0.026134  0.020437  0.029765  0.009829   0.01931  0.000084   \n",
       "\n",
       "   p_vec_12  p_vec_13  p_vec_14  p_vec_15  \n",
       "0 -0.031064  0.014659 -0.018649  -0.01217  \n",
       "1 -0.031064  0.014659 -0.018649  -0.01217  \n",
       "2 -0.031064  0.014659 -0.018649  -0.01217  \n",
       "3 -0.031064  0.014659 -0.018649  -0.01217  \n",
       "4 -0.031064  0.014659 -0.018649  -0.01217  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_vec_df = pd.merge(lda_df, vec_df, left_index=True, right_index=True)\n",
    "unseen_vec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s0_lda_wrong</th>\n",
       "      <th>s1_lda_note</th>\n",
       "      <th>s2_lda_lives</th>\n",
       "      <th>s3_lda_agenda</th>\n",
       "      <th>s4_lda_reason</th>\n",
       "      <th>s5_lda_way</th>\n",
       "      <th>s6_lda_face</th>\n",
       "      <th>s7_lda_subjection</th>\n",
       "      <th>s8_lda_objection</th>\n",
       "      <th>s9_lda_evil</th>\n",
       "      <th>...</th>\n",
       "      <th>p_vec_8</th>\n",
       "      <th>p_vec_9</th>\n",
       "      <th>p_vec_10</th>\n",
       "      <th>p_vec_11</th>\n",
       "      <th>p_vec_12</th>\n",
       "      <th>p_vec_13</th>\n",
       "      <th>p_vec_14</th>\n",
       "      <th>p_vec_15</th>\n",
       "      <th>p_num</th>\n",
       "      <th>s_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>0.012011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029765</td>\n",
       "      <td>0.009829</td>\n",
       "      <td>0.01931</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.031064</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>-0.018649</td>\n",
       "      <td>-0.01217</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.596593</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029765</td>\n",
       "      <td>0.009829</td>\n",
       "      <td>0.01931</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.031064</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>-0.018649</td>\n",
       "      <td>-0.01217</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.638646</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029765</td>\n",
       "      <td>0.009829</td>\n",
       "      <td>0.01931</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.031064</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>-0.018649</td>\n",
       "      <td>-0.01217</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029765</td>\n",
       "      <td>0.009829</td>\n",
       "      <td>0.01931</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.031064</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>-0.018649</td>\n",
       "      <td>-0.01217</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029765</td>\n",
       "      <td>0.009829</td>\n",
       "      <td>0.01931</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.031064</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>-0.018649</td>\n",
       "      <td>-0.01217</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   s0_lda_wrong  s1_lda_note  s2_lda_lives  s3_lda_agenda  s4_lda_reason  \\\n",
       "0      0.012011     0.012011      0.012011       0.012011       0.012011   \n",
       "1      0.013013     0.013013      0.013013       0.596593       0.013013   \n",
       "2      0.011657     0.011657      0.011657       0.638646       0.011657   \n",
       "3      0.000000     0.000000      0.000000       0.000000       0.000000   \n",
       "4      0.000000     0.000000      0.000000       0.000000       0.000000   \n",
       "\n",
       "   s5_lda_way  s6_lda_face  s7_lda_subjection  s8_lda_objection  s9_lda_evil  \\\n",
       "0    0.012011     0.012011           0.012011          0.012011     0.012011   \n",
       "1    0.013013     0.013013           0.013013          0.013013     0.013013   \n",
       "2    0.011657     0.011657           0.011657          0.011657     0.011657   \n",
       "3    0.000000     0.000000           0.000000          0.000000     0.000000   \n",
       "4    0.000000     0.000000           0.000000          0.000000     0.000000   \n",
       "\n",
       "   ...     p_vec_8   p_vec_9  p_vec_10  p_vec_11  p_vec_12  p_vec_13  \\\n",
       "0  ...    0.029765  0.009829   0.01931  0.000084 -0.031064  0.014659   \n",
       "1  ...    0.029765  0.009829   0.01931  0.000084 -0.031064  0.014659   \n",
       "2  ...    0.029765  0.009829   0.01931  0.000084 -0.031064  0.014659   \n",
       "3  ...    0.029765  0.009829   0.01931  0.000084 -0.031064  0.014659   \n",
       "4  ...    0.029765  0.009829   0.01931  0.000084 -0.031064  0.014659   \n",
       "\n",
       "   p_vec_14  p_vec_15  p_num  s_num  \n",
       "0 -0.018649  -0.01217      0      0  \n",
       "1 -0.018649  -0.01217      0      1  \n",
       "2 -0.018649  -0.01217      0      2  \n",
       "3 -0.018649  -0.01217      0      3  \n",
       "4 -0.018649  -0.01217      0      4  \n",
       "\n",
       "[5 rows x 98 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_vec_df['p_num'] = unseen_df.p_num\n",
    "unseen_vec_df['s_num'] = unseen_df.s_num\n",
    "unseen_vec_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Factorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Start Key</th>\n",
       "      <th>End Key</th>\n",
       "      <th>Category</th>\n",
       "      <th>Bumper Sticker</th>\n",
       "      <th>Original Language</th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>Year Val</th>\n",
       "      <th>Wiki Link</th>\n",
       "      <th>Wiki Text</th>\n",
       "      <th>Paragraphs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Categories</td>\n",
       "      <td>Aristotle</td>\n",
       "      <td>aristotle_categories.txt</td>\n",
       "      <td>*** START OF THIS PROJECT GUTENBERG EBOOK THE ...</td>\n",
       "      <td>End of the Project Gutenberg EBook of The Cate...</td>\n",
       "      <td>Hylomorphism</td>\n",
       "      <td>Being is a compound of matter and form</td>\n",
       "      <td>Greek</td>\n",
       "      <td>Greece</td>\n",
       "      <td>~335 BC</td>\n",
       "      <td>-335</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Categories_(Aris...</td>\n",
       "      <td>The Categories (Greek Κατηγορίαι Katēgoriai; L...</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Poetics</td>\n",
       "      <td>Aristotle</td>\n",
       "      <td>aristotle_poetics.txt</td>\n",
       "      <td>ARISTOTLE ON THE ART OF POETRY</td>\n",
       "      <td>End of the Project Gutenberg EBook of The Poet...</td>\n",
       "      <td>Dramatic and Literary Theory</td>\n",
       "      <td>Dramatic works imitate but vary in music, char...</td>\n",
       "      <td>Greek</td>\n",
       "      <td>Greece</td>\n",
       "      <td>335 BC</td>\n",
       "      <td>-335</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Poetics_(Aristotle)</td>\n",
       "      <td>Aristotle's Poetics (Greek: Περὶ ποιητικῆς; La...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Analects</td>\n",
       "      <td>Confucius</td>\n",
       "      <td>confucius_analects.txt</td>\n",
       "      <td>THE CHINESE CLASSICS</td>\n",
       "      <td>End of Project Gutenberg Etext THE CHINESE CLA...</td>\n",
       "      <td>Confucianism</td>\n",
       "      <td>Moral welfare and human virtue spring from alt...</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>China</td>\n",
       "      <td>~475-206 BC</td>\n",
       "      <td>-206</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Analects</td>\n",
       "      <td>The Analects (Chinese: 論語; pinyin: Lúnyǔ; Old ...</td>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Doctrine of the Mean</td>\n",
       "      <td>Confucius</td>\n",
       "      <td>confucius_mean.txt</td>\n",
       "      <td>THE DOCTRINE OF THE MEAN</td>\n",
       "      <td>THE END</td>\n",
       "      <td>Confucianism</td>\n",
       "      <td>The superior man uses self-watchfulness, lenie...</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>China</td>\n",
       "      <td>~500 BC</td>\n",
       "      <td>-500</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Doctrine_of_the_...</td>\n",
       "      <td>The Doctrine of the Mean or Zhongyong is both ...</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meditations on First Philosophy</td>\n",
       "      <td>Descartes, Rene</td>\n",
       "      <td>descartes_meditations.txt</td>\n",
       "      <td>TO THE MOST WISE AND ILLUSTRIOUS THE</td>\n",
       "      <td>1Copyright: 1996, James Fieser (jfieser@utm.ed...</td>\n",
       "      <td>Skepticism</td>\n",
       "      <td>Man is a thinking being capable of understandi...</td>\n",
       "      <td>Latin</td>\n",
       "      <td>France</td>\n",
       "      <td>1641</td>\n",
       "      <td>1641</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Meditations_on_F...</td>\n",
       "      <td>Meditations on First Philosophy in which the e...</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Title           Author  \\\n",
       "0                   The Categories        Aristotle   \n",
       "1                      The Poetics        Aristotle   \n",
       "2                         Analects        Confucius   \n",
       "3         The Doctrine of the Mean        Confucius   \n",
       "4  Meditations on First Philosophy  Descartes, Rene   \n",
       "\n",
       "                    Filename  \\\n",
       "0   aristotle_categories.txt   \n",
       "1      aristotle_poetics.txt   \n",
       "2     confucius_analects.txt   \n",
       "3         confucius_mean.txt   \n",
       "4  descartes_meditations.txt   \n",
       "\n",
       "                                           Start Key  \\\n",
       "0  *** START OF THIS PROJECT GUTENBERG EBOOK THE ...   \n",
       "1                     ARISTOTLE ON THE ART OF POETRY   \n",
       "2                               THE CHINESE CLASSICS   \n",
       "3                           THE DOCTRINE OF THE MEAN   \n",
       "4               TO THE MOST WISE AND ILLUSTRIOUS THE   \n",
       "\n",
       "                                             End Key  \\\n",
       "0  End of the Project Gutenberg EBook of The Cate...   \n",
       "1  End of the Project Gutenberg EBook of The Poet...   \n",
       "2  End of Project Gutenberg Etext THE CHINESE CLA...   \n",
       "3                                            THE END   \n",
       "4  1Copyright: 1996, James Fieser (jfieser@utm.ed...   \n",
       "\n",
       "                       Category  \\\n",
       "0                  Hylomorphism   \n",
       "1  Dramatic and Literary Theory   \n",
       "2                  Confucianism   \n",
       "3                  Confucianism   \n",
       "4                    Skepticism   \n",
       "\n",
       "                                      Bumper Sticker Original Language  \\\n",
       "0             Being is a compound of matter and form             Greek   \n",
       "1  Dramatic works imitate but vary in music, char...             Greek   \n",
       "2  Moral welfare and human virtue spring from alt...           Chinese   \n",
       "3  The superior man uses self-watchfulness, lenie...           Chinese   \n",
       "4  Man is a thinking being capable of understandi...             Latin   \n",
       "\n",
       "  Country         Year  Year Val  \\\n",
       "0  Greece      ~335 BC      -335   \n",
       "1  Greece       335 BC      -335   \n",
       "2   China  ~475-206 BC      -206   \n",
       "3   China      ~500 BC      -500   \n",
       "4  France         1641      1641   \n",
       "\n",
       "                                           Wiki Link  \\\n",
       "0  https://en.wikipedia.org/wiki/Categories_(Aris...   \n",
       "1  https://en.wikipedia.org/wiki/Poetics_(Aristotle)   \n",
       "2             https://en.wikipedia.org/wiki/Analects   \n",
       "3  https://en.wikipedia.org/wiki/Doctrine_of_the_...   \n",
       "4  https://en.wikipedia.org/wiki/Meditations_on_F...   \n",
       "\n",
       "                                           Wiki Text  Paragraphs  \n",
       "0  The Categories (Greek Κατηγορίαι Katēgoriai; L...         132  \n",
       "1  Aristotle's Poetics (Greek: Περὶ ποιητικῆς; La...          72  \n",
       "2  The Analects (Chinese: 論語; pinyin: Lúnyǔ; Old ...         509  \n",
       "3  The Doctrine of the Mean or Zhongyong is both ...          70  \n",
       "4  Meditations on First Philosophy in which the e...         144  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_meta = pd.read_csv('./data_source_text/text_meta.csv')\n",
    "text_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>titles</th>\n",
       "      <th>categories</th>\n",
       "      <th>bumper_stickers</th>\n",
       "      <th>country</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aristotle</td>\n",
       "      <td>The Categories\\nThe Poetics</td>\n",
       "      <td>Hylomorphism\\nDramatic and Literary Theory</td>\n",
       "      <td>Being is a compound of matter and form\\nDramat...</td>\n",
       "      <td>Greece</td>\n",
       "      <td>~335 BC\\n335 BC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Confucius</td>\n",
       "      <td>Analects\\nThe Doctrine of the Mean</td>\n",
       "      <td>Confucianism</td>\n",
       "      <td>Moral welfare and human virtue spring from alt...</td>\n",
       "      <td>China</td>\n",
       "      <td>~475-206 BC\\n~500 BC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Descartes, Rene</td>\n",
       "      <td>Meditations on First Philosophy\\nThe Principle...</td>\n",
       "      <td>Skepticism\\nRationalism</td>\n",
       "      <td>Man is a thinking being capable of understandi...</td>\n",
       "      <td>France</td>\n",
       "      <td>1641\\n1644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gibran, Khalil</td>\n",
       "      <td>The Prophet</td>\n",
       "      <td>Mysticism</td>\n",
       "      <td>The human condition is linked to union with th...</td>\n",
       "      <td>United States</td>\n",
       "      <td>1923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hobbes, Thomas</td>\n",
       "      <td>Leviathan</td>\n",
       "      <td>Political Philosophy</td>\n",
       "      <td>Only strong unified government can save from \"...</td>\n",
       "      <td>England</td>\n",
       "      <td>1651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author                                             titles  \\\n",
       "0        Aristotle                        The Categories\\nThe Poetics   \n",
       "1        Confucius                 Analects\\nThe Doctrine of the Mean   \n",
       "2  Descartes, Rene  Meditations on First Philosophy\\nThe Principle...   \n",
       "3   Gibran, Khalil                                        The Prophet   \n",
       "4   Hobbes, Thomas                                          Leviathan   \n",
       "\n",
       "                                   categories  \\\n",
       "0  Hylomorphism\\nDramatic and Literary Theory   \n",
       "1                                Confucianism   \n",
       "2                     Skepticism\\nRationalism   \n",
       "3                                   Mysticism   \n",
       "4                        Political Philosophy   \n",
       "\n",
       "                                     bumper_stickers        country  \\\n",
       "0  Being is a compound of matter and form\\nDramat...         Greece   \n",
       "1  Moral welfare and human virtue spring from alt...          China   \n",
       "2  Man is a thinking being capable of understandi...         France   \n",
       "3  The human condition is linked to union with th...  United States   \n",
       "4  Only strong unified government can save from \"...        England   \n",
       "\n",
       "                  years  \n",
       "0       ~335 BC\\n335 BC  \n",
       "1  ~475-206 BC\\n~500 BC  \n",
       "2            1641\\n1644  \n",
       "3                  1923  \n",
       "4                  1651  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_df = pd.DataFrame(columns=['author','titles','categories','bumper_stickers','country','years'])\n",
    "\n",
    "a_num = -1\n",
    "prev_author = None\n",
    "\n",
    "for i, row in text_meta.iterrows():\n",
    "    if row.Author != prev_author:\n",
    "        a_num += 1\n",
    "        author_df.loc[a_num, 'author'] = row.Author\n",
    "        author_df.loc[a_num, 'titles'] = row.Title\n",
    "        author_df.loc[a_num, 'categories'] = row.Category\n",
    "        author_df.loc[a_num, 'bumper_stickers'] = row['Bumper Sticker']\n",
    "        author_df.loc[a_num, 'country'] = row.Country\n",
    "        author_df.loc[a_num, 'years'] = row.Year\n",
    "        prev_author = row.Author\n",
    "    else:\n",
    "        author_df.loc[a_num, 'titles'] += str('\\n'+row.Title)\n",
    "        if row.Category != author_df.loc[a_num, 'categories']:\n",
    "            author_df.loc[a_num, 'categories'] += str('\\n'+row.Category)\n",
    "        author_df.loc[a_num, 'bumper_stickers'] += str('\\n'+row['Bumper Sticker'])\n",
    "        author_df.loc[a_num, 'years'] += str('\\n'+row.Year)\n",
    "\n",
    "author_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 6)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_df.to_csv('./data_source_text/author_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Factorize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rnn.predict(unseen_vec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>author</th>\n",
       "      <th>Aristotle</th>\n",
       "      <th>Confucius</th>\n",
       "      <th>Descartes, Rene</th>\n",
       "      <th>Gibran, Khalil</th>\n",
       "      <th>Hobbes, Thomas</th>\n",
       "      <th>Hume, David</th>\n",
       "      <th>James, William</th>\n",
       "      <th>Kant, Immanuel</th>\n",
       "      <th>Khyyam, Omar</th>\n",
       "      <th>Locke, John</th>\n",
       "      <th>Machiavelli, Nicolo</th>\n",
       "      <th>Mill, John Stuart</th>\n",
       "      <th>Nietzsche, Friedrich</th>\n",
       "      <th>Paine, Thomas</th>\n",
       "      <th>Plato</th>\n",
       "      <th>Rousseau, Jean-Jacques</th>\n",
       "      <th>Russell, Bertrand</th>\n",
       "      <th>Spinoza, Baruch</th>\n",
       "      <th>Sun Tzu</th>\n",
       "      <th>Thoreau, Henry David</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.069437e-02</td>\n",
       "      <td>3.469700e-02</td>\n",
       "      <td>2.984563e-02</td>\n",
       "      <td>3.953587e-02</td>\n",
       "      <td>3.886298e-02</td>\n",
       "      <td>4.314043e-02</td>\n",
       "      <td>5.358728e-02</td>\n",
       "      <td>0.054222</td>\n",
       "      <td>3.281839e-02</td>\n",
       "      <td>0.046305</td>\n",
       "      <td>2.995647e-02</td>\n",
       "      <td>0.070545</td>\n",
       "      <td>4.382015e-02</td>\n",
       "      <td>0.076121</td>\n",
       "      <td>0.055180</td>\n",
       "      <td>0.076130</td>\n",
       "      <td>7.096393e-02</td>\n",
       "      <td>0.067273</td>\n",
       "      <td>0.070868</td>\n",
       "      <td>3.543240e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.287724e-04</td>\n",
       "      <td>8.468950e-04</td>\n",
       "      <td>1.135269e-03</td>\n",
       "      <td>1.153726e-03</td>\n",
       "      <td>2.335769e-03</td>\n",
       "      <td>1.425093e-03</td>\n",
       "      <td>2.169908e-02</td>\n",
       "      <td>0.022314</td>\n",
       "      <td>4.603830e-04</td>\n",
       "      <td>0.031670</td>\n",
       "      <td>4.687805e-04</td>\n",
       "      <td>0.235597</td>\n",
       "      <td>2.634636e-03</td>\n",
       "      <td>0.020981</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.388955</td>\n",
       "      <td>5.202569e-02</td>\n",
       "      <td>0.110697</td>\n",
       "      <td>0.088636</td>\n",
       "      <td>1.703698e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.105362e-09</td>\n",
       "      <td>2.660647e-08</td>\n",
       "      <td>2.357356e-08</td>\n",
       "      <td>9.035495e-08</td>\n",
       "      <td>6.298944e-06</td>\n",
       "      <td>5.006128e-08</td>\n",
       "      <td>1.094326e-04</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>4.890494e-09</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>2.499723e-08</td>\n",
       "      <td>0.114226</td>\n",
       "      <td>4.538642e-06</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.855320</td>\n",
       "      <td>1.773053e-04</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>2.340868e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.959659e-14</td>\n",
       "      <td>1.121142e-11</td>\n",
       "      <td>1.924142e-11</td>\n",
       "      <td>1.886660e-11</td>\n",
       "      <td>5.060637e-08</td>\n",
       "      <td>6.771470e-11</td>\n",
       "      <td>7.832289e-07</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>7.767505e-13</td>\n",
       "      <td>0.050587</td>\n",
       "      <td>9.386862e-12</td>\n",
       "      <td>0.079136</td>\n",
       "      <td>3.565652e-08</td>\n",
       "      <td>0.003228</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.862521</td>\n",
       "      <td>6.678251e-06</td>\n",
       "      <td>0.004344</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>1.815128e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.285001e-17</td>\n",
       "      <td>6.601790e-14</td>\n",
       "      <td>4.954925e-14</td>\n",
       "      <td>1.050992e-14</td>\n",
       "      <td>6.326504e-10</td>\n",
       "      <td>1.259581e-12</td>\n",
       "      <td>2.283182e-09</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>9.186127e-16</td>\n",
       "      <td>0.595079</td>\n",
       "      <td>2.820067e-14</td>\n",
       "      <td>0.030633</td>\n",
       "      <td>3.223998e-10</td>\n",
       "      <td>0.004805</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.368195</td>\n",
       "      <td>1.293953e-07</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>2.623799e-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "author     Aristotle     Confucius  Descartes, Rene  Gibran, Khalil  \\\n",
       "0       3.069437e-02  3.469700e-02     2.984563e-02    3.953587e-02   \n",
       "1       4.287724e-04  8.468950e-04     1.135269e-03    1.153726e-03   \n",
       "2       2.105362e-09  2.660647e-08     2.357356e-08    9.035495e-08   \n",
       "3       9.959659e-14  1.121142e-11     1.924142e-11    1.886660e-11   \n",
       "4       7.285001e-17  6.601790e-14     4.954925e-14    1.050992e-14   \n",
       "\n",
       "author  Hobbes, Thomas   Hume, David  James, William  Kant, Immanuel  \\\n",
       "0         3.886298e-02  4.314043e-02    5.358728e-02        0.054222   \n",
       "1         2.335769e-03  1.425093e-03    2.169908e-02        0.022314   \n",
       "2         6.298944e-06  5.006128e-08    1.094326e-04        0.000165   \n",
       "3         5.060637e-08  6.771470e-11    7.832289e-07        0.000040   \n",
       "4         6.326504e-10  1.259581e-12    2.283182e-09        0.000007   \n",
       "\n",
       "author  Khyyam, Omar  Locke, John  Machiavelli, Nicolo  Mill, John Stuart  \\\n",
       "0       3.281839e-02     0.046305         2.995647e-02           0.070545   \n",
       "1       4.603830e-04     0.031670         4.687805e-04           0.235597   \n",
       "2       4.890494e-09     0.011225         2.499723e-08           0.114226   \n",
       "3       7.767505e-13     0.050587         9.386862e-12           0.079136   \n",
       "4       9.186127e-16     0.595079         2.820067e-14           0.030633   \n",
       "\n",
       "author  Nietzsche, Friedrich  Paine, Thomas     Plato  Rousseau, Jean-Jacques  \\\n",
       "0               4.382015e-02       0.076121  0.055180                0.076130   \n",
       "1               2.634636e-03       0.020981  0.014831                0.388955   \n",
       "2               4.538642e-06       0.002608  0.000232                0.855320   \n",
       "3               3.565652e-08       0.003228  0.000008                0.862521   \n",
       "4               3.223998e-10       0.004805  0.000001                0.368195   \n",
       "\n",
       "author  Russell, Bertrand  Spinoza, Baruch   Sun Tzu  Thoreau, Henry David  \n",
       "0            7.096393e-02         0.067273  0.070868          3.543240e-02  \n",
       "1            5.202569e-02         0.110697  0.088636          1.703698e-03  \n",
       "2            1.773053e-04         0.013100  0.002827          2.340868e-07  \n",
       "3            6.678251e-06         0.004344  0.000128          1.815128e-10  \n",
       "4            1.293953e-07         0.001275  0.000004          2.623799e-13  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = pd.DataFrame(preds, columns=author_df.author)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 20)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = pred_df.mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nietzsche, Friedrich'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nlp_factorize(lda_df, model=model_full, ss=ss):\n",
    "#     lda_sc = ss.transform(lda_df.values)    \n",
    "#     preds = model.predict_proba(lda_sc)\n",
    "#     return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = nlp_factorize(bs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need Spacy `nlp`, stopwords list `sw`\n",
    "\n",
    "def display_nlp_factors(block_str, n_factors=5, n_details=1):\n",
    "    # convert block string to list of paragraph strings\n",
    "    c_pars = par_list(block_str.strip().split('\\n'))\n",
    "    \n",
    "    # convert list of paragraph strings to dataframe\n",
    "    par_df = pd.DataFrame(columns=['paragraph'])\n",
    "    for i, book in enumerate(tqdm(c_pars)):\n",
    "        par_df.loc[i, 'paragraph'] = book\n",
    "    \n",
    "    # spaCy preprocessing to dataframe\n",
    "    unseen_df = preprocess_to_df(par_df, nlp, sw)\n",
    "    \n",
    "    # bag of words corpus using Gensim\n",
    "    bow_corpus_s = [g_dict.doc2bow(sent) for sent in unseen_df.sent_lemma]\n",
    "    bow_corpus_p = [g_dict.doc2bow(par) for par in unseen_df.par_lemma]\n",
    "    \n",
    "    # TFIDF vectorization with Gensim\n",
    "    corpus_s = tfidf[bow_corpus_s]\n",
    "    corpus_p = tfidf[bow_corpus_p]\n",
    "    \n",
    "    # LDA vectors\n",
    "    lda_df_s = pd.DataFrame(columns=[n for n in range(lda_multi_s.num_topics)])\n",
    "    for i, doc in enumerate(lda_multi_s.get_document_topics(tqdm(corpus_s))):\n",
    "        for topic, proba in doc:\n",
    "            lda_df_s.loc[i, topic] = proba\n",
    "    lda_df_s.fillna(0, inplace=True)\n",
    "    \n",
    "    lda_df_p = pd.DataFrame(columns=[n for n in range(lda_multi_p.num_topics)])\n",
    "    for i, doc in enumerate(lda_multi_p.get_document_topics(tqdm(corpus_p))):\n",
    "        for topic, proba in doc:\n",
    "            lda_df_p.loc[i, topic] = proba\n",
    "    lda_df_p.fillna(0, inplace=True)\n",
    "    \n",
    "    # LDA dataframe\n",
    "    lda_df = pd.merge(lda_df_s, lda_df_p, left_index=True, right_index=True)\n",
    "    lda_df.columns = lda_col\n",
    "    \n",
    "    # Doc2Vec\n",
    "    unseen_s_vecs = []\n",
    "    for _, sent_vec in unseen_df.sent_lemma.iteritems():\n",
    "        unseen_s_vecs.append(d2v_s.infer_vector(sent_vec))\n",
    "        \n",
    "    unseen_p_vecs = []\n",
    "    for _, par_vec in unseen_df.par_lemma.iteritems():\n",
    "        unseen_p_vecs.append(d2v_p.infer_vector(par_vec))\n",
    "    \n",
    "    # column names for Doc2Vec\n",
    "    s_vec_cols = ['s_vec_'+str(i) for i in range(len(unseen_s_vecs[0]))]\n",
    "    p_vec_cols = ['p_vec_'+str(j) for j in range(len(unseen_p_vecs[0]))]\n",
    "    \n",
    "    # dataframe from d2v vectors\n",
    "    vec_df = pd.DataFrame(unseen_s_vecs, columns=s_vec_cols)\n",
    "    p_vec_df = pd.DataFrame(unseen_p_vecs, columns=p_vec_cols)\n",
    "    for col_name in p_vec_cols:\n",
    "        vec_df[col_name] = p_vec_df[col_name]\n",
    "    \n",
    "    # merge with LDA dataframe\n",
    "    unseen_vec_df = pd.merge(lda_df, vec_df, left_index=True, right_index=True)\n",
    "    unseen_vec_df['p_num'] = unseen_df.p_num\n",
    "    unseen_vec_df['s_num'] = unseen_df.s_num\n",
    "    \n",
    "    # predict on input\n",
    "    preds = rnn.predict(unseen_vec_df)\n",
    "    pred_df = pd.DataFrame(preds, columns=author_df.author)\n",
    "    \n",
    "    # combine predicted results and sort\n",
    "    factors = pred_df.mean().sort_values(ascending=False)\n",
    "    \n",
    "    # format output display\n",
    "    print('-'*30)\n",
    "    print('Text:\\n', '\"'+block_str+'\"')\n",
    "    print('-'*30)\n",
    "    print('\\nTop Authors:\\n')\n",
    "    \n",
    "    for i in range(n_details):\n",
    "        a_name = factors.index[i]\n",
    "        titles = author_df[author_df.author==factors.index[i]].titles.item().split('\\n')\n",
    "        cats = author_df[author_df.author==factors.index[i]].categories.item().split('\\n')\n",
    "        bs = author_df[author_df.author==factors.index[i]].bumper_stickers.item().split('\\n')\n",
    "        for _, b in enumerate(bs):\n",
    "            if len(b) >= 65:\n",
    "                spl_ix = b[:65].rfind(' ')\n",
    "                bs[_] = ''.join((b[:spl_ix],'\\n\\t\\t\\t',b[spl_ix:]))\n",
    "        country = author_df[author_df.author==factors.index[i]].country.item()\n",
    "        years = author_df[author_df.author==factors.index[i]].years.item().split('\\n')\n",
    "        \n",
    "        print('\\tAuthor:\\t\\t{}'.format(a_name))\n",
    "        print('\\tCountry:\\t{}'.format(country))\n",
    "        if len(titles) > 1:\n",
    "            for t, title in enumerate(titles):\n",
    "                if t == 0: print('\\tWorks:\\t\\t{} ({})'.format(titles[t], years[t]))\n",
    "                else: print('\\t\\t\\t{} ({})'.format(titles[t], years[t]))\n",
    "        else: print('\\tWork:\\t\\t{} ({})'.format(titles[0], years[0]))\n",
    "        if len(cats) > 1:\n",
    "            for c, cat in enumerate(cats):\n",
    "                if c == 0: print('\\tCategory:\\t{}'.format(cats[c]))\n",
    "                else: print('\\t\\t\\t{}'.format(cats[c]))\n",
    "        else: print('\\tCategory:\\t{}'.format(cats[0]))\n",
    "        if len(bs) > 1:\n",
    "            for b, bstkr in enumerate(bs):\n",
    "                if b == 0: print('\\tThemes:\\t\\t{}'.format(bs[b]))\n",
    "                else: print('\\t\\t\\t{}'.format(bs[b]))\n",
    "        else: print('\\tTheme:\\t\\t{}'.format(bs[0]))\n",
    "        print('')\n",
    "    \n",
    "    print('-'*30)\n",
    "    print('\\nTop Factors:')\n",
    "    for j in range(n_factors):\n",
    "        print('\\t{}:{}{:.3f}'.format(factors.index[j], (' '*(30-len(factors.index[j]))), factors[j]))\n",
    "    print('')\n",
    "    print('-'*30)\n",
    "    return factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWhy give a robot an order to obey orders—why aren't the original orders enough? \\nWhy command a robot not to do harm—wouldn't it be easier never to command it to \\ndo harm in the first place? Does the universe contain a mysterious force pulling \\nentities toward malevolence, so that a positronic brain must be programmed to \\nwithstand it? Do intelligent beings inevitably develop an attitude problem?\\n\\nNow that computers really have become smarter and more powerful, the anxiety has \\nwaned. Today's ubiquitous, networked computers have an unprecedented ability to \\ndo mischief should they ever go to the bad. But the only mayhem comes from \\nunpredictable chaos or from human malice in the form of viruses. We no longer \\nworry about electronic serial killers or subversive silicon cabals because we \\nare beginning to appreciate that malevolence—like vision, motor coordination, \\nand common sense—does not come free with computation but has to be programmed in.\\n\\nAggression, like every other part of human behavior we take for granted, is a \\nchallenging engineering problem!\\n\\nSteven Pinker\\n\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 1012.99it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 66.53it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 6505.32it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 2247.15it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 6355.01it/s]\n",
      " 50%|█████     | 2/4 [00:00<00:00, 18.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/8: nlp of paragraphs...\n",
      "2/8: nlp lemmatizing, part-of-speech, stopwords...\n",
      "3/8: additional stopwords...\n",
      "4/8: saving sentence text...\n",
      "5/8: nlp of sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 30.34it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4478.70it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 9335.04it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 109.49it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 87.44it/s]\n",
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/8: nlp lemmatizing, part-of-speech, stopwords...\n",
      "7/8: additional stopwords...\n",
      "8/8: constructing dataframe...\n",
      "complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 11/11 [00:00<00:00, 122.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Text:\n",
      " \"\n",
      "Why give a robot an order to obey orders—why aren't the original orders enough? \n",
      "Why command a robot not to do harm—wouldn't it be easier never to command it to \n",
      "do harm in the first place? Does the universe contain a mysterious force pulling \n",
      "entities toward malevolence, so that a positronic brain must be programmed to \n",
      "withstand it? Do intelligent beings inevitably develop an attitude problem?\n",
      "\n",
      "Now that computers really have become smarter and more powerful, the anxiety has \n",
      "waned. Today's ubiquitous, networked computers have an unprecedented ability to \n",
      "do mischief should they ever go to the bad. But the only mayhem comes from \n",
      "unpredictable chaos or from human malice in the form of viruses. We no longer \n",
      "worry about electronic serial killers or subversive silicon cabals because we \n",
      "are beginning to appreciate that malevolence—like vision, motor coordination, \n",
      "and common sense—does not come free with computation but has to be programmed in.\n",
      "\n",
      "Aggression, like every other part of human behavior we take for granted, is a \n",
      "challenging engineering problem!\n",
      "\n",
      "Steven Pinker\n",
      "\"\n",
      "------------------------------\n",
      "\n",
      "Top Authors:\n",
      "\n",
      "\tAuthor:\t\tNietzsche, Friedrich\n",
      "\tCountry:\tGermany\n",
      "\tWork:\t\tThus Spake Zarathustra (1891)\n",
      "\tCategory:\tPerspectivism\n",
      "\tTheme:\t\tAll ideations take place from particular perspectives\n",
      "\n",
      "\tAuthor:\t\tRousseau, Jean-Jacques\n",
      "\tCountry:\tFrance\n",
      "\tWork:\t\tThe Social Contract (1762)\n",
      "\tCategory:\tSocial Contract\n",
      "\tTheme:\t\tThe people are sovereign in forming a political community to\n",
      "\t\t\t address problems in a commercial society\n",
      "\n",
      "\tAuthor:\t\tLocke, John\n",
      "\tCountry:\tEngland\n",
      "\tWorks:\t\tA Letter Concerning Toleration (1689)\n",
      "\t\t\tAn Essay Concerning Human Understanding (1690)\n",
      "\tCategory:\tEmpiricism\n",
      "\tThemes:\t\tThe problem of religion overtaking government can be addressed\n",
      "\t\t\t with religious toleration\n",
      "\t\t\tAll knowledge comes from and is limited by experience, and the\n",
      "\t\t\t mind starts as a blank slate\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Top Factors:\n",
      "\tNietzsche, Friedrich:          0.542\n",
      "\tRousseau, Jean-Jacques:        0.230\n",
      "\tLocke, John:                   0.068\n",
      "\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = display_nlp_factors(quote, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "Doubt as sin. — Christianity has done its utmost to close the circle and \n",
    "declared even doubt to be sin. One is supposed to be cast into belief \n",
    "without reason, by a miracle, and from then on to swim in it as in the \n",
    "brightest and least ambiguous of elements: even a glance towards land, \n",
    "even the thought that one perhaps exists for something else as well as \n",
    "swimming, even the slightest impulse of our amphibious nature — is sin! \n",
    "And notice that all this means that the foundation of belief and all \n",
    "reflection on its origin is likewise excluded as sinful. What is wanted \n",
    "are blindness and intoxication and an eternal song over the waves in which \n",
    "reason has drowned.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 744.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 65.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3165.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2878.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3837.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 54.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1847.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3026.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 272.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 368.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 113.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/8: nlp of paragraphs...\n",
      "2/8: nlp lemmatizing, part-of-speech, stopwords...\n",
      "3/8: additional stopwords...\n",
      "4/8: saving sentence text...\n",
      "5/8: nlp of sentences...\n",
      "6/8: nlp lemmatizing, part-of-speech, stopwords...\n",
      "7/8: additional stopwords...\n",
      "8/8: constructing dataframe...\n",
      "complete\n",
      "------------------------------\n",
      "Text:\n",
      " \"\n",
      "To teach how to live without certainty, and yet without being paralyzed \n",
      "by hesitation, is perhaps the chief thing that philosophy, in our age, \n",
      "can still do for those who study it.\n",
      "\"\n",
      "------------------------------\n",
      "\n",
      "Top Authors:\n",
      "\n",
      "\tAuthor:\t\tPaine, Thomas\n",
      "\tCountry:\tUnited States\n",
      "\tWorks:\t\tCommon Sense (1776)\n",
      "\t\t\tThe Rights of Man (1791)\n",
      "\tCategory:\tPolitical Philosophy\n",
      "\tThemes:\t\tAmerican colonists should revolt against inevitable British\n",
      "\t\t\t oppression in pursuit of egalitarian government\n",
      "\t\t\tPolitical revolution is permissible when a government does not\n",
      "\t\t\t safeguard the natural rights of its people\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Top Factors:\n",
      "\tPaine, Thomas:                 0.108\n",
      "\tRousseau, Jean-Jacques:        0.103\n",
      "\tMill, John Stuart:             0.081\n",
      "\tRussell, Bertrand:             0.075\n",
      "\tSpinoza, Baruch:               0.074\n",
      "\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = display_nlp_factors(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "To teach how to live without certainty, and yet without being paralyzed \n",
    "by hesitation, is perhaps the chief thing that philosophy, in our age, \n",
    "can still do for those who study it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 557.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 71.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3569.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3034.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3979.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 58.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3644.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3059.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 283.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 342.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 114.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/8: nlp of paragraphs...\n",
      "2/8: nlp lemmatizing, part-of-speech, stopwords...\n",
      "3/8: additional stopwords...\n",
      "4/8: saving sentence text...\n",
      "5/8: nlp of sentences...\n",
      "6/8: nlp lemmatizing, part-of-speech, stopwords...\n",
      "7/8: additional stopwords...\n",
      "8/8: constructing dataframe...\n",
      "complete\n",
      "------------------------------\n",
      "Text:\n",
      " \"\n",
      "To teach how to live without certainty, and yet without being paralyzed \n",
      "by hesitation, is perhaps the chief thing that philosophy, in our age, \n",
      "can still do for those who study it.\n",
      "\"\n",
      "------------------------------\n",
      "\n",
      "Top Authors:\n",
      "\n",
      "\tAuthor:\t\tPaine, Thomas\n",
      "\tCountry:\tUnited States\n",
      "\tWorks:\t\tCommon Sense (1776)\n",
      "\t\t\tThe Rights of Man (1791)\n",
      "\tCategory:\tPolitical Philosophy\n",
      "\tThemes:\t\tAmerican colonists should revolt against inevitable British\n",
      "\t\t\t oppression in pursuit of egalitarian government\n",
      "\t\t\tPolitical revolution is permissible when a government does not\n",
      "\t\t\t safeguard the natural rights of its people\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Top Factors:\n",
      "\tPaine, Thomas:                 0.108\n",
      "\tRousseau, Jean-Jacques:        0.103\n",
      "\tMill, John Stuart:             0.081\n",
      "\tRussell, Bertrand:             0.075\n",
      "\tSpinoza, Baruch:               0.074\n",
      "\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = display_nlp_factors(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
