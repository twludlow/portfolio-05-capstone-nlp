{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone: Philosophical Factors for NLP\n",
    "**_Measuring Similarity to Philosophical Concepts in Text Data_**\n",
    "\n",
    "## Thomas W. Ludlow, Jr.\n",
    "**General Assembly Data Science Immersive DSI-NY-6**\n",
    "\n",
    "**February 12, 2019**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4 - Multiclass Classifier Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**4.1 Data Preparation**](#4.1-Data-Preparation)\n",
    "- [4.1.1 Define X and y](#4.1.1-Define-X-and-y)\n",
    "- [4.1.2 Standardize and Weight](#4.1.2-Standardize-and-Weight)\n",
    "\n",
    "[**4.2 Logistic Regression**](#4.2-Logistic-Regression)\n",
    "- [4.2.1 Logistic Regression Assembly](#4.2.1-Logistic-Regression-Assembly)\n",
    "- [4.2.2 Logistic Regression Optimization](#4.2.2-Logistic-Regression-Optimization)\n",
    "\n",
    "[**4.3 Recurrent Neural Net**](#4.4-Recurrent-Neural-Net)\n",
    "- [4.3.1 Keras FFRNN Assembly](#4.4.1-Keras-FFRNN-Assembly)\n",
    "- [4.3.2 Keras FFRNN Optimization](#4.4.2-Keras-FFRNN-Optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Python Data Science\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Natural Language Processing\n",
    "import spacy\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel, ldamulticore, CoherenceModel\n",
    "\n",
    "# Modeling Prep\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Neural Net\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Override deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Train and Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_train = pd.read_csv('../data_vec/prep_train.csv')\n",
    "prep_test = pd.read_csv('../data_vec/prep_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63932, 99)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s_vec_0</th>\n",
       "      <th>s_vec_1</th>\n",
       "      <th>s_vec_2</th>\n",
       "      <th>s_vec_3</th>\n",
       "      <th>s_vec_4</th>\n",
       "      <th>s_vec_5</th>\n",
       "      <th>s_vec_6</th>\n",
       "      <th>s_vec_7</th>\n",
       "      <th>s_vec_8</th>\n",
       "      <th>s_vec_9</th>\n",
       "      <th>...</th>\n",
       "      <th>p9_lda_civil</th>\n",
       "      <th>p10_lda_deadites</th>\n",
       "      <th>p11_lda_idea</th>\n",
       "      <th>p12_lda_love</th>\n",
       "      <th>p13_lda_sense</th>\n",
       "      <th>p14_lda_biomolecular</th>\n",
       "      <th>p15_lda_thee</th>\n",
       "      <th>a_num</th>\n",
       "      <th>p_num</th>\n",
       "      <th>s_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005141</td>\n",
       "      <td>0.007563</td>\n",
       "      <td>-0.005119</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>0.014759</td>\n",
       "      <td>-0.000401</td>\n",
       "      <td>-0.013146</td>\n",
       "      <td>-0.010836</td>\n",
       "      <td>0.013186</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>0.805130</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000672</td>\n",
       "      <td>-0.008255</td>\n",
       "      <td>0.004202</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>-0.008325</td>\n",
       "      <td>0.010428</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.013675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>0.805130</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.009192</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.004282</td>\n",
       "      <td>0.003132</td>\n",
       "      <td>0.009840</td>\n",
       "      <td>0.002502</td>\n",
       "      <td>-0.015351</td>\n",
       "      <td>-0.012835</td>\n",
       "      <td>0.005541</td>\n",
       "      <td>-0.002684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>0.805130</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>0.012991</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.012862</td>\n",
       "      <td>0.005095</td>\n",
       "      <td>0.007222</td>\n",
       "      <td>-0.000288</td>\n",
       "      <td>0.005181</td>\n",
       "      <td>0.011893</td>\n",
       "      <td>-0.000838</td>\n",
       "      <td>-0.004347</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>-0.013354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013657</td>\n",
       "      <td>0.013657</td>\n",
       "      <td>0.013657</td>\n",
       "      <td>0.013657</td>\n",
       "      <td>0.736205</td>\n",
       "      <td>0.013657</td>\n",
       "      <td>0.013657</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005489</td>\n",
       "      <td>-0.004147</td>\n",
       "      <td>-0.003460</td>\n",
       "      <td>-0.008962</td>\n",
       "      <td>0.004228</td>\n",
       "      <td>-0.000990</td>\n",
       "      <td>-0.011614</td>\n",
       "      <td>-0.008368</td>\n",
       "      <td>0.003744</td>\n",
       "      <td>0.005757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013657</td>\n",
       "      <td>0.013657</td>\n",
       "      <td>0.013657</td>\n",
       "      <td>0.013657</td>\n",
       "      <td>0.735830</td>\n",
       "      <td>0.013657</td>\n",
       "      <td>0.013657</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    s_vec_0   s_vec_1   s_vec_2   s_vec_3   s_vec_4   s_vec_5   s_vec_6  \\\n",
       "0  0.005141  0.007563 -0.005119  0.003487  0.014759 -0.000401 -0.013146   \n",
       "1  0.000672 -0.008255  0.004202  0.012191 -0.008325  0.010428 -0.010309   \n",
       "2  0.009192  0.003800  0.004282  0.003132  0.009840  0.002502 -0.015351   \n",
       "3  0.012862  0.005095  0.007222 -0.000288  0.005181  0.011893 -0.000838   \n",
       "4  0.005489 -0.004147 -0.003460 -0.008962  0.004228 -0.000990 -0.011614   \n",
       "\n",
       "    s_vec_7   s_vec_8   s_vec_9  ...    p9_lda_civil  p10_lda_deadites  \\\n",
       "0 -0.010836  0.013186  0.000359  ...        0.012991          0.012991   \n",
       "1  0.007246  0.000948  0.013675  ...        0.012991          0.012991   \n",
       "2 -0.012835  0.005541 -0.002684  ...        0.012991          0.012991   \n",
       "3 -0.004347  0.000892 -0.013354  ...        0.013657          0.013657   \n",
       "4 -0.008368  0.003744  0.005757  ...        0.013657          0.013657   \n",
       "\n",
       "   p11_lda_idea  p12_lda_love  p13_lda_sense  p14_lda_biomolecular  \\\n",
       "0      0.805130      0.012991       0.012991              0.012991   \n",
       "1      0.805130      0.012991       0.012991              0.012991   \n",
       "2      0.805130      0.012991       0.012991              0.012991   \n",
       "3      0.013657      0.013657       0.736205              0.013657   \n",
       "4      0.013657      0.013657       0.735830              0.013657   \n",
       "\n",
       "   p15_lda_thee  a_num  p_num  s_num  \n",
       "0      0.012991      0      0      0  \n",
       "1      0.012991      0      0      1  \n",
       "2      0.012991      0      0      2  \n",
       "3      0.013657      0      1      0  \n",
       "4      0.013657      0      1      1  \n",
       "\n",
       "[5 rows x 99 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7935, 99)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s_vec_0</th>\n",
       "      <th>s_vec_1</th>\n",
       "      <th>s_vec_2</th>\n",
       "      <th>s_vec_3</th>\n",
       "      <th>s_vec_4</th>\n",
       "      <th>s_vec_5</th>\n",
       "      <th>s_vec_6</th>\n",
       "      <th>s_vec_7</th>\n",
       "      <th>s_vec_8</th>\n",
       "      <th>s_vec_9</th>\n",
       "      <th>...</th>\n",
       "      <th>p9_lda_civil</th>\n",
       "      <th>p10_lda_deadites</th>\n",
       "      <th>p11_lda_idea</th>\n",
       "      <th>p12_lda_love</th>\n",
       "      <th>p13_lda_sense</th>\n",
       "      <th>p14_lda_biomolecular</th>\n",
       "      <th>p15_lda_thee</th>\n",
       "      <th>a_num</th>\n",
       "      <th>p_num</th>\n",
       "      <th>s_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.012852</td>\n",
       "      <td>-0.000564</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.015578</td>\n",
       "      <td>-0.009460</td>\n",
       "      <td>-0.001825</td>\n",
       "      <td>0.006729</td>\n",
       "      <td>0.014028</td>\n",
       "      <td>-0.012019</td>\n",
       "      <td>-0.005941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012556</td>\n",
       "      <td>0.012556</td>\n",
       "      <td>0.012556</td>\n",
       "      <td>0.012556</td>\n",
       "      <td>0.267877</td>\n",
       "      <td>0.012556</td>\n",
       "      <td>0.012556</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.014496</td>\n",
       "      <td>-0.015362</td>\n",
       "      <td>-0.008466</td>\n",
       "      <td>0.012413</td>\n",
       "      <td>0.012671</td>\n",
       "      <td>-0.000903</td>\n",
       "      <td>0.015530</td>\n",
       "      <td>-0.005830</td>\n",
       "      <td>-0.007259</td>\n",
       "      <td>0.014899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011455</td>\n",
       "      <td>0.011455</td>\n",
       "      <td>0.147549</td>\n",
       "      <td>0.011455</td>\n",
       "      <td>0.011455</td>\n",
       "      <td>0.011455</td>\n",
       "      <td>0.011455</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.005569</td>\n",
       "      <td>0.010911</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.004631</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.012376</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>-0.012309</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.012049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011455</td>\n",
       "      <td>0.011455</td>\n",
       "      <td>0.147652</td>\n",
       "      <td>0.011455</td>\n",
       "      <td>0.011455</td>\n",
       "      <td>0.011455</td>\n",
       "      <td>0.011455</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.012320</td>\n",
       "      <td>-0.007913</td>\n",
       "      <td>-0.014627</td>\n",
       "      <td>-0.005808</td>\n",
       "      <td>-0.004552</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>-0.012969</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>-0.009987</td>\n",
       "      <td>-0.009841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012644</td>\n",
       "      <td>0.012644</td>\n",
       "      <td>0.012644</td>\n",
       "      <td>0.012644</td>\n",
       "      <td>0.012644</td>\n",
       "      <td>0.012644</td>\n",
       "      <td>0.012644</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001876</td>\n",
       "      <td>-0.003734</td>\n",
       "      <td>-0.006766</td>\n",
       "      <td>-0.008286</td>\n",
       "      <td>0.006277</td>\n",
       "      <td>-0.012269</td>\n",
       "      <td>-0.003850</td>\n",
       "      <td>-0.011094</td>\n",
       "      <td>0.013328</td>\n",
       "      <td>0.011740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015147</td>\n",
       "      <td>0.015147</td>\n",
       "      <td>0.015147</td>\n",
       "      <td>0.015147</td>\n",
       "      <td>0.015148</td>\n",
       "      <td>0.015147</td>\n",
       "      <td>0.015147</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    s_vec_0   s_vec_1   s_vec_2   s_vec_3   s_vec_4   s_vec_5   s_vec_6  \\\n",
       "0  0.012852 -0.000564  0.000924  0.015578 -0.009460 -0.001825  0.006729   \n",
       "1  0.014496 -0.015362 -0.008466  0.012413  0.012671 -0.000903  0.015530   \n",
       "2 -0.005569  0.010911  0.007000  0.004631  0.000567  0.012376  0.001706   \n",
       "3  0.012320 -0.007913 -0.014627 -0.005808 -0.004552  0.002174 -0.012969   \n",
       "4  0.001876 -0.003734 -0.006766 -0.008286  0.006277 -0.012269 -0.003850   \n",
       "\n",
       "    s_vec_7   s_vec_8   s_vec_9  ...    p9_lda_civil  p10_lda_deadites  \\\n",
       "0  0.014028 -0.012019 -0.005941  ...        0.012556          0.012556   \n",
       "1 -0.005830 -0.007259  0.014899  ...        0.011455          0.011455   \n",
       "2 -0.012309  0.000371  0.012049  ...        0.011455          0.011455   \n",
       "3  0.000185 -0.009987 -0.009841  ...        0.012644          0.012644   \n",
       "4 -0.011094  0.013328  0.011740  ...        0.015147          0.015147   \n",
       "\n",
       "   p11_lda_idea  p12_lda_love  p13_lda_sense  p14_lda_biomolecular  \\\n",
       "0      0.012556      0.012556       0.267877              0.012556   \n",
       "1      0.147549      0.011455       0.011455              0.011455   \n",
       "2      0.147652      0.011455       0.011455              0.011455   \n",
       "3      0.012644      0.012644       0.012644              0.012644   \n",
       "4      0.015147      0.015147       0.015148              0.015147   \n",
       "\n",
       "   p15_lda_thee  a_num  p_num  s_num  \n",
       "0      0.012556      0      0      0  \n",
       "1      0.011455      0      1      0  \n",
       "2      0.011455      0      1      1  \n",
       "3      0.012644      0      2      0  \n",
       "4      0.015147      0      3      0  \n",
       "\n",
       "[5 rows x 99 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s_vec_0</th>\n",
       "      <th>s_vec_1</th>\n",
       "      <th>s_vec_2</th>\n",
       "      <th>s_vec_3</th>\n",
       "      <th>s_vec_4</th>\n",
       "      <th>s_vec_5</th>\n",
       "      <th>s_vec_6</th>\n",
       "      <th>s_vec_7</th>\n",
       "      <th>s_vec_8</th>\n",
       "      <th>s_vec_9</th>\n",
       "      <th>...</th>\n",
       "      <th>p9_lda_civil</th>\n",
       "      <th>p10_lda_deadites</th>\n",
       "      <th>p11_lda_idea</th>\n",
       "      <th>p12_lda_love</th>\n",
       "      <th>p13_lda_sense</th>\n",
       "      <th>p14_lda_biomolecular</th>\n",
       "      <th>p15_lda_thee</th>\n",
       "      <th>a_num</th>\n",
       "      <th>p_num</th>\n",
       "      <th>s_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7930</th>\n",
       "      <td>-0.006734</td>\n",
       "      <td>0.011162</td>\n",
       "      <td>0.009488</td>\n",
       "      <td>-0.008400</td>\n",
       "      <td>-0.010261</td>\n",
       "      <td>-0.010349</td>\n",
       "      <td>-0.011043</td>\n",
       "      <td>0.010268</td>\n",
       "      <td>-0.007921</td>\n",
       "      <td>-0.014453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029041</td>\n",
       "      <td>0.201526</td>\n",
       "      <td>16</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7931</th>\n",
       "      <td>0.012661</td>\n",
       "      <td>-0.008760</td>\n",
       "      <td>-0.008574</td>\n",
       "      <td>-0.010942</td>\n",
       "      <td>0.009893</td>\n",
       "      <td>-0.002269</td>\n",
       "      <td>-0.000573</td>\n",
       "      <td>-0.009278</td>\n",
       "      <td>-0.014053</td>\n",
       "      <td>-0.005179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158665</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028997</td>\n",
       "      <td>0.201952</td>\n",
       "      <td>16</td>\n",
       "      <td>83</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7932</th>\n",
       "      <td>-0.015370</td>\n",
       "      <td>0.013310</td>\n",
       "      <td>-0.009113</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>-0.010596</td>\n",
       "      <td>-0.002114</td>\n",
       "      <td>0.003677</td>\n",
       "      <td>0.011908</td>\n",
       "      <td>-0.007208</td>\n",
       "      <td>-0.006414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029009</td>\n",
       "      <td>0.201803</td>\n",
       "      <td>16</td>\n",
       "      <td>83</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7933</th>\n",
       "      <td>-0.002627</td>\n",
       "      <td>-0.011851</td>\n",
       "      <td>0.002627</td>\n",
       "      <td>-0.002608</td>\n",
       "      <td>0.009272</td>\n",
       "      <td>-0.013032</td>\n",
       "      <td>-0.010604</td>\n",
       "      <td>0.009540</td>\n",
       "      <td>-0.007297</td>\n",
       "      <td>-0.007836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012244</td>\n",
       "      <td>0.012244</td>\n",
       "      <td>0.012244</td>\n",
       "      <td>0.466041</td>\n",
       "      <td>0.012244</td>\n",
       "      <td>0.012244</td>\n",
       "      <td>0.247581</td>\n",
       "      <td>16</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7934</th>\n",
       "      <td>-0.004780</td>\n",
       "      <td>-0.000294</td>\n",
       "      <td>-0.002042</td>\n",
       "      <td>0.013785</td>\n",
       "      <td>0.015297</td>\n",
       "      <td>-0.000199</td>\n",
       "      <td>-0.004119</td>\n",
       "      <td>-0.002273</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>-0.005124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012244</td>\n",
       "      <td>0.012244</td>\n",
       "      <td>0.012244</td>\n",
       "      <td>0.465861</td>\n",
       "      <td>0.012244</td>\n",
       "      <td>0.012244</td>\n",
       "      <td>0.247772</td>\n",
       "      <td>16</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       s_vec_0   s_vec_1   s_vec_2   s_vec_3   s_vec_4   s_vec_5   s_vec_6  \\\n",
       "7930 -0.006734  0.011162  0.009488 -0.008400 -0.010261 -0.010349 -0.011043   \n",
       "7931  0.012661 -0.008760 -0.008574 -0.010942  0.009893 -0.002269 -0.000573   \n",
       "7932 -0.015370  0.013310 -0.009113  0.002124 -0.010596 -0.002114  0.003677   \n",
       "7933 -0.002627 -0.011851  0.002627 -0.002608  0.009272 -0.013032 -0.010604   \n",
       "7934 -0.004780 -0.000294 -0.002042  0.013785  0.015297 -0.000199 -0.004119   \n",
       "\n",
       "       s_vec_7   s_vec_8   s_vec_9  ...    p9_lda_civil  p10_lda_deadites  \\\n",
       "7930  0.010268 -0.007921 -0.014453  ...        0.000000          0.000000   \n",
       "7931 -0.009278 -0.014053 -0.005179  ...        0.000000          0.000000   \n",
       "7932  0.011908 -0.007208 -0.006414  ...        0.000000          0.000000   \n",
       "7933  0.009540 -0.007297 -0.007836  ...        0.012244          0.012244   \n",
       "7934 -0.002273  0.001138 -0.005124  ...        0.012244          0.012244   \n",
       "\n",
       "      p11_lda_idea  p12_lda_love  p13_lda_sense  p14_lda_biomolecular  \\\n",
       "7930      0.000000      0.158337       0.000000              0.029041   \n",
       "7931      0.000000      0.158665       0.000000              0.028997   \n",
       "7932      0.000000      0.158550       0.000000              0.029009   \n",
       "7933      0.012244      0.466041       0.012244              0.012244   \n",
       "7934      0.012244      0.465861       0.012244              0.012244   \n",
       "\n",
       "      p15_lda_thee  a_num  p_num  s_num  \n",
       "7930      0.201526     16     83      1  \n",
       "7931      0.201952     16     83      2  \n",
       "7932      0.201803     16     83      3  \n",
       "7933      0.247581     16     84      0  \n",
       "7934      0.247772     16     84      1  \n",
       "\n",
       "[5 rows x 99 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Define X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = prep_train.drop(['a_num'], axis=1)\n",
    "X_test = prep_test.drop(['a_num'], axis=1)\n",
    "y_train = prep_train.a_num\n",
    "y_train_d = pd.get_dummies(prep_train['a_num'])\n",
    "y_test = prep_test.a_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_y = prep_test.a_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5     2407\n",
       "13     720\n",
       "9      663\n",
       "14     549\n",
       "16     515\n",
       "10     438\n",
       "15     410\n",
       "12     346\n",
       "4      333\n",
       "2      329\n",
       "3      307\n",
       "0      230\n",
       "11     175\n",
       "1      160\n",
       "6      131\n",
       "8      130\n",
       "7       92\n",
       "Name: a_num, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_y += 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_y.loc[adj_y[adj_y<=18].index] -= 1 # Sun Tzu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_y.loc[adj_y[adj_y<=8].index] -= 1 # Khayyam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_y.loc[adj_y[adj_y<=4].index] -= 1 # Hobbes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  5,  6,  7,  9, 10, 11, 12, 13, 14, 15, 16, 17, 19])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_d = pd.get_dummies(adj_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_nums = [4,8,18]\n",
    "for num in add_nums:\n",
    "    y_test_d[num] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>19</th>\n",
       "      <th>4</th>\n",
       "      <th>8</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   5   6   7   9   10  11  12  13  14  15  16  17  19  4   8   \\\n",
       "0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "2   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "3   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "4   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
       "\n",
       "   18  \n",
       "0   0  \n",
       "1   0  \n",
       "2   0  \n",
       "3   0  \n",
       "4   0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_d = y_test_d[list(range(20))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7935, 98)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7935,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Standardize and Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize X Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63932, 98)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7935, 98)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_sc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63932,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: a_num, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7935,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: a_num, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63932,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {i:val for i, val in y_train.value_counts(sort=False).iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1111,\n",
       " 1: 1955,\n",
       " 2: 1205,\n",
       " 3: 569,\n",
       " 4: 6160,\n",
       " 5: 1566,\n",
       " 6: 2492,\n",
       " 7: 6067,\n",
       " 8: 224,\n",
       " 9: 8417,\n",
       " 10: 1159,\n",
       " 11: 7556,\n",
       " 12: 5497,\n",
       " 13: 3982,\n",
       " 14: 2631,\n",
       " 15: 3171,\n",
       " 16: 3426,\n",
       " 17: 3910,\n",
       " 18: 1715,\n",
       " 19: 1119}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Logistic Regression Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_runs = pd.DataFrame(columns=['train_accuracy','test_accuracy','params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_single = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(penalty='l2', random_state=211, n_jobs=3, \n",
    "                            multi_class='multinomial', solver='lbfgs', \n",
    "                            class_weight='balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63932, 98)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63932,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='multinomial', n_jobs=3, penalty='l2',\n",
       "          random_state=211, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "          warm_start=False)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_single['train_accuracy'] = logreg.score(X_train_sc, y_train)\n",
    "lr_single['test_accuracy'] = logreg.score(X_test_sc, y_test)\n",
    "lr_single['params'] = {'penalty':logreg.penalty, 'C':logreg.C, 'solver':logreg.solver}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_accuracy': 0.5724832634674342,\n",
       " 'test_accuracy': 0.14984247006931317,\n",
       " 'params': {'penalty': 'l2', 'C': 1.0, 'solver': 'lbfgs'}}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Logistic Regression Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_single = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {\n",
    "    'penalty':['l2'],\n",
    "    'C':np.arange(.005, .05, .001),\n",
    "    'solver':['lbfgs'],\n",
    "    'class_weight':['balanced'],\n",
    "    'multi_class':['multinomial'],\n",
    "    'n_jobs':[3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(logreg, lr_params, cv=3)\n",
    "grid.fit(X_train_sc, y_train)\n",
    "lr_single['train_accuracy'] = grid.score(X_train_sc, y_train)\n",
    "lr_single['test_accuracy'] = grid.score(X_test_sc, y_test)\n",
    "lr_single['params'] = grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_runs = lr_runs.append(lr_single, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.569199</td>\n",
       "      <td>0.148834</td>\n",
       "      <td>{'C': 0.1, 'class_weight': 'balanced', 'multi_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.553588</td>\n",
       "      <td>0.144549</td>\n",
       "      <td>{'C': 0.01, 'class_weight': 'balanced', 'multi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.546049</td>\n",
       "      <td>0.146314</td>\n",
       "      <td>{'C': 0.005, 'class_weight': 'balanced', 'mult...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_accuracy  test_accuracy  \\\n",
       "0        0.569199       0.148834   \n",
       "1        0.553588       0.144549   \n",
       "2        0.546049       0.146314   \n",
       "\n",
       "                                              params  \n",
       "0  {'C': 0.1, 'class_weight': 'balanced', 'multi_...  \n",
       "1  {'C': 0.01, 'class_weight': 'balanced', 'multi...  \n",
       "2  {'C': 0.005, 'class_weight': 'balanced', 'mult...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Logistic Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=0.1, penalty='l2', random_state=211, n_jobs=3, \n",
    "                            multi_class='multinomial', solver='lbfgs', \n",
    "                            class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='multinomial', n_jobs=3, penalty='l2',\n",
       "          random_state=211, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "          warm_start=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14883427851291745"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Final Logistic Regression to Disk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/logreg']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(logreg, '../models/logreg')\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "#logreg = joblib.load('./models/logreg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Recurrent Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Keras FFRNN Assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNN Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_pct = .25\n",
    "dense_1_nodes = 64\n",
    "dense_2_nodes = 128\n",
    "dense_3_nodes = 64\n",
    "target_nodes = 20\n",
    "epochs = 50\n",
    "batch_size = 1000\n",
    "patience = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(X_train_sc.shape[0], input_dim=X_train_sc.shape[1], activation='relu'))\n",
    "model.add(Dropout(dropout_pct))\n",
    "model.add(Dense(dense_1_nodes, activation='relu'))\n",
    "model.add(Dropout(dropout_pct))\n",
    "model.add(Dense(dense_2_nodes, activation='relu'))\n",
    "model.add(Dropout(dropout_pct))\n",
    "model.add(Dense(dense_3_nodes, activation='relu'))\n",
    "model.add(Dense(y_test_d.shape[1], activation=None))\n",
    "model.add(Activation(tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=patience, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/50\n",
      "74115/74115 [==============================] - 14s 183us/step - loss: 8622.3826 - acc: 0.4038 - val_loss: 3.2356 - val_acc: 0.1067\n",
      "Epoch 2/50\n",
      "74115/74115 [==============================] - 10s 131us/step - loss: 6614.3858 - acc: 0.5113 - val_loss: 3.1147 - val_acc: 0.1380\n",
      "Epoch 3/50\n",
      "74115/74115 [==============================] - 10s 132us/step - loss: 5885.9867 - acc: 0.5546 - val_loss: 3.2827 - val_acc: 0.1306\n",
      "Epoch 4/50\n",
      "74115/74115 [==============================] - 10s 132us/step - loss: 5388.9272 - acc: 0.5817 - val_loss: 3.0534 - val_acc: 0.1451\n",
      "Epoch 5/50\n",
      "74115/74115 [==============================] - 10s 131us/step - loss: 5059.6172 - acc: 0.6020 - val_loss: 3.2585 - val_acc: 0.1298\n",
      "Epoch 6/50\n",
      "74115/74115 [==============================] - 10s 131us/step - loss: 4698.6653 - acc: 0.6197 - val_loss: 3.2792 - val_acc: 0.1545\n",
      "Epoch 7/50\n",
      "74115/74115 [==============================] - 10s 131us/step - loss: 4413.7012 - acc: 0.6380 - val_loss: 3.4277 - val_acc: 0.1328\n",
      "Epoch 8/50\n",
      "74115/74115 [==============================] - 10s 132us/step - loss: 4211.0359 - acc: 0.6463 - val_loss: 3.2020 - val_acc: 0.1528\n",
      "Epoch 9/50\n",
      "74115/74115 [==============================] - 10s 132us/step - loss: 3955.4013 - acc: 0.6633 - val_loss: 3.6029 - val_acc: 0.1324\n",
      "Epoch 10/50\n",
      "74115/74115 [==============================] - 10s 132us/step - loss: 3764.2633 - acc: 0.6727 - val_loss: 3.4506 - val_acc: 0.1619\n",
      "Epoch 11/50\n",
      "74115/74115 [==============================] - 10s 132us/step - loss: 3586.2105 - acc: 0.6848 - val_loss: 3.5547 - val_acc: 0.1402\n",
      "Epoch 12/50\n",
      "74115/74115 [==============================] - 10s 132us/step - loss: 3452.7258 - acc: 0.6920 - val_loss: 3.6493 - val_acc: 0.1400\n",
      "Epoch 13/50\n",
      "74115/74115 [==============================] - 10s 132us/step - loss: 3227.2560 - acc: 0.7047 - val_loss: 3.8199 - val_acc: 0.1564\n",
      "Epoch 14/50\n",
      "74115/74115 [==============================] - 10s 132us/step - loss: 3121.9425 - acc: 0.7140 - val_loss: 3.8578 - val_acc: 0.1444\n",
      "Epoch 15/50\n",
      "74115/74115 [==============================] - 10s 133us/step - loss: 2935.9542 - acc: 0.7223 - val_loss: 3.9256 - val_acc: 0.1343\n",
      "Epoch 16/50\n",
      "74115/74115 [==============================] - 10s 133us/step - loss: 2943.6133 - acc: 0.7272 - val_loss: 3.9985 - val_acc: 0.1405\n",
      "Epoch 17/50\n",
      "74115/74115 [==============================] - 10s 132us/step - loss: 2814.8271 - acc: 0.7350 - val_loss: 3.8882 - val_acc: 0.1336\n",
      "Epoch 18/50\n",
      "74115/74115 [==============================] - 10s 133us/step - loss: 2626.0188 - acc: 0.7440 - val_loss: 4.0577 - val_acc: 0.1280\n",
      "Epoch 19/50\n",
      "74115/74115 [==============================] - 10s 132us/step - loss: 2631.4029 - acc: 0.7451 - val_loss: 4.0391 - val_acc: 0.1459\n",
      "Epoch 20/50\n",
      "74115/74115 [==============================] - 10s 132us/step - loss: 2415.7657 - acc: 0.7574 - val_loss: 4.2796 - val_acc: 0.1422\n",
      "Epoch 21/50\n",
      "74115/74115 [==============================] - 10s 132us/step - loss: 2302.3506 - acc: 0.7653 - val_loss: 4.3062 - val_acc: 0.1303\n",
      "Epoch 22/50\n",
      "74115/74115 [==============================] - 10s 132us/step - loss: 2239.9174 - acc: 0.7697 - val_loss: 4.2259 - val_acc: 0.1451\n",
      "Epoch 23/50\n",
      "74115/74115 [==============================] - 10s 132us/step - loss: 2334.4152 - acc: 0.7653 - val_loss: 4.3964 - val_acc: 0.1308\n",
      "Epoch 24/50\n",
      "74115/74115 [==============================] - 10s 133us/step - loss: 2247.8830 - acc: 0.7699 - val_loss: 4.3623 - val_acc: 0.1414\n",
      "Epoch 00024: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_sc, \n",
    "                    y_train_d, \n",
    "                    validation_data=(X_test_sc, y_test_d), \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[early_stop], \n",
    "                    class_weight=class_weights\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With Class Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe4a1f179e8>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8FfW9//HXJxuBkIVsJBB2MBC2gGFRURZRXGppRa11rdoHt7dWu3lvua33Z6u3vWpbt7q0tNK6oLhQK60LogW9SAUCiuwSWROzkbAFCHByvr8/zkBTBRJCkpOceT8fjzwyZ84snxnCeZ/5zsx3zDmHiIj4T1S4CxARkfBQAIiI+JQCQETEpxQAIiI+pQAQEfEpBYCIiE8pAEREfEoBICLiUwoAERGfigl3ASeTnp7uevfuHe4yRETalRUrVux0zmU0NF2bDoDevXtTWFgY7jJERNoVM9vWmOnUBCQi4lMKABERn1IAiIj4VJs+ByAibdORI0coLi6mtrY23KX4Wnx8PDk5OcTGxjZpfgWAiJyy4uJiEhMT6d27N2YW7nJ8yTlHVVUVxcXF9OnTp0nLUBOQiJyy2tpa0tLS9OEfRmZGWlraaR2FKQBEpEn04R9+p/tvEJEBULL7IPe9uYHSPQfDXYqISJsVkQGw/1CAJxZ9ysINleEuRUSaWVVVFfn5+eTn55OVlUX37t2PvT58+HCjlnHTTTexcePGk07z2GOPMXv27OYomXHjxvHRRx81y7KaU0SeBB6Q2ZluyfEs2ljBNWN6hrscEWlGaWlpxz5Mf/rTn9K5c2fuuOOOf5nGOYdzjqio43/H/eMf/9jgem699dbTL7aNi8gjADNjwsBM3i/ayeFAMNzliEgrKCoqIi8vj2uvvZbBgwdTWlrK9OnTKSgoYPDgwdx9993Hpj36jTwQCJCSksKMGTMYPnw4Z511FhUVFQDceeedPPTQQ8emnzFjBqNHjyY3N5clS5YAsH//fqZNm0ZeXh5XXHEFBQUFDX7Tf/bZZxk6dChDhgzhxz/+MQCBQIDrr7/+2PhHHnkEgAcffJC8vDyGDRvGdddd1+z7LCKPAAAm5mby3NLtFG6t5uz+6eEuRyRi/eyva1n32d5mXWZetyTuumzwKc+3YcMGnn76aQoKCgC49957SU1NJRAIMHHiRK644gry8vL+ZZ49e/Ywfvx47r33Xn7wgx8wa9YsZsyY8YVlO+dYtmwZ8+bN4+677+bNN9/kN7/5DVlZWcydO5dVq1YxcuTIk9ZXXFzMnXfeSWFhIcnJyUyePJm//e1vZGRksHPnTlavXg3A7t27Abj//vvZtm0bcXFxx8Y1p4g8AgA4u18acdFRLNxYEe5SRKSV9OvX79iHP8Dzzz/PyJEjGTlyJOvXr2fdunVfmKdjx45cfPHFAJx55pls3br1uMu+/PLLvzDN4sWLufrqqwEYPnw4gwefPLSWLl3KpEmTSE9PJzY2lmuuuYb33nuP/v37s3HjRm6//Xbmz59PcnIyAIMHD+a6665j9uzZTb7Z62Qi9gggoUMMo/uksnBjJT+5NNzViESupnxTbykJCQnHhjdt2sTDDz/MsmXLSElJ4brrrjvuNfNxcXHHhqOjowkEAsdddocOHRqcpqnS0tL4+OOPeeONN3jssceYO3cuM2fOZP78+bz77rvMmzePX/ziF3z88cdER0c323oj9ggAYEJuBkUVNeyoPhDuUkSkle3du5fExESSkpIoLS1l/vz5zb6Oc845hxdffBGA1atXH/cIo74xY8awcOFCqqqqCAQCzJkzh/Hjx1NZWYlzjiuvvJK7776blStXUldXR3FxMZMmTeL+++9n586dHDjQvJ9lEXsEADAhN5P/eW09iz6p5PqxvcJdjoi0opEjR5KXl8fAgQPp1asX55xzTrOv47bbbuOGG24gLy/v2M/R5pvjycnJ4Z577mHChAk457jsssu49NJLWblyJbfccgvOOcyM++67j0AgwDXXXMO+ffsIBoPccccdJCYmNmv95pxr1gU2p4KCAnc6D4RxznHu/QsZmJXIH24c1YyVifjb+vXrGTRoULjLCLtAIEAgECA+Pp5NmzZx4YUXsmnTJmJiWu+79fH+LcxshXOu4ASzHBPRRwBmxsTcTF5eUUztkTriY5uv7UxEpKamhvPPP59AIIBzjt/97net+uF/utpPpU00cWAGz3ywjeVbqzl3QIOPyBQRabSUlBRWrFgR7jKarFEngc3s+2a21szWmNnzZhZvZn3MbKmZFZnZC2YW503bwXtd5L3fu95y/ssbv9HMprTMJv2rs/qmExcTpW4hREQ+p8EAMLPuwO1AgXNuCBANXA3cBzzonOsP7AJu8Wa5BdjljX/Qmw4zy/PmGwxcBDxuZi3eJtMxLpqxfdNY9InuBxARqa+xl4HGAB3NLAboBJQCk4CXvfefAr7iDU/1XuO9f76F+iydCsxxzh1yzm0BioDRp78JDZtwRgabK/ezrWp/a6xORKRdaDAAnHMlwK+A7YQ++PcAK4Ddzrmjd0MUA9294e7ADm/egDd9Wv3xx5mnRU0cmAnAoo1qBhIROaoxTUBdCH177wN0AxIINeG0CDObbmaFZlZYWdk8H9h90hPondaJReoWQqTda47uoAFmzZpFWVnZsdeN6SK6MY52MNceNOYqoMnAFudcJYCZ/Rk4B0gxsxjvW34OUOJNXwL0AIq9JqNkoKre+KPqz3OMc24mMBNC9wE0ZaOOZ0JuJs8v267LQUXaucZ0B90Ys2bNYuTIkWRlZQGN6yI60jTmHMB2YKyZdfLa8s8H1gELgSu8aW4EXvWG53mv8d7/uwvdbTYPuNq7SqgPMABY1jyb0bAJuRkcCgT5YHNVa61SRFrZU089xejRo8nPz+fb3/42wWDwuF0tv/DCC3z00Ud87WtfO3bk0Jguojdt2sSYMWMYOnQoP/nJTxr8ph8MBvnBD37AkCFDGDp0KC+/HDptWlJSwrhx48jPz2fIkCEsWbLkhF1Ct6QGjwCcc0vN7GVgJRAAPiT0Df01YI6Z/Y837klvlieBZ8ysCKgmdOUPzrm1ZvYiofAIALc65+qaeXtOaGzfNDrERLFoYyUTcjNba7Uike+NGVC2unmXmTUULr73lGZZs2YNr7zyCkuWLCEmJobp06czZ84c+vXr94WullNSUvjNb37Do48+Sn5+/heWdaIuom+77TbuuOMOrrzySh599NEGa3rppZdYv349q1atorKyklGjRnHeeefx7LPPctlll/GjH/2Iuro6Dh48yIoVK47bJXRLatRVQM65u5xzA51zQ5xz13tX8mx2zo12zvV3zl3pnDvkTVvrve7vvb+53nJ+7pzr55zLdc690VIbdTzxsdGc3S9N5wFEItTbb7/N8uXLKSgoID8/n3fffZdPP/30hF0tn8yJuoheunQp06ZNA+Caa65pcDmLFy/m61//OtHR0WRlZTFu3DgKCwsZNWoUf/jDH/jZz37GmjVr6Ny5c5PqPF0RfydwfRNyM1m4cS1bdu6nT3pCwzOISMNO8Zt6S3HOcfPNN3PPPfd84b3jdbV8Mo3tIrqpJk2axKJFi3jttde44YYb+M///E+uvfbaU67zdEV0d9CfN9Fr+lm4QUcBIpFm8uTJvPjii+zcuRMIXS20ffv243a1DJCYmMi+fftOaR2jR4/mlVdeAWDOnDkNTn/uuecyZ84cgsEg5eXlvP/++xQUFLBt2zaysrKYPn06N910Ex9++OEJ62xJvjoC6JnWib4ZCSz6pJKbx/UJdzki0oyGDh3KXXfdxeTJkwkGg8TGxvLb3/6W6OjoL3S1DKHLPr/5zW/SsWNHli1r3PUojzzyCNdffz0/+9nPmDJlSoPNNFdccQUffPABw4YNw8x44IEHyMzMZNasWTzwwAPExsaSmJjIM888w44dO45bZ0uK6O6gj+fuv67j2aXbWPX/LqRjnC4HFWkKv3YHvX//fjp16oSZ8eyzz/LKK68wd+7csNZ0Ot1B+6oJCEK9gx4OBPnH5p3hLkVE2pnly5czYsQIhg0bxu9//3t++ctfhruk0+KrJiCA0X1S6RgbzcINlUwa2DXc5YhIOzJhwoRjN6FFAt8dAXSIieac/mks3FhBW27+Emnr9P8n/E7338B3AQAwPjeT4l0H+bRSvYOKNEV8fDxVVVUKgTByzlFVVUV8fHyTl+G7JiAIdQ8NsGhjBf0zO4e5GpH2Jycnh+LiYpqrw0Zpmvj4eHJycpo8vy8DoEdqJwZkdmbRxkq+eW7fcJcj0u7ExsbSp48upW7vfNkEBKHO4ZZtqWb/oea9w09EpL3wbQBMzM3kcF2QJZ+qd1AR8SffBkBB71QS4qJZqM7hRMSnfBsAcTFRnNM/nXc3VupKBhHxJd8GAIR6By3ZfZBNFTXhLkVEpNX5PAD+eTmoiIjf+DoAuqV0ZGBWIgs36FpmEfEfXwcAwPjcDAq3VbOv9ki4SxERaVW+D4CJuZkcqXO8X6TLQUXEX3wfAGf26kJihxidBxAR3/F9AMRGRzFuQDqLdDmoiPiM7wMAQlcDle2tZUPZqT0fVESkPVMAELofAGDRRl0NJCL+oQAAuibFk5edpG4hRMRXFACeCbkZrNi2i726HFREfEIB4Jk4MJO6oOP/PtHD4kXEHxQAnhE9UshM7MCvF2zUTWEi4gsKAE9MdBSPfH0E26oO8MMXVxEM6pJQEYlsCoB6xvZN48eXDOKtdeU8vqgo3OWIiLQoBcDn3HxOb6bmd+PXCz7RVUEiEtEUAJ9jZtx7+TByuyby3ec/ZFvV/nCXJCLSIhQAx9ExLpqZ1xdgZvzbMys4cFgPjheRyKMAOIGeaZ14+Op8NpbvY8bc1eonSEQijgLgJCbkZnLHhbnMW/UZTy7eEu5yRESalQKgAd+e0I8pg7vyv29sYMmnuklMRCKHAqABZsavrhxO77RO3Pbch3y2+2C4SxIRaRYKgEZIjI/ld9cXcCgQ5FvPrqD2SF24SxIROW2NCgAzSzGzl81sg5mtN7OzzCzVzBaY2SbvdxdvWjOzR8ysyMw+NrOR9ZZzozf9JjO7saU2qiX0z+zMr68azsfFe/h/r67RSWERafcaewTwMPCmc24gMBxYD8wA3nHODQDe8V4DXAwM8H6mA08AmFkqcBcwBhgN3HU0NNqLKYOz+M7E/rxYWMxzy7aHuxwRkdPSYACYWTJwHvAkgHPusHNuNzAVeMqb7CngK97wVOBpF/IBkGJm2cAUYIFzrto5twtYAFzUrFvTCr5/wRmMPyODn85by4ptu8JdjohIkzXmCKAPUAn80cw+NLM/mFkC0NU5V+pNUwZ09Ya7AzvqzV/sjTvR+H9hZtPNrNDMCisr294TuqKjjEeuHkF2cke+PXsFFftqw12SiEiTNCYAYoCRwBPOuRHAfv7Z3AOACzWIN0ujuHNupnOuwDlXkJGR0RyLbHbJnWL53fVnsufgEW6dvZLDgWC4SxIROWWNCYBioNg5t9R7/TKhQCj3mnbwfh/tOa0E6FFv/hxv3InGt0uDspO4b9owlm/dxZ1/WU2duo8WkXamwQBwzpUBO8ws1xt1PrAOmAccvZLnRuBVb3gecIN3NdBYYI/XVDQfuNDMungnfy/0xrVbU/O7c9uk0Enh6U8Xsv+Q+gwSkfYjppHT3QbMNrM4YDNwE6HweNHMbgG2AVd5074OXAIUAQe8aXHOVZvZPcByb7q7nXPVzbIVYfTDC3PJTOzAXfPWctXv/sGsb4yia1J8uMsSEWmQteXr2QsKClxhYWG4y2iUhRsq+M5zK0nqGMusb4xiUHZSuEsSEZ8ysxXOuYKGptOdwM1k4sBMXvzWWTgHV/72HyzSw2REpI1TADSjwd2SeeXWs+mR2olbnipk9tJt4S5JROSEFADNLDu5Iy996yzOG5DOT15Zwy9eX68HzItIm6QAaAGdO8Tw+xsKuH5sL2a+t5lbn1upDuREpM1RALSQmOgo7p46mDsvHcSba8u4euYHVO47FO6yRESOUQC0IDPjm+f25Ylrz2RD2V6++vj7FFXsC3dZIiKAAqBVXDQkixemn0XtkSCXP75ETxYTkTZBAdBKhvdI4ZVvn03XpHhueHIZLyzfrmcKiEhYKQBaUY/UTrz872cztm8aP5q7mqtnfsCakj3hLktEfEoB0MqSO8byp5tGcc9XhvBJ+T4ue3QxM+Z+rBPEItLqFABhEBMdxfVje7HoPyZyyzl9eHlFMRN/tYgnFn2qy0VFpNUoAMIouWMsd34pj7e+fx5j+6Zx35sbuODBd3ljdanOD4hIi1MAtAF9MzrzhxsLePaWMXSKjeHfZ6/U+QERaXEKgDZk3IB0Xrt9HD//6hA2VdRw2aOL+dHLH+uxkyLSIhQAbUxMdBTXjunFwjsm8M1xffjzh8VM/OUiHl9UpPMDItKsFABtVHLHWH5yaR5vfX88Z/dP5/43N3LBg+/y9w3l4S5NRCKEAqCN65OewO9vKOC5b44hPiaam/9UyK2zV1K+V81CInJ6FADtxNn903nt9nP5jym5vL2+nMm/fpdn/rFVD6MXkSZTALQjcTFR3DqxP/O/dx75PVP471fXMu2JJaz7bG+4SxORdkgB0A71Tk/g6ZtH8/DV+RTvOsBljy7mf19fz4HDgXCXJiLtiAKgnTIzpuZ35+0fjOeqghx+995mLnjgPRZu0LOIRaRxFADtXEqnOP738mG89K2z6BQXzU1/Ws6ts1dSoZPEItIABUCEGNU7ldduP5c7LjyDBevLOf/X7/LMB9v0PGIROSEFQASJi4niO5MG8Nb3zmN4jxT++y9rmPbbJazcvivcpYlIG6QAiEC90xN45pbRPPS1fHZUH+Dyx5dw/ZNLKdxaHe7SRKQNsbbc62RBQYErLCwMdxnt2v5DAZ79YBsz39tM1f7DnN0vje+eP4AxfdPCXZqItBAzW+GcK2hwOgWAPxw4HOC5pdv57bub2VlziDF9Uvnu5AGc1TcNMwt3eSLSjBQAcly1R+p4ftl2nlj0KRX7DjGqdxduP38A4/qnKwhEIoQCQE6q9kgdLxbu4PGFn1K2t5aRPVO4/fwBjD8jQ0Eg0s4pAKRRDgXqeLGwmCcWFvHZnlqG90jhu+f3Z2JupoJApJ1SAMgpORwIMndlMY8tLKJ410H6ZSQwNb87Xx7ejd7pCeEuT0ROgQJAmuRIXZC/fFjCSyuKWbYldNno8B4pTB3ejS8NzyYzMT7MFYpIQxQActo+232Qv676jFc/+ox1pXuJMji7Xzpfzu/GRUOySIqPDXeJInIcCgBpVpvK9zFv1WfMW/UZ26oOEBcTxaTcTKbmd2PiwEziY6PDXaKIeBQA0iKcc6wq3sOrH5Xw11Wl7Kw5RGKHGKYMyeLyEd05q5/uKxAJNwWAtLhAXZAPNlfz6kclvLmmjH2HAgzMSuTmcX348vBuOioQCRMFgLSq2iN1/HXVZzy5eAsbyvaR3jmO68b24rqxvUjv3CHc5Yn4SmMDoNGdwZlZtJl9aGZ/8173MbOlZlZkZi+YWZw3voP3ush7v3e9ZfyXN36jmU059c2Stio+NporC3rwxnfPZfY3xzC0ezIPvb2Js+/9Oz96+WM+Kd8X7hJF5HNiTmHa7wLrgSTv9X3Ag865OWb2W+AW4Anv9y7nXH8zu9qb7mtmlgdcDQwGugFvm9kZzrm6ZtoWaQPMjHP6p3NO/3SKKmr44/tbmLuymBcKd3DugHRuGddHdxuLtBGNOgIwsxzgUuAP3msDJgEve5M8BXzFG57qvcZ7/3xv+qnAHOfcIefcFqAIGN0cGyFtU//Mzvz8q0P5x4zz+Y8puWws28c3/ricCx98j+eXbaf2iLJfJJwa2wT0EPCfQNB7nQbsds4dfQp5MdDdG+4O7ADw3t/jTX9s/HHmOcbMpptZoZkVVlZWnsKmSFvVJSGOWyf2Z/GPJvHAVcOJjY7iv/68mrPv/Tu/mr+RooqacJco4ksNNgGZ2ZeACufcCjOb0NIFOedmAjMhdBK4pdcnrScuJorLR+bw1RHd+WBzNU8u3sJji4p4dGERuV0TuWRoNpcMzWJA18RwlyriC405B3AO8GUzuwSIJ3QO4GEgxcxivG/5OUCJN30J0AMoNrMYIBmoqjf+qPrziI+YGWf1S+OsfmmU7anljTWlvLG6jIfe+YQH3/6EAZmduWRoNpcOy+YMhYFIizmly0C9I4A7nHNfMrOXgLn1TgJ/7Jx73MxuBYY6577lnQS+3Dl3lZkNBp4j1O7fDXgHGHCyk8C6DNRfyvfW8uaaMl5fXcqyrdU4FzqPcMmQLC4Zlk1u10SdPBZphBa5D+BzAdAXmAOkAh8C1znnDplZPPAMMAKoBq52zm325v8JcDMQAL7nnHvjZOtTAPhXxb5a5q8p47XVpSzbUk3QQd+MBC4dms0lQ7MZmKUwEDkR3QgmEaNy3yHmrw0dGXywuYqgg56pnbggrysX5HWloFcXYqIbfUuLSMRTAEhE2llziLfWlrNgXRnvf1rF4UCQlE6xTBqYyYV5XTl3QAYJHU7l9haRyKMAkIi3/1CA9z6pZMG6cv6+sYLdB44QFxPFuP7pTB7Ulcl5mXp+gfiSAkB8JVAXZPnWXSxYV86C9WXsqD4IQH6PFC7I68qFeV3pn9lZ5w3EFxQA4lvOOTaW72PB2nIWrC/n4+I9AGQnx3Nmry4U9OpCQe9UBmYl6tyBRCQFgIinbE8tb68vZ+mWagq3VlO6pxaAhLhoRvTsEgqF3l0Y0bMLnXX+QCKAAkDkBEp2H6RwazUrtu1i+dZdbCzbS9BBlMGg7CQKenXhzN6pFPTqQreUjuEuV+SUKQBEGmlf7RE+3L6bwm27KNxazUc7dnPgcOj+xL4ZCVxV0INpI3PISNRzDaR9UACINFGgLsj60n0s31rNm2vKWLa1mpgoY/KgrnxtVA/OOyOD6CidTJa2SwEg0kyKKmp4qXAHL68opmr/YbKT47nyzByuLOhBj9RO4S5P5AsUACLN7HAgyN83lDNn+Q7e/STUVfm4/ulcPaonk/My6RCjZyBL26AAEGlBJbsP8lLhDl4qLKZk90FSE+K4fER3vjaqh7qzlrBTAIi0grqgY3HRTl5Yvp0F68o5UucYlpNMfo8UBmYlMSg7kdysRDrF6fJSaT2NDQD9VYqchugoY/wZGYw/I4OdNYd4ZWUJ89eW8eeVJdQc2gaAGfRJS2BgdiKDspIYlJ3EoG5JdEuO153JElY6AhBpAc45incdZF3pXjaU7mN96V7Wl+1lW9WBY9MkxccwMDuJvOzQkcJ5Z2SQnaz7DuT06QhAJIzMjB6pneiR2okpg7OOja85FGBjmRcI3s9LhTvY7913kN8jhYuHZHHRkCx6pSWEq3zxCR0BiIRZMOj4tLKGt9aV8+aaMlaXhPouGpSdxEWDs7h4aBYD1JGdnAKdBBZpp4p3HeDNNWXMX1tG4bZdOAd90xO4yDsyGNo9WWEgJ6UAEIkAFXtrmb+unPlryvjH5irqgo7uKR2ZMjgUBj1SO1IXdASDEAgGCTpHIOi+MK7u6HAQuiTEkttVPaFGMgWASITZtf8wb68vZ/7aMt7btJPDgWCTl9UpLpphOcmM9HpDHdGzC6kJcc1YrYSTAkAkgtUcCvB/n1Sy++ARoqOMaLPQ7/o/nxsX5b0u3XOQldt2sXL7btaV7qUuGPoM6JOewIieKYzs2YWRPbuQm5WoPo/aKQWAiDTo4OE6Pi7ezcrtu1m5fRcrt+2iav9hIPS8hOE9UjizVxdG9U5lbN804mLUbNQe6DJQEWlQx7hoxvRNY0zfNCB0/8L26gNeGIRC4fFFn1IXLCIxPoYLBnXl4qHZnDsgnfhY9X3U3ikAROQYM6NXWgK90hL46ogcAPYfCvDB5ireWFPGW2vL+POHJSTERXP+oK5cMjSL8Wdk0jFOYdAeqQlIRBrtcCDIPzZX8cbqUt5aV071/sN0jI1m4sAMLh6SzaSBmSTosZphp3MAItKiAnVBlm2p5vU1pby5ppydNYfoEBPF+DMyuHhoFucP6kpihxicI3QpqnM4F+pAL+gcQRe6Ce7YsPd+ZmIHonTy+bQoAESk1dQFHSu27eL11aW8uaaMsr21TV5W95SOTDszh2kju6s7jCZSAIhIWASDjg937GZJ0U4CQUeUGVEGUd6lqFEW6kXVzIj2xoeGjSN1Qd5eX87iop04B6N7pzLtzO5cMjSbxPjYcG9au6EAEJF2q3TPQV75sISXVxSzuXI/8bFRXDwkm2kjczi7X5qaiBqgABCRds+50NHE3BXFzFv1GftqA3RLjufykTlMOzOHPulqIjoeBYCIRJTaI3W8vb6cl1cU894nlQQdnNmrC9NG5jBlcFfSOncId4lthgJARCJW+d5a/uI1EW2qqAGgW3I8g7snM6RbMkNzkhjSLZnMpPgwVxoeCgARiXjOOVaX7GHp5mpWl+xhzWd72LJzP0c/1jISOzC0ezJDuiUxuHsyQ7snk91Kj+IMBh2f7TnI1p0H6JIQS5/0hFZ7NrS6ghCRiGdmDMtJYVhOyrFxNYcCrPtsL2u8QFhbspdFGyvw+rwjNSGOwd2SGJiVSHZyR7KT48nyfjI6dzjlbrLrgo7iXQfYVF7DpooaNlXso6iihqKKGg54T3o7Kjs5nj7pCfTNSKBPemf6esPdUzqGpXtuHQGISMQ7eLiO9WV7WVuyJ3SkULKXosqaL3SpHWWQmRhP1+R4spNCoXAsIJLiSeoYy7aq/fU+7GvYXFnDoXrLyUqKZ0DXzvTP7MyAzER6p3di94EjbK6sYfPO/Wyu3M/myhr21gaOzRMbHeqC42g49E1PYHC3ZIZ0T27S9uoIQETE0zEu+lg310c559h14Ahle2op23uQ0j21oeE9tZTtraWosobFRTupORQ47jJzunRkQGZnzh2QTv/Mzsd+khpxv4Jzjur9h9lyNBB27mfLzho2V+7n3Y2VHK4Lctnwbvzm6yOabR8cjwJARHzJzEhNiCM1IY68bkknnG5f7RHK99ZSuqeWPQeP0Cs1gX6Zp9eeb2akde5AWucOFPRO/Zf36oKOkl0HCbZ+jSJpAAAGnUlEQVRC60yDW2BmPYCnga6AA2Y65x42s1TgBaA3sBW4yjm3y0JnVx4GLgEOAN9wzq30lnUjcKe36P9xzj3VvJsjItK8EuNjSYyPpX9mYqusLzrK6JnWqVXW1ZizDgHgh865PGAscKuZ5QEzgHeccwOAd7zXABcDA7yf6cATAF5g3AWMAUYDd5lZF0REJCwaDADnXOnRb/DOuX3AeqA7MBU4+g3+KeAr3vBU4GkX8gGQYmbZwBRggXOu2jm3C1gAXNSsWyMiIo12StcdmVlvYASwFOjqnCv13ioj1EQEoXDYUW+2Ym/cicaLiEgYNDoAzKwzMBf4nnNub/33XOha0mY5Y2Fm082s0MwKKysrm2ORIiJyHI0KADOLJfThP9s592dvdLnXtIP3u8IbXwL0qDd7jjfuROP/hXNupnOuwDlXkJGRcSrbIiIip6DBAPCu6nkSWO+ce6DeW/OAG73hG4FX642/wULGAnu8pqL5wIVm1sU7+XuhN05ERMKgMReyngNcD6w2s4+8cT8G7gVeNLNbgG3AVd57rxO6BLSI0GWgNwE456rN7B5guTfd3c656mbZChEROWXqCkJEJMI0tiuI1u99SERE2gQFgIiITykARER8SgEgIuJTCgAREZ9SAIiI+JQCQETEpxQAIiI+pQAQEfEpBYCIiE8pAEREfEoBICLiUwoAERGfUgCIiPiUAkBExKcUACIiPqUAEBHxKQWAiIhPKQBERHxKASAi4lMKABERn1IAiIj4lAJARMSnFAAiIj6lABAR8SkFgIiITykARER8SgEgIuJTCgAREZ9SAIiI+JQCQETEpxQAIiI+pQAQEfEpBYCIiE8pAEREfEoBICLiUwoAERGfavUAMLOLzGyjmRWZ2YzWXr+IiIS0agCYWTTwGHAxkAd83czyWrMGEREJae0jgNFAkXNus3PuMDAHmNrKNYiICBDTyuvrDuyo97oYGNPsaylfCy99o9kX2zR2itO75l29a+blAVgjt+kL627OWlppv7bE/jv+ilppPW3cSfe3z/bRgClwyf0tuorWDoAGmdl0YDpAz549m7aQ2I7QdfCpz+dc4z/cGru8E7/JCT/EmrOG0AKbcVkn2abj7r/PvW6ObWvqh3KT193c/x4nWk0rracpmvv/xkmdZD3h3ketuR/SB7T4Klo7AEqAHvVe53jjjnHOzQRmAhQUFDTtf3pqX7jyT02rUETEJ1r7HMByYICZ9TGzOOBqYF4r1yAiIrTyEYBzLmBm3wHmA9HALOfc2tasQUREQlr9HIBz7nXg9dZer4iI/CvdCSwi4lMKABERn1IAiIj4lAJARMSnFAAiIj5lrtVudT91ZlYJbDuNRaQDO5upnPZM+yFE+yFE+yEkkvdDL+dcRkMTtekAOF1mVuicKwh3HeGm/RCi/RCi/RCi/aAmIBER31IAiIj4VKQHwMxwF9BGaD+EaD+EaD+E+H4/RPQ5ABERObFIPwIQEZETiMgA0IPnQ8xsq5mtNrOPzKww3PW0JjObZWYVZram3rhUM1tgZpu8313CWWNrOMF++KmZlXh/Fx+Z2SXhrLE1mFkPM1toZuvMbK2Zfdcb77u/ifoiLgD04PkvmOicy/fh5W5/Ai763LgZwDvOuQHAO97rSPcnvrgfAB70/i7yvR56I10A+KFzLg8YC9zqfS748W/imIgLAPTgeQGcc+8B1Z8bPRV4yht+CvhKqxYVBifYD77jnCt1zq30hvcB6wk9o9x3fxP1RWIAHO/B893DVEu4OeAtM1vhPWvZ77o650q94TKgaziLCbPvmNnHXhORr5o9zKw3MAJYis//JiIxAOSfxjnnRhJqDrvVzM4Ld0FthQtd/ubXS+CeAPoB+UAp8OvwltN6zKwzMBf4nnNub/33/Pg3EYkB0OCD5/3COVfi/a4AXiHUPOZn5WaWDeD9rghzPWHhnCt3ztU554LA7/HJ34WZxRL68J/tnPuzN9rXfxORGAB68DxgZglmlnh0GLgQWHPyuSLePOBGb/hG4NUw1hI2Rz/wPF/FB38XZmbAk8B659wD9d7y9d9ERN4I5l3W9hD/fPD8z8NcUqszs76EvvVD6NnPz/lpP5jZ88AEQj0+lgN3AX8BXgR6Eupl9irnXESfID3BfphAqPnHAVuBf6vXDh6RzGwc8H/AaiDojf4xofMAvvqbqC8iA0BERBoWiU1AIiLSCAoAERGfUgCIiPiUAkBExKcUACIiPqUAEBHxKQWAiIhPKQBERHzq/wNdvmWcrX+7IwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "plt.plot(train_loss, label='Training loss')\n",
    "plt.plot(test_loss, label='Testing loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 Keras FFRNN Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_opt_params = {\n",
    "    'dropout_pct' : [.2, .25],\n",
    "    'dense_1_nodes' : [64, 128, 256],\n",
    "    'dense_2_nodes' : [64, 128],\n",
    "    'dense_3_nodes' : [32, 64],\n",
    "    'target_nodes' : [20],\n",
    "    'epochs' : [25],\n",
    "    'batch_size' : [500],\n",
    "    'patience' : [3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n",
      "74115/74115 [==============================] - 13s 171us/step - loss: 1.9297 - acc: 0.4183 - val_loss: 2.7881 - val_acc: 0.1156\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 11s 149us/step - loss: 1.4891 - acc: 0.5336 - val_loss: 2.7357 - val_acc: 0.1779\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 11s 149us/step - loss: 1.3459 - acc: 0.5694 - val_loss: 2.7352 - val_acc: 0.1731\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 11s 149us/step - loss: 1.2215 - acc: 0.6029 - val_loss: 2.8679 - val_acc: 0.1910\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 11s 149us/step - loss: 1.1450 - acc: 0.6257 - val_loss: 2.9622 - val_acc: 0.1594\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 11s 149us/step - loss: 1.0699 - acc: 0.6496 - val_loss: 2.9966 - val_acc: 0.1707\n",
      "Epoch 7/25\n",
      "74115/74115 [==============================] - 11s 149us/step - loss: 1.0117 - acc: 0.6648 - val_loss: 3.2471 - val_acc: 0.1805\n",
      "Epoch 8/25\n",
      "74115/74115 [==============================] - 11s 150us/step - loss: 0.9561 - acc: 0.6825 - val_loss: 3.2769 - val_acc: 0.1943\n",
      "Epoch 9/25\n",
      "74115/74115 [==============================] - 11s 150us/step - loss: 0.8961 - acc: 0.7005 - val_loss: 3.6268 - val_acc: 0.1534\n",
      "Epoch 10/25\n",
      "74115/74115 [==============================] - 11s 150us/step - loss: 0.8607 - acc: 0.7104 - val_loss: 3.5311 - val_acc: 0.1682\n",
      "Epoch 11/25\n",
      "74115/74115 [==============================] - 11s 151us/step - loss: 0.8121 - acc: 0.7264 - val_loss: 3.6257 - val_acc: 0.1766\n",
      "Epoch 12/25\n",
      "74115/74115 [==============================] - 11s 151us/step - loss: 0.7700 - acc: 0.7379 - val_loss: 3.6805 - val_acc: 0.1718\n",
      "Epoch 13/25\n",
      "74115/74115 [==============================] - 11s 150us/step - loss: 0.7310 - acc: 0.7526 - val_loss: 4.0768 - val_acc: 0.1734\n",
      "Epoch 00013: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n",
      "74115/74115 [==============================] - 13s 171us/step - loss: 1.8076 - acc: 0.4459 - val_loss: 2.6399 - val_acc: 0.2174\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 11s 150us/step - loss: 1.4264 - acc: 0.5413 - val_loss: 2.8233 - val_acc: 0.1387\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 11s 151us/step - loss: 1.2847 - acc: 0.5822 - val_loss: 2.8724 - val_acc: 0.1621\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 11s 151us/step - loss: 1.1871 - acc: 0.6134 - val_loss: 2.8377 - val_acc: 0.1746\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 11s 150us/step - loss: 1.0974 - acc: 0.6384 - val_loss: 3.0097 - val_acc: 0.1626\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 11s 151us/step - loss: 1.0312 - acc: 0.6586 - val_loss: 3.1948 - val_acc: 0.1485\n",
      "Epoch 00006: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n",
      "74115/74115 [==============================] - 13s 176us/step - loss: 1.7993 - acc: 0.4605 - val_loss: 2.7025 - val_acc: 0.1396\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 11s 151us/step - loss: 1.3990 - acc: 0.5553 - val_loss: 2.9492 - val_acc: 0.1477\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 11s 151us/step - loss: 1.2441 - acc: 0.6001 - val_loss: 3.0197 - val_acc: 0.1639\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 11s 151us/step - loss: 1.1406 - acc: 0.6277 - val_loss: 3.0159 - val_acc: 0.1616\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 11s 151us/step - loss: 1.0435 - acc: 0.6586 - val_loss: 2.9858 - val_acc: 0.1921\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 11s 151us/step - loss: 0.9628 - acc: 0.6826 - val_loss: 3.3716 - val_acc: 0.1605\n",
      "Epoch 7/25\n",
      "74115/74115 [==============================] - 11s 151us/step - loss: 0.8964 - acc: 0.7017 - val_loss: 3.3737 - val_acc: 0.1933\n",
      "Epoch 8/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 0.8370 - acc: 0.7230 - val_loss: 3.4553 - val_acc: 0.1850\n",
      "Epoch 9/25\n",
      "74115/74115 [==============================] - 11s 151us/step - loss: 0.7716 - acc: 0.7411 - val_loss: 3.5931 - val_acc: 0.1725\n",
      "Epoch 10/25\n",
      "74115/74115 [==============================] - 11s 151us/step - loss: 0.7301 - acc: 0.7548 - val_loss: 3.7390 - val_acc: 0.1909\n",
      "Epoch 11/25\n",
      "74115/74115 [==============================] - 11s 151us/step - loss: 0.6868 - acc: 0.7689 - val_loss: 3.9711 - val_acc: 0.1560\n",
      "Epoch 12/25\n",
      "74115/74115 [==============================] - 11s 151us/step - loss: 0.6395 - acc: 0.7830 - val_loss: 4.0496 - val_acc: 0.1696\n",
      "Epoch 00012: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n",
      "74115/74115 [==============================] - 13s 175us/step - loss: 1.7199 - acc: 0.4695 - val_loss: 2.7534 - val_acc: 0.1573\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 11s 151us/step - loss: 1.3594 - acc: 0.5631 - val_loss: 2.8281 - val_acc: 0.1444\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 11s 151us/step - loss: 1.2167 - acc: 0.6027 - val_loss: 2.9316 - val_acc: 0.1533\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 1.1191 - acc: 0.6317 - val_loss: 3.1033 - val_acc: 0.1667\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 1.0277 - acc: 0.6585 - val_loss: 3.1713 - val_acc: 0.1577\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 0.9556 - acc: 0.6808 - val_loss: 3.3394 - val_acc: 0.1605\n",
      "Epoch 7/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 0.8881 - acc: 0.7017 - val_loss: 3.3923 - val_acc: 0.1674\n",
      "Epoch 8/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 0.8328 - acc: 0.7202 - val_loss: 3.5028 - val_acc: 0.1690\n",
      "Epoch 9/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 0.7715 - acc: 0.7414 - val_loss: 3.6089 - val_acc: 0.1756\n",
      "Epoch 10/25\n",
      "74115/74115 [==============================] - 11s 151us/step - loss: 0.7281 - acc: 0.7525 - val_loss: 3.9853 - val_acc: 0.1584\n",
      "Epoch 11/25\n",
      "74115/74115 [==============================] - 11s 151us/step - loss: 0.6905 - acc: 0.7679 - val_loss: 3.9510 - val_acc: 0.1743\n",
      "Epoch 12/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 0.6383 - acc: 0.7828 - val_loss: 4.1283 - val_acc: 0.1554\n",
      "Epoch 13/25\n",
      "74115/74115 [==============================] - 11s 151us/step - loss: 0.6022 - acc: 0.7941 - val_loss: 4.2571 - val_acc: 0.1823\n",
      "Epoch 14/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 0.5818 - acc: 0.8007 - val_loss: 4.4162 - val_acc: 0.1855\n",
      "Epoch 15/25\n",
      "74115/74115 [==============================] - 11s 151us/step - loss: 0.5459 - acc: 0.8144 - val_loss: 4.4990 - val_acc: 0.1611\n",
      "Epoch 16/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 0.5157 - acc: 0.8255 - val_loss: 4.5445 - val_acc: 0.1750\n",
      "Epoch 17/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 0.4835 - acc: 0.8348 - val_loss: 4.5998 - val_acc: 0.1737\n",
      "Epoch 18/25\n",
      "74115/74115 [==============================] - 11s 153us/step - loss: 0.4715 - acc: 0.8391 - val_loss: 4.7150 - val_acc: 0.1711\n",
      "Epoch 19/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 0.4449 - acc: 0.8486 - val_loss: 4.9176 - val_acc: 0.1641\n",
      "Epoch 00019: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n",
      "74115/74115 [==============================] - 15s 208us/step - loss: 1.8578 - acc: 0.4363 - val_loss: 2.7237 - val_acc: 0.1479\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 13s 179us/step - loss: 1.4255 - acc: 0.5463 - val_loss: 2.7853 - val_acc: 0.1569\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 13s 179us/step - loss: 1.2777 - acc: 0.5878 - val_loss: 2.8084 - val_acc: 0.1603\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 1.1621 - acc: 0.6232 - val_loss: 3.0028 - val_acc: 0.1637\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 1.0785 - acc: 0.6479 - val_loss: 3.2177 - val_acc: 0.1685\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 0.9945 - acc: 0.6734 - val_loss: 3.2332 - val_acc: 0.1742\n",
      "Epoch 7/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 0.9245 - acc: 0.6961 - val_loss: 3.2745 - val_acc: 0.1766\n",
      "Epoch 8/25\n",
      "74115/74115 [==============================] - 13s 179us/step - loss: 0.8581 - acc: 0.7179 - val_loss: 3.6548 - val_acc: 0.1626\n",
      "Epoch 9/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 0.8063 - acc: 0.7311 - val_loss: 3.6111 - val_acc: 0.1674\n",
      "Epoch 10/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 0.7597 - acc: 0.7484 - val_loss: 3.8975 - val_acc: 0.1647\n",
      "Epoch 11/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.7010 - acc: 0.7655 - val_loss: 4.1151 - val_acc: 0.1789\n",
      "Epoch 12/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 0.6618 - acc: 0.7790 - val_loss: 3.9727 - val_acc: 0.1787\n",
      "Epoch 13/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 0.6185 - acc: 0.7916 - val_loss: 4.1141 - val_acc: 0.1858\n",
      "Epoch 14/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 0.5722 - acc: 0.8088 - val_loss: 4.5942 - val_acc: 0.1557\n",
      "Epoch 15/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 0.5469 - acc: 0.8170 - val_loss: 4.2898 - val_acc: 0.1913\n",
      "Epoch 16/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 0.5228 - acc: 0.8246 - val_loss: 4.6692 - val_acc: 0.1657\n",
      "Epoch 17/25\n",
      "74115/74115 [==============================] - 13s 179us/step - loss: 0.4866 - acc: 0.8368 - val_loss: 5.0226 - val_acc: 0.1537\n",
      "Epoch 18/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 0.4601 - acc: 0.8457 - val_loss: 4.9344 - val_acc: 0.1638\n",
      "Epoch 19/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.4478 - acc: 0.8497 - val_loss: 4.9915 - val_acc: 0.1579\n",
      "Epoch 20/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 0.4329 - acc: 0.8557 - val_loss: 5.0073 - val_acc: 0.1741\n",
      "Epoch 00020: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n",
      "74115/74115 [==============================] - 15s 209us/step - loss: 1.7089 - acc: 0.4717 - val_loss: 2.7381 - val_acc: 0.1591\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 1.3550 - acc: 0.5638 - val_loss: 2.8378 - val_acc: 0.1558\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 1.2219 - acc: 0.6045 - val_loss: 2.9564 - val_acc: 0.1782\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 1.1076 - acc: 0.6394 - val_loss: 3.0171 - val_acc: 0.1784\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 13s 179us/step - loss: 1.0101 - acc: 0.6700 - val_loss: 3.2688 - val_acc: 0.1726\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 13s 179us/step - loss: 0.9336 - acc: 0.6945 - val_loss: 3.6238 - val_acc: 0.1674\n",
      "Epoch 7/25\n",
      "74115/74115 [==============================] - 13s 179us/step - loss: 0.8685 - acc: 0.7133 - val_loss: 3.7707 - val_acc: 0.1584\n",
      "Epoch 8/25\n",
      "74115/74115 [==============================] - 13s 179us/step - loss: 0.8017 - acc: 0.7338 - val_loss: 3.7825 - val_acc: 0.1717\n",
      "Epoch 9/25\n",
      "74115/74115 [==============================] - 13s 179us/step - loss: 0.7516 - acc: 0.7507 - val_loss: 3.9635 - val_acc: 0.1536\n",
      "Epoch 00009: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n",
      "74115/74115 [==============================] - 16s 212us/step - loss: 1.7007 - acc: 0.4798 - val_loss: 2.6627 - val_acc: 0.1504\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 1.3177 - acc: 0.5768 - val_loss: 2.8175 - val_acc: 0.1829\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 1.1803 - acc: 0.6168 - val_loss: 3.0282 - val_acc: 0.1788\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 1.0656 - acc: 0.6509 - val_loss: 3.0621 - val_acc: 0.1657\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.9700 - acc: 0.6809 - val_loss: 3.3270 - val_acc: 0.1726\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 0.8899 - acc: 0.7070 - val_loss: 3.2717 - val_acc: 0.1843\n",
      "Epoch 7/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.8106 - acc: 0.7322 - val_loss: 3.5168 - val_acc: 0.1669\n",
      "Epoch 8/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.7460 - acc: 0.7511 - val_loss: 3.8667 - val_acc: 0.1993\n",
      "Epoch 9/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 0.6964 - acc: 0.7666 - val_loss: 3.7639 - val_acc: 0.1807\n",
      "Epoch 10/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 0.6292 - acc: 0.7894 - val_loss: 4.1722 - val_acc: 0.1711\n",
      "Epoch 11/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 0.5875 - acc: 0.8028 - val_loss: 4.1333 - val_acc: 0.1839\n",
      "Epoch 12/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.5403 - acc: 0.8192 - val_loss: 4.2877 - val_acc: 0.1793\n",
      "Epoch 13/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.5038 - acc: 0.8305 - val_loss: 4.5616 - val_acc: 0.1893\n",
      "Epoch 00013: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n",
      "74115/74115 [==============================] - 16s 215us/step - loss: 1.6566 - acc: 0.4838 - val_loss: 2.8215 - val_acc: 0.1247\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 1.3047 - acc: 0.5766 - val_loss: 2.9731 - val_acc: 0.1766\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 1.1642 - acc: 0.6201 - val_loss: 2.9845 - val_acc: 0.1747\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 1.0461 - acc: 0.6540 - val_loss: 3.1314 - val_acc: 0.1614\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.9675 - acc: 0.6804 - val_loss: 3.3107 - val_acc: 0.1634\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.8832 - acc: 0.7061 - val_loss: 3.4022 - val_acc: 0.1634\n",
      "Epoch 7/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.8152 - acc: 0.7286 - val_loss: 3.6657 - val_acc: 0.1846\n",
      "Epoch 8/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.7488 - acc: 0.7502 - val_loss: 3.7981 - val_acc: 0.1814\n",
      "Epoch 9/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.6958 - acc: 0.7662 - val_loss: 3.9197 - val_acc: 0.1549\n",
      "Epoch 10/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 0.6385 - acc: 0.7860 - val_loss: 4.1538 - val_acc: 0.1726\n",
      "Epoch 11/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.5971 - acc: 0.7988 - val_loss: 4.2466 - val_acc: 0.1773\n",
      "Epoch 12/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 0.5542 - acc: 0.8119 - val_loss: 4.5195 - val_acc: 0.1620\n",
      "Epoch 00012: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n",
      "74115/74115 [==============================] - 20s 266us/step - loss: 1.7259 - acc: 0.4723 - val_loss: 2.6648 - val_acc: 0.1834\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 17s 229us/step - loss: 1.3419 - acc: 0.5733 - val_loss: 2.9089 - val_acc: 0.1791\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 17s 229us/step - loss: 1.1929 - acc: 0.6148 - val_loss: 2.9129 - val_acc: 0.2046\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 1.0887 - acc: 0.6478 - val_loss: 2.9216 - val_acc: 0.1809\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 17s 229us/step - loss: 0.9922 - acc: 0.6751 - val_loss: 3.2849 - val_acc: 0.1744\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 17s 228us/step - loss: 0.9089 - acc: 0.7010 - val_loss: 3.5787 - val_acc: 0.1768\n",
      "Epoch 7/25\n",
      "74115/74115 [==============================] - 17s 229us/step - loss: 0.8336 - acc: 0.7251 - val_loss: 3.6225 - val_acc: 0.1780\n",
      "Epoch 8/25\n",
      "74115/74115 [==============================] - 17s 229us/step - loss: 0.7661 - acc: 0.7458 - val_loss: 3.8989 - val_acc: 0.1674\n",
      "Epoch 00008: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74115/74115 [==============================] - 20s 268us/step - loss: 1.6539 - acc: 0.4879 - val_loss: 2.7122 - val_acc: 0.1855\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 1.3034 - acc: 0.5826 - val_loss: 2.8800 - val_acc: 0.1713\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 1.1649 - acc: 0.6230 - val_loss: 3.0232 - val_acc: 0.1640\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 1.0590 - acc: 0.6543 - val_loss: 3.1840 - val_acc: 0.1626\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 17s 229us/step - loss: 0.9687 - acc: 0.6800 - val_loss: 3.2525 - val_acc: 0.1570\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 0.8810 - acc: 0.7108 - val_loss: 3.3782 - val_acc: 0.1692\n",
      "Epoch 00006: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n",
      "74115/74115 [==============================] - 20s 267us/step - loss: 1.7277 - acc: 0.4748 - val_loss: 2.7560 - val_acc: 0.1662\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 17s 229us/step - loss: 1.3263 - acc: 0.5778 - val_loss: 2.7223 - val_acc: 0.1879\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 1.1745 - acc: 0.6204 - val_loss: 2.8709 - val_acc: 0.1786\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 17s 229us/step - loss: 1.0600 - acc: 0.6541 - val_loss: 3.0377 - val_acc: 0.1725\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 0.9567 - acc: 0.6845 - val_loss: 3.1853 - val_acc: 0.1980\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 0.8787 - acc: 0.7100 - val_loss: 3.4534 - val_acc: 0.1773\n",
      "Epoch 7/25\n",
      "74115/74115 [==============================] - 17s 229us/step - loss: 0.8006 - acc: 0.7337 - val_loss: 3.5374 - val_acc: 0.1808\n",
      "Epoch 8/25\n",
      "74115/74115 [==============================] - 17s 229us/step - loss: 0.7371 - acc: 0.7517 - val_loss: 3.6248 - val_acc: 0.1687\n",
      "Epoch 9/25\n",
      "74115/74115 [==============================] - 17s 228us/step - loss: 0.6819 - acc: 0.7721 - val_loss: 3.8677 - val_acc: 0.1701\n",
      "Epoch 10/25\n",
      "74115/74115 [==============================] - 17s 228us/step - loss: 0.6299 - acc: 0.7882 - val_loss: 3.9650 - val_acc: 0.1855\n",
      "Epoch 00010: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n",
      "74115/74115 [==============================] - 20s 270us/step - loss: 1.5831 - acc: 0.5048 - val_loss: 2.6747 - val_acc: 0.2126\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 17s 229us/step - loss: 1.2398 - acc: 0.5994 - val_loss: 2.8603 - val_acc: 0.1848\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 17s 229us/step - loss: 1.1092 - acc: 0.6367 - val_loss: 3.1232 - val_acc: 0.1759\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 0.9955 - acc: 0.6719 - val_loss: 3.1864 - val_acc: 0.1771\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 0.9021 - acc: 0.7003 - val_loss: 3.3562 - val_acc: 0.1789\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 0.8182 - acc: 0.7259 - val_loss: 3.7631 - val_acc: 0.1676\n",
      "Epoch 00006: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n",
      "74115/74115 [==============================] - 14s 191us/step - loss: 1.9336 - acc: 0.4156 - val_loss: 2.7164 - val_acc: 0.1123\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 1.5356 - acc: 0.5197 - val_loss: 2.7952 - val_acc: 0.1731\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 1.3907 - acc: 0.5593 - val_loss: 2.7569 - val_acc: 0.1552\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 1.2799 - acc: 0.5894 - val_loss: 2.8223 - val_acc: 0.1811\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 1.1950 - acc: 0.6148 - val_loss: 2.8981 - val_acc: 0.1786\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 1.1252 - acc: 0.6343 - val_loss: 3.0055 - val_acc: 0.1837\n",
      "Epoch 7/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 1.0555 - acc: 0.6558 - val_loss: 3.0963 - val_acc: 0.1733\n",
      "Epoch 8/25\n",
      "74115/74115 [==============================] - 11s 153us/step - loss: 1.0016 - acc: 0.6719 - val_loss: 3.1778 - val_acc: 0.1709\n",
      "Epoch 9/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 0.9423 - acc: 0.6889 - val_loss: 3.2262 - val_acc: 0.1581\n",
      "Epoch 10/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 0.9025 - acc: 0.7010 - val_loss: 3.4385 - val_acc: 0.1593\n",
      "Epoch 11/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 0.8614 - acc: 0.7147 - val_loss: 3.5889 - val_acc: 0.1657\n",
      "Epoch 00011: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n",
      "74115/74115 [==============================] - 14s 195us/step - loss: 1.9334 - acc: 0.4100 - val_loss: 2.6596 - val_acc: 0.1581\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 1.5285 - acc: 0.5108 - val_loss: 2.7456 - val_acc: 0.1503\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 1.3994 - acc: 0.5493 - val_loss: 2.8277 - val_acc: 0.1726\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 1.3083 - acc: 0.5752 - val_loss: 2.9158 - val_acc: 0.1626\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 1.2305 - acc: 0.5961 - val_loss: 3.0694 - val_acc: 0.1657\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 11s 153us/step - loss: 1.1702 - acc: 0.6138 - val_loss: 3.1518 - val_acc: 0.1676\n",
      "Epoch 7/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 1.1135 - acc: 0.6301 - val_loss: 3.1188 - val_acc: 0.1678\n",
      "Epoch 8/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 1.0762 - acc: 0.6431 - val_loss: 3.2961 - val_acc: 0.1714\n",
      "Epoch 00008: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n",
      "74115/74115 [==============================] - 15s 196us/step - loss: 1.8595 - acc: 0.4373 - val_loss: 2.6824 - val_acc: 0.1610\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 11s 153us/step - loss: 1.4617 - acc: 0.5351 - val_loss: 2.7443 - val_acc: 0.1538\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 11s 153us/step - loss: 1.3153 - acc: 0.5774 - val_loss: 2.8327 - val_acc: 0.1565\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 11s 154us/step - loss: 1.2086 - acc: 0.6066 - val_loss: 2.9227 - val_acc: 0.1572\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 11s 153us/step - loss: 1.1258 - acc: 0.6295 - val_loss: 2.9826 - val_acc: 0.1554\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 1.0574 - acc: 0.6531 - val_loss: 3.0446 - val_acc: 0.1599\n",
      "Epoch 00006: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n",
      "74115/74115 [==============================] - 15s 197us/step - loss: 1.7532 - acc: 0.4539 - val_loss: 2.7306 - val_acc: 0.1399\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 1.4156 - acc: 0.5491 - val_loss: 2.8435 - val_acc: 0.1805\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 1.2773 - acc: 0.5860 - val_loss: 2.9504 - val_acc: 0.1416\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 1.1823 - acc: 0.6143 - val_loss: 2.9470 - val_acc: 0.1614\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 1.0968 - acc: 0.6405 - val_loss: 2.9884 - val_acc: 0.1875\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 1.0297 - acc: 0.6614 - val_loss: 3.2813 - val_acc: 0.1599\n",
      "Epoch 7/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 0.9711 - acc: 0.6815 - val_loss: 3.2953 - val_acc: 0.1647\n",
      "Epoch 8/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 0.9183 - acc: 0.6980 - val_loss: 3.3638 - val_acc: 0.1560\n",
      "Epoch 9/25\n",
      "74115/74115 [==============================] - 11s 152us/step - loss: 0.8641 - acc: 0.7131 - val_loss: 3.5707 - val_acc: 0.1674\n",
      "Epoch 10/25\n",
      "74115/74115 [==============================] - 11s 153us/step - loss: 0.8236 - acc: 0.7259 - val_loss: 3.7118 - val_acc: 0.1526\n",
      "Epoch 00010: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n",
      "74115/74115 [==============================] - 17s 227us/step - loss: 1.8539 - acc: 0.4451 - val_loss: 2.7261 - val_acc: 0.1414\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 1.4470 - acc: 0.5448 - val_loss: 2.6543 - val_acc: 0.1671\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 1.3006 - acc: 0.5844 - val_loss: 2.8860 - val_acc: 0.1742\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 1.1939 - acc: 0.6169 - val_loss: 3.0182 - val_acc: 0.1486\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 1.1059 - acc: 0.6421 - val_loss: 2.9905 - val_acc: 0.1756\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 13s 182us/step - loss: 1.0320 - acc: 0.6651 - val_loss: 3.2774 - val_acc: 0.1651\n",
      "Epoch 7/25\n",
      "74115/74115 [==============================] - 13s 182us/step - loss: 0.9643 - acc: 0.6851 - val_loss: 3.2709 - val_acc: 0.1711\n",
      "Epoch 8/25\n",
      "74115/74115 [==============================] - 13s 182us/step - loss: 0.9026 - acc: 0.7052 - val_loss: 3.4138 - val_acc: 0.1656\n",
      "Epoch 9/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.8533 - acc: 0.7202 - val_loss: 3.7247 - val_acc: 0.1663\n",
      "Epoch 10/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.7964 - acc: 0.7381 - val_loss: 3.6555 - val_acc: 0.1649\n",
      "Epoch 00010: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n",
      "74115/74115 [==============================] - 17s 228us/step - loss: 1.7407 - acc: 0.4648 - val_loss: 2.6991 - val_acc: 0.1911\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 1.4034 - acc: 0.5529 - val_loss: 2.6893 - val_acc: 0.2015\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 1.2701 - acc: 0.5927 - val_loss: 2.9039 - val_acc: 0.1952\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 1.1697 - acc: 0.6196 - val_loss: 2.9027 - val_acc: 0.1848\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 13s 182us/step - loss: 1.0736 - acc: 0.6494 - val_loss: 3.0392 - val_acc: 0.1696\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 1.0035 - acc: 0.6716 - val_loss: 3.1255 - val_acc: 0.1853\n",
      "Epoch 7/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.9405 - acc: 0.6899 - val_loss: 3.2047 - val_acc: 0.1974\n",
      "Epoch 00007: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n",
      "74115/74115 [==============================] - 17s 231us/step - loss: 1.7628 - acc: 0.4698 - val_loss: 2.8007 - val_acc: 0.1766\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 1.3810 - acc: 0.5625 - val_loss: 2.9538 - val_acc: 0.1661\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 1.2416 - acc: 0.5991 - val_loss: 2.9610 - val_acc: 0.1815\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 1.1234 - acc: 0.6344 - val_loss: 3.0888 - val_acc: 0.1762\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 1.0445 - acc: 0.6589 - val_loss: 3.2603 - val_acc: 0.2100\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 0.9585 - acc: 0.6861 - val_loss: 3.4818 - val_acc: 0.1762\n",
      "Epoch 7/25\n",
      "74115/74115 [==============================] - 13s 180us/step - loss: 0.8810 - acc: 0.7112 - val_loss: 3.4896 - val_acc: 0.1894\n",
      "Epoch 8/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.8191 - acc: 0.7287 - val_loss: 3.7569 - val_acc: 0.1786\n",
      "Epoch 9/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.7616 - acc: 0.7478 - val_loss: 3.7219 - val_acc: 0.1871\n",
      "Epoch 10/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.7128 - acc: 0.7626 - val_loss: 4.0040 - val_acc: 0.1838\n",
      "Epoch 00010: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n",
      "74115/74115 [==============================] - 17s 235us/step - loss: 1.7303 - acc: 0.4671 - val_loss: 2.6636 - val_acc: 0.1595\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 1.3720 - acc: 0.5619 - val_loss: 2.8412 - val_acc: 0.1612\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 13s 182us/step - loss: 1.2250 - acc: 0.6043 - val_loss: 2.9808 - val_acc: 0.1521\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 14s 182us/step - loss: 1.1224 - acc: 0.6340 - val_loss: 3.0908 - val_acc: 0.1610\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 1.0359 - acc: 0.6602 - val_loss: 3.1525 - val_acc: 0.1587\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 14s 183us/step - loss: 0.9553 - acc: 0.6845 - val_loss: 3.1501 - val_acc: 0.1941\n",
      "Epoch 7/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.8909 - acc: 0.7036 - val_loss: 3.3853 - val_acc: 0.1853\n",
      "Epoch 8/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.8286 - acc: 0.7246 - val_loss: 3.4892 - val_acc: 0.1666\n",
      "Epoch 9/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.7747 - acc: 0.7422 - val_loss: 3.5415 - val_acc: 0.1959\n",
      "Epoch 10/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.7254 - acc: 0.7585 - val_loss: 3.8393 - val_acc: 0.1814\n",
      "Epoch 11/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.6883 - acc: 0.7712 - val_loss: 3.9439 - val_acc: 0.1714\n",
      "Epoch 12/25\n",
      "74115/74115 [==============================] - 13s 181us/step - loss: 0.6384 - acc: 0.7860 - val_loss: 4.0423 - val_acc: 0.1775\n",
      "Epoch 13/25\n",
      "74115/74115 [==============================] - 13s 182us/step - loss: 0.6035 - acc: 0.7994 - val_loss: 3.9922 - val_acc: 0.1915\n",
      "Epoch 14/25\n",
      "74115/74115 [==============================] - 13s 182us/step - loss: 0.5620 - acc: 0.8123 - val_loss: 4.2521 - val_acc: 0.1922\n",
      "Epoch 00014: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n",
      "74115/74115 [==============================] - 21s 288us/step - loss: 1.7983 - acc: 0.4567 - val_loss: 2.6893 - val_acc: 0.1703\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 17s 229us/step - loss: 1.3984 - acc: 0.5596 - val_loss: 2.7877 - val_acc: 0.1744\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 1.2416 - acc: 0.6025 - val_loss: 2.8904 - val_acc: 0.1806\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 17s 231us/step - loss: 1.1355 - acc: 0.6356 - val_loss: 3.2323 - val_acc: 0.1870\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 1.0465 - acc: 0.6629 - val_loss: 3.2067 - val_acc: 0.1802\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 0.9534 - acc: 0.6883 - val_loss: 3.4277 - val_acc: 0.1625\n",
      "Epoch 7/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 0.8899 - acc: 0.7089 - val_loss: 3.6114 - val_acc: 0.1879\n",
      "Epoch 8/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 0.8173 - acc: 0.7305 - val_loss: 3.7895 - val_acc: 0.1785\n",
      "Epoch 9/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 0.7563 - acc: 0.7511 - val_loss: 3.7885 - val_acc: 0.1824\n",
      "Epoch 10/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 0.7011 - acc: 0.7684 - val_loss: 3.9909 - val_acc: 0.1599\n",
      "Epoch 11/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 0.6645 - acc: 0.7799 - val_loss: 4.3976 - val_acc: 0.1866\n",
      "Epoch 12/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 0.6105 - acc: 0.7974 - val_loss: 4.1526 - val_acc: 0.1684\n",
      "Epoch 00012: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74115/74115 [==============================] - 22s 291us/step - loss: 1.7237 - acc: 0.4669 - val_loss: 2.7243 - val_acc: 0.1512\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 17s 231us/step - loss: 1.3577 - acc: 0.5669 - val_loss: 2.8449 - val_acc: 0.1919\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 1.2234 - acc: 0.6052 - val_loss: 2.9005 - val_acc: 0.1814\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 1.1218 - acc: 0.6371 - val_loss: 3.0552 - val_acc: 0.1772\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 17s 231us/step - loss: 1.0364 - acc: 0.6624 - val_loss: 3.2899 - val_acc: 0.1702\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 0.9619 - acc: 0.6858 - val_loss: 3.5082 - val_acc: 0.1780\n",
      "Epoch 7/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 0.8899 - acc: 0.7077 - val_loss: 3.5472 - val_acc: 0.1666\n",
      "Epoch 00007: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n",
      "74115/74115 [==============================] - 22s 293us/step - loss: 1.7719 - acc: 0.4653 - val_loss: 2.8223 - val_acc: 0.1234\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 1.3628 - acc: 0.5671 - val_loss: 2.8533 - val_acc: 0.1479\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 1.2174 - acc: 0.6061 - val_loss: 2.9873 - val_acc: 0.1426\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 17s 231us/step - loss: 1.0992 - acc: 0.6444 - val_loss: 3.1537 - val_acc: 0.1808\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 1.0035 - acc: 0.6728 - val_loss: 3.1586 - val_acc: 0.1566\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 0.9303 - acc: 0.6934 - val_loss: 3.3374 - val_acc: 0.1648\n",
      "Epoch 7/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 0.8646 - acc: 0.7157 - val_loss: 3.6849 - val_acc: 0.1672\n",
      "Epoch 8/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 0.7905 - acc: 0.7359 - val_loss: 3.6174 - val_acc: 0.1693\n",
      "Epoch 9/25\n",
      "74115/74115 [==============================] - 17s 231us/step - loss: 0.7381 - acc: 0.7557 - val_loss: 4.0720 - val_acc: 0.1585\n",
      "Epoch 00009: early stopping\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/25\n",
      "74115/74115 [==============================] - 22s 294us/step - loss: 1.6191 - acc: 0.4970 - val_loss: 2.7605 - val_acc: 0.1641\n",
      "Epoch 2/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 1.2984 - acc: 0.5803 - val_loss: 2.9711 - val_acc: 0.1611\n",
      "Epoch 3/25\n",
      "74115/74115 [==============================] - 17s 231us/step - loss: 1.1635 - acc: 0.6220 - val_loss: 3.0793 - val_acc: 0.1741\n",
      "Epoch 4/25\n",
      "74115/74115 [==============================] - 17s 231us/step - loss: 1.0558 - acc: 0.6523 - val_loss: 3.1735 - val_acc: 0.1794\n",
      "Epoch 5/25\n",
      "74115/74115 [==============================] - 17s 231us/step - loss: 0.9674 - acc: 0.6819 - val_loss: 3.2616 - val_acc: 0.1817\n",
      "Epoch 6/25\n",
      "74115/74115 [==============================] - 17s 231us/step - loss: 0.8888 - acc: 0.7040 - val_loss: 3.4384 - val_acc: 0.1805\n",
      "Epoch 7/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 0.8139 - acc: 0.7299 - val_loss: 3.6512 - val_acc: 0.1789\n",
      "Epoch 8/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 0.7546 - acc: 0.7460 - val_loss: 3.8449 - val_acc: 0.1794\n",
      "Epoch 9/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 0.6979 - acc: 0.7659 - val_loss: 3.8918 - val_acc: 0.1793\n",
      "Epoch 10/25\n",
      "74115/74115 [==============================] - 17s 230us/step - loss: 0.6438 - acc: 0.7852 - val_loss: 4.1606 - val_acc: 0.1755\n",
      "Epoch 00010: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dropout_pct</th>\n",
       "      <th>dense_1_nodes</th>\n",
       "      <th>dense_2_nodes</th>\n",
       "      <th>dense_3_nodes</th>\n",
       "      <th>target_nodes</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>patience</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.929674621391721, 1.4891164448469876, 1.3458...</td>\n",
       "      <td>[2.788091388872269, 2.7357079937062916, 2.7352...</td>\n",
       "      <td>[0.4183093846489654, 0.5335627086332418, 0.569...</td>\n",
       "      <td>[0.11555806102636433, 0.17790304550170277, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.807594449785664, 1.4264270857600783, 1.2847...</td>\n",
       "      <td>[2.639851927085283, 2.8233394692688676, 2.8723...</td>\n",
       "      <td>[0.4459151326554479, 0.541307428624155, 0.5821...</td>\n",
       "      <td>[0.21736189578119097, 0.1386696750617333, 0.16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>64.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.7993344856982916, 1.3989937976570672, 1.244...</td>\n",
       "      <td>[2.7024768084254043, 2.9492240141585824, 3.019...</td>\n",
       "      <td>[0.460541049611739, 0.5553126883259075, 0.6000...</td>\n",
       "      <td>[0.13957158876152492, 0.14768883820807954, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2</td>\n",
       "      <td>64.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.71986253795728, 1.3594488705780023, 1.21665...</td>\n",
       "      <td>[2.7534403096729094, 2.8281125191771324, 2.931...</td>\n",
       "      <td>[0.4694731150819956, 0.5630709044019566, 0.602...</td>\n",
       "      <td>[0.15727170218641134, 0.14441939342657803, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.8578103558219237, 1.4254890923910857, 1.277...</td>\n",
       "      <td>[2.7237480103364793, 2.7852583254069057, 2.808...</td>\n",
       "      <td>[0.4363489153453843, 0.5463131596779426, 0.587...</td>\n",
       "      <td>[0.14791431621015239, 0.15693348356372475, 0.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dropout_pct  dense_1_nodes  dense_2_nodes  dense_3_nodes  target_nodes  \\\n",
       "0          0.2           64.0           64.0           32.0          20.0   \n",
       "1          0.2           64.0           64.0           64.0          20.0   \n",
       "2          0.2           64.0          128.0           32.0          20.0   \n",
       "3          0.2           64.0          128.0           64.0          20.0   \n",
       "4          0.2          128.0           64.0           32.0          20.0   \n",
       "\n",
       "   epochs  batch_size  patience  \\\n",
       "0    25.0       500.0       5.0   \n",
       "1    25.0       500.0       5.0   \n",
       "2    25.0       500.0       5.0   \n",
       "3    25.0       500.0       5.0   \n",
       "4    25.0       500.0       5.0   \n",
       "\n",
       "                                          train_loss  \\\n",
       "0  [1.929674621391721, 1.4891164448469876, 1.3458...   \n",
       "1  [1.807594449785664, 1.4264270857600783, 1.2847...   \n",
       "2  [1.7993344856982916, 1.3989937976570672, 1.244...   \n",
       "3  [1.71986253795728, 1.3594488705780023, 1.21665...   \n",
       "4  [1.8578103558219237, 1.4254890923910857, 1.277...   \n",
       "\n",
       "                                           test_loss  \\\n",
       "0  [2.788091388872269, 2.7357079937062916, 2.7352...   \n",
       "1  [2.639851927085283, 2.8233394692688676, 2.8723...   \n",
       "2  [2.7024768084254043, 2.9492240141585824, 3.019...   \n",
       "3  [2.7534403096729094, 2.8281125191771324, 2.931...   \n",
       "4  [2.7237480103364793, 2.7852583254069057, 2.808...   \n",
       "\n",
       "                                           train_acc  \\\n",
       "0  [0.4183093846489654, 0.5335627086332418, 0.569...   \n",
       "1  [0.4459151326554479, 0.541307428624155, 0.5821...   \n",
       "2  [0.460541049611739, 0.5553126883259075, 0.6000...   \n",
       "3  [0.4694731150819956, 0.5630709044019566, 0.602...   \n",
       "4  [0.4363489153453843, 0.5463131596779426, 0.587...   \n",
       "\n",
       "                                            test_acc  \n",
       "0  [0.11555806102636433, 0.17790304550170277, 0.1...  \n",
       "1  [0.21736189578119097, 0.1386696750617333, 0.16...  \n",
       "2  [0.13957158876152492, 0.14768883820807954, 0.1...  \n",
       "3  [0.15727170218641134, 0.14441939342657803, 0.1...  \n",
       "4  [0.14791431621015239, 0.15693348356372475, 0.1...  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_df = pd.DataFrame(columns=['dropout_pct','dense_1_nodes','dense_2_nodes',\n",
    "                               'dense_3_nodes','target_nodes','epochs','batch_size',\n",
    "                               'patience'])\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "for dp in rnn_opt_params['dropout_pct']:\n",
    "    for d1 in rnn_opt_params['dense_1_nodes']:\n",
    "        for d2 in rnn_opt_params['dense_2_nodes']:\n",
    "            for d3 in rnn_opt_params['dense_3_nodes']:\n",
    "                for tn in rnn_opt_params['target_nodes']:\n",
    "                    for e in rnn_opt_params['epochs']:\n",
    "                        for bs in rnn_opt_params['batch_size']:\n",
    "                            for p in rnn_opt_params['patience']:\n",
    "                                temp_df = pd.Series(index=['dropout_pct','dense_1_nodes','dense_2_nodes',\n",
    "                                                                'dense_3_nodes','target_nodes','epochs',\n",
    "                                                                'batch_size','patience'])\n",
    "                                temp_df['dropout_pct'] = dp\n",
    "                                temp_df['dense_1_nodes'] = d1\n",
    "                                temp_df['dense_2_nodes'] = d2\n",
    "                                temp_df['dense_3_nodes'] = d3\n",
    "                                temp_df['target_nodes'] = tn\n",
    "                                temp_df['epochs'] = e\n",
    "                                temp_df['batch_size'] = bs\n",
    "                                temp_df['patience'] = p\n",
    "                                \n",
    "                                model = Sequential()\n",
    "                                model.add(Dense(X_train_sc.shape[0], input_dim=X_train_sc.shape[1], activation='relu'))\n",
    "                                model.add(Dropout(dp))\n",
    "                                model.add(Dense(d1, activation='relu'))\n",
    "                                model.add(Dropout(dp))\n",
    "                                model.add(Dense(d2, activation='relu'))\n",
    "                                model.add(Dropout(dp))\n",
    "                                model.add(Dense(d3, activation='relu'))\n",
    "                                model.add(Dense(y_test_d.shape[1], activation=None))\n",
    "                                model.add(Activation(tf.nn.softmax))\n",
    "                                model.compile(loss='categorical_crossentropy', \n",
    "                                                   optimizer='adam', metrics=['accuracy'])\n",
    "                                early_stop = EarlyStopping(monitor='val_acc', \n",
    "                                                                min_delta=0, patience=p, \n",
    "                                                                verbose=1, mode='auto')\n",
    "                                history = model.fit(X_train_sc, \n",
    "                                                    y_train_d, \n",
    "                                                    validation_data=(X_test_sc, y_test_d), \n",
    "                                                    epochs=e, \n",
    "                                                    batch_size=bs, \n",
    "                                                    callbacks=[early_stop])\n",
    "                                \n",
    "                                train_loss_list.append(history.history['loss'])\n",
    "                                test_loss_list.append(history.history['val_loss'])\n",
    "                                train_acc_list.append(history.history['acc'])\n",
    "                                test_acc_list.append(history.history['val_acc'])\n",
    "                                \n",
    "                                rnn_df = rnn_df.append(temp_df, ignore_index=True)\n",
    "\n",
    "rnn_df['train_loss'] = train_loss_list\n",
    "rnn_df['test_loss'] = test_loss_list\n",
    "rnn_df['train_acc'] = train_acc_list\n",
    "rnn_df['test_acc'] = test_acc_list\n",
    "rnn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dropout_pct</th>\n",
       "      <th>dense_1_nodes</th>\n",
       "      <th>dense_2_nodes</th>\n",
       "      <th>dense_3_nodes</th>\n",
       "      <th>target_nodes</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>patience</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.20</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.929674621391721, 1.4891164448469876, 1.3458...</td>\n",
       "      <td>[2.788091388872269, 2.7357079937062916, 2.7352...</td>\n",
       "      <td>[0.4183093846489654, 0.5335627086332418, 0.569...</td>\n",
       "      <td>[0.11555806102636433, 0.17790304550170277, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.20</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.807594449785664, 1.4264270857600783, 1.2847...</td>\n",
       "      <td>[2.639851927085283, 2.8233394692688676, 2.8723...</td>\n",
       "      <td>[0.4459151326554479, 0.541307428624155, 0.5821...</td>\n",
       "      <td>[0.21736189578119097, 0.1386696750617333, 0.16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.20</td>\n",
       "      <td>64.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.7993344856982916, 1.3989937976570672, 1.244...</td>\n",
       "      <td>[2.7024768084254043, 2.9492240141585824, 3.019...</td>\n",
       "      <td>[0.460541049611739, 0.5553126883259075, 0.6000...</td>\n",
       "      <td>[0.13957158876152492, 0.14768883820807954, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.20</td>\n",
       "      <td>64.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.71986253795728, 1.3594488705780023, 1.21665...</td>\n",
       "      <td>[2.7534403096729094, 2.8281125191771324, 2.931...</td>\n",
       "      <td>[0.4694731150819956, 0.5630709044019566, 0.602...</td>\n",
       "      <td>[0.15727170218641134, 0.14441939342657803, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.20</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.8578103558219237, 1.4254890923910857, 1.277...</td>\n",
       "      <td>[2.7237480103364793, 2.7852583254069057, 2.808...</td>\n",
       "      <td>[0.4363489153453843, 0.5463131596779426, 0.587...</td>\n",
       "      <td>[0.14791431621015239, 0.15693348356372475, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.20</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.708908194393183, 1.3549844702511096, 1.2218...</td>\n",
       "      <td>[2.738069522474906, 2.8378082673740495, 2.9564...</td>\n",
       "      <td>[0.4716993843018104, 0.5638129975216226, 0.604...</td>\n",
       "      <td>[0.15907553345828054, 0.15580608808564647, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.20</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.7006871572094457, 1.3177226839829237, 1.180...</td>\n",
       "      <td>[2.662666428586407, 2.817536994865271, 3.02822...</td>\n",
       "      <td>[0.47979491403419255, 0.5768198077072139, 0.61...</td>\n",
       "      <td>[0.15039458889577206, 0.18286358455212823, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.20</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.656575946618709, 1.3046723698134977, 1.1642...</td>\n",
       "      <td>[2.8214556730048903, 2.973141695493507, 2.9844...</td>\n",
       "      <td>[0.4838291853827149, 0.5766444046881922, 0.620...</td>\n",
       "      <td>[0.12468996625750618, 0.17655016868474988, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.20</td>\n",
       "      <td>256.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.7258722047803212, 1.3418520391691087, 1.192...</td>\n",
       "      <td>[2.664769741970092, 2.9089486238787114, 2.9129...</td>\n",
       "      <td>[0.47233353568170505, 0.5732847612363996, 0.61...</td>\n",
       "      <td>[0.18342728455043833, 0.17914317980389768, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.20</td>\n",
       "      <td>256.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.653877157138152, 1.3034362699518576, 1.1648...</td>\n",
       "      <td>[2.7122033260586296, 2.8799524264244245, 3.023...</td>\n",
       "      <td>[0.48786345460609026, 0.5825676327894342, 0.62...</td>\n",
       "      <td>[0.18545659288570807, 0.17125140943195935, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.20</td>\n",
       "      <td>256.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.7277202048636573, 1.3263481758644986, 1.174...</td>\n",
       "      <td>[2.7560128442598706, 2.722325649175461, 2.8709...</td>\n",
       "      <td>[0.4748161618127149, 0.5777777757093281, 0.620...</td>\n",
       "      <td>[0.16617812837811088, 0.18793686500749432, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.20</td>\n",
       "      <td>256.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.5830821341801726, 1.2398308042507458, 1.109...</td>\n",
       "      <td>[2.6746810533551515, 2.8602569264516218, 3.123...</td>\n",
       "      <td>[0.5047965993946287, 0.5993793404014666, 0.636...</td>\n",
       "      <td>[0.21262683382356518, 0.1847801577644223, 0.17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.25</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.9336174445813994, 1.535602472358119, 1.3906...</td>\n",
       "      <td>[2.7163990651338183, 2.7951953663766855, 2.756...</td>\n",
       "      <td>[0.41558388954013786, 0.5197193547434904, 0.55...</td>\n",
       "      <td>[0.11228861333434853, 0.17305524593004967, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.25</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.9333609014695243, 1.5284599337767293, 1.399...</td>\n",
       "      <td>[2.6596007072939005, 2.745630766923425, 2.8276...</td>\n",
       "      <td>[0.40998448319504444, 0.5108277666993384, 0.54...</td>\n",
       "      <td>[0.1580608814268078, 0.15028184916343215, 0.17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.25</td>\n",
       "      <td>64.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.8594601172663572, 1.4616523882416765, 1.315...</td>\n",
       "      <td>[2.6824462308958927, 2.7442520368999666, 2.832...</td>\n",
       "      <td>[0.4372664101902856, 0.5350738709604885, 0.577...</td>\n",
       "      <td>[0.16099210763444627, 0.15377677615423263, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.25</td>\n",
       "      <td>64.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.7532347543177025, 1.4155757677970866, 1.277...</td>\n",
       "      <td>[2.7305580279469623, 2.8435316040120777, 2.950...</td>\n",
       "      <td>[0.45392970371457847, 0.5491465973337757, 0.58...</td>\n",
       "      <td>[0.1399098073821116, 0.18049605578717512, 0.14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.25</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.8539496986745203, 1.4469730766857152, 1.300...</td>\n",
       "      <td>[2.7261333798878358, 2.654273995955337, 2.8859...</td>\n",
       "      <td>[0.4450651008460088, 0.544788505029302, 0.5843...</td>\n",
       "      <td>[0.14137542133063094, 0.1670800436607309, 0.17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.25</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.740689390625523, 1.4034176332493957, 1.2701...</td>\n",
       "      <td>[2.6990935756765064, 2.6892931114592438, 2.903...</td>\n",
       "      <td>[0.4648316810018764, 0.5528975232992306, 0.592...</td>\n",
       "      <td>[0.19109357155443682, 0.2014656165040391, 0.19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.25</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.7627708862275477, 1.3809633585377792, 1.241...</td>\n",
       "      <td>[2.8007456323608606, 2.953763317497365, 2.9610...</td>\n",
       "      <td>[0.4698239210868649, 0.5625311998850355, 0.599...</td>\n",
       "      <td>[0.1765501700665092, 0.16606538759842682, 0.18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.25</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.7303024082059024, 1.3719581807101215, 1.225...</td>\n",
       "      <td>[2.663593047097947, 2.8412177500891445, 2.9807...</td>\n",
       "      <td>[0.46713890457779805, 0.5619240383351372, 0.60...</td>\n",
       "      <td>[0.15952649472277136, 0.1612175871416498, 0.15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.25</td>\n",
       "      <td>256.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.7982682664730703, 1.3983633961665005, 1.241...</td>\n",
       "      <td>[2.6892852264434404, 2.7876953855576887, 2.890...</td>\n",
       "      <td>[0.4566956733900116, 0.5595898272287811, 0.602...</td>\n",
       "      <td>[0.17034949173073557, 0.17440811680470253, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.25</td>\n",
       "      <td>256.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.7237460048609012, 1.357706175121968, 1.2233...</td>\n",
       "      <td>[2.7243145917959115, 2.8448930800028287, 2.900...</td>\n",
       "      <td>[0.4668555611530023, 0.5668623095049933, 0.605...</td>\n",
       "      <td>[0.15118376392526753, 0.19188274836887972, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.25</td>\n",
       "      <td>256.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.7718516034426288, 1.36280243782623, 1.21735...</td>\n",
       "      <td>[2.8222811063489086, 2.8532550641353396, 2.987...</td>\n",
       "      <td>[0.46533090391678905, 0.5670781898762319, 0.60...</td>\n",
       "      <td>[0.12344983257794293, 0.14791432028718235, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.25</td>\n",
       "      <td>256.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.6191195239906875, 1.2984211186539794, 1.163...</td>\n",
       "      <td>[2.760512154427564, 2.9710780800128065, 3.0793...</td>\n",
       "      <td>[0.49701140122114307, 0.5803278687953595, 0.62...</td>\n",
       "      <td>[0.16414881451370397, 0.16110484871547145, 0.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dropout_pct  dense_1_nodes  dense_2_nodes  dense_3_nodes  target_nodes  \\\n",
       "0          0.20           64.0           64.0           32.0          20.0   \n",
       "1          0.20           64.0           64.0           64.0          20.0   \n",
       "2          0.20           64.0          128.0           32.0          20.0   \n",
       "3          0.20           64.0          128.0           64.0          20.0   \n",
       "4          0.20          128.0           64.0           32.0          20.0   \n",
       "5          0.20          128.0           64.0           64.0          20.0   \n",
       "6          0.20          128.0          128.0           32.0          20.0   \n",
       "7          0.20          128.0          128.0           64.0          20.0   \n",
       "8          0.20          256.0           64.0           32.0          20.0   \n",
       "9          0.20          256.0           64.0           64.0          20.0   \n",
       "10         0.20          256.0          128.0           32.0          20.0   \n",
       "11         0.20          256.0          128.0           64.0          20.0   \n",
       "12         0.25           64.0           64.0           32.0          20.0   \n",
       "13         0.25           64.0           64.0           64.0          20.0   \n",
       "14         0.25           64.0          128.0           32.0          20.0   \n",
       "15         0.25           64.0          128.0           64.0          20.0   \n",
       "16         0.25          128.0           64.0           32.0          20.0   \n",
       "17         0.25          128.0           64.0           64.0          20.0   \n",
       "18         0.25          128.0          128.0           32.0          20.0   \n",
       "19         0.25          128.0          128.0           64.0          20.0   \n",
       "20         0.25          256.0           64.0           32.0          20.0   \n",
       "21         0.25          256.0           64.0           64.0          20.0   \n",
       "22         0.25          256.0          128.0           32.0          20.0   \n",
       "23         0.25          256.0          128.0           64.0          20.0   \n",
       "\n",
       "    epochs  batch_size  patience  \\\n",
       "0     25.0       500.0       5.0   \n",
       "1     25.0       500.0       5.0   \n",
       "2     25.0       500.0       5.0   \n",
       "3     25.0       500.0       5.0   \n",
       "4     25.0       500.0       5.0   \n",
       "5     25.0       500.0       5.0   \n",
       "6     25.0       500.0       5.0   \n",
       "7     25.0       500.0       5.0   \n",
       "8     25.0       500.0       5.0   \n",
       "9     25.0       500.0       5.0   \n",
       "10    25.0       500.0       5.0   \n",
       "11    25.0       500.0       5.0   \n",
       "12    25.0       500.0       5.0   \n",
       "13    25.0       500.0       5.0   \n",
       "14    25.0       500.0       5.0   \n",
       "15    25.0       500.0       5.0   \n",
       "16    25.0       500.0       5.0   \n",
       "17    25.0       500.0       5.0   \n",
       "18    25.0       500.0       5.0   \n",
       "19    25.0       500.0       5.0   \n",
       "20    25.0       500.0       5.0   \n",
       "21    25.0       500.0       5.0   \n",
       "22    25.0       500.0       5.0   \n",
       "23    25.0       500.0       5.0   \n",
       "\n",
       "                                           train_loss  \\\n",
       "0   [1.929674621391721, 1.4891164448469876, 1.3458...   \n",
       "1   [1.807594449785664, 1.4264270857600783, 1.2847...   \n",
       "2   [1.7993344856982916, 1.3989937976570672, 1.244...   \n",
       "3   [1.71986253795728, 1.3594488705780023, 1.21665...   \n",
       "4   [1.8578103558219237, 1.4254890923910857, 1.277...   \n",
       "5   [1.708908194393183, 1.3549844702511096, 1.2218...   \n",
       "6   [1.7006871572094457, 1.3177226839829237, 1.180...   \n",
       "7   [1.656575946618709, 1.3046723698134977, 1.1642...   \n",
       "8   [1.7258722047803212, 1.3418520391691087, 1.192...   \n",
       "9   [1.653877157138152, 1.3034362699518576, 1.1648...   \n",
       "10  [1.7277202048636573, 1.3263481758644986, 1.174...   \n",
       "11  [1.5830821341801726, 1.2398308042507458, 1.109...   \n",
       "12  [1.9336174445813994, 1.535602472358119, 1.3906...   \n",
       "13  [1.9333609014695243, 1.5284599337767293, 1.399...   \n",
       "14  [1.8594601172663572, 1.4616523882416765, 1.315...   \n",
       "15  [1.7532347543177025, 1.4155757677970866, 1.277...   \n",
       "16  [1.8539496986745203, 1.4469730766857152, 1.300...   \n",
       "17  [1.740689390625523, 1.4034176332493957, 1.2701...   \n",
       "18  [1.7627708862275477, 1.3809633585377792, 1.241...   \n",
       "19  [1.7303024082059024, 1.3719581807101215, 1.225...   \n",
       "20  [1.7982682664730703, 1.3983633961665005, 1.241...   \n",
       "21  [1.7237460048609012, 1.357706175121968, 1.2233...   \n",
       "22  [1.7718516034426288, 1.36280243782623, 1.21735...   \n",
       "23  [1.6191195239906875, 1.2984211186539794, 1.163...   \n",
       "\n",
       "                                            test_loss  \\\n",
       "0   [2.788091388872269, 2.7357079937062916, 2.7352...   \n",
       "1   [2.639851927085283, 2.8233394692688676, 2.8723...   \n",
       "2   [2.7024768084254043, 2.9492240141585824, 3.019...   \n",
       "3   [2.7534403096729094, 2.8281125191771324, 2.931...   \n",
       "4   [2.7237480103364793, 2.7852583254069057, 2.808...   \n",
       "5   [2.738069522474906, 2.8378082673740495, 2.9564...   \n",
       "6   [2.662666428586407, 2.817536994865271, 3.02822...   \n",
       "7   [2.8214556730048903, 2.973141695493507, 2.9844...   \n",
       "8   [2.664769741970092, 2.9089486238787114, 2.9129...   \n",
       "9   [2.7122033260586296, 2.8799524264244245, 3.023...   \n",
       "10  [2.7560128442598706, 2.722325649175461, 2.8709...   \n",
       "11  [2.6746810533551515, 2.8602569264516218, 3.123...   \n",
       "12  [2.7163990651338183, 2.7951953663766855, 2.756...   \n",
       "13  [2.6596007072939005, 2.745630766923425, 2.8276...   \n",
       "14  [2.6824462308958927, 2.7442520368999666, 2.832...   \n",
       "15  [2.7305580279469623, 2.8435316040120777, 2.950...   \n",
       "16  [2.7261333798878358, 2.654273995955337, 2.8859...   \n",
       "17  [2.6990935756765064, 2.6892931114592438, 2.903...   \n",
       "18  [2.8007456323608606, 2.953763317497365, 2.9610...   \n",
       "19  [2.663593047097947, 2.8412177500891445, 2.9807...   \n",
       "20  [2.6892852264434404, 2.7876953855576887, 2.890...   \n",
       "21  [2.7243145917959115, 2.8448930800028287, 2.900...   \n",
       "22  [2.8222811063489086, 2.8532550641353396, 2.987...   \n",
       "23  [2.760512154427564, 2.9710780800128065, 3.0793...   \n",
       "\n",
       "                                            train_acc  \\\n",
       "0   [0.4183093846489654, 0.5335627086332418, 0.569...   \n",
       "1   [0.4459151326554479, 0.541307428624155, 0.5821...   \n",
       "2   [0.460541049611739, 0.5553126883259075, 0.6000...   \n",
       "3   [0.4694731150819956, 0.5630709044019566, 0.602...   \n",
       "4   [0.4363489153453843, 0.5463131596779426, 0.587...   \n",
       "5   [0.4716993843018104, 0.5638129975216226, 0.604...   \n",
       "6   [0.47979491403419255, 0.5768198077072139, 0.61...   \n",
       "7   [0.4838291853827149, 0.5766444046881922, 0.620...   \n",
       "8   [0.47233353568170505, 0.5732847612363996, 0.61...   \n",
       "9   [0.48786345460609026, 0.5825676327894342, 0.62...   \n",
       "10  [0.4748161618127149, 0.5777777757093281, 0.620...   \n",
       "11  [0.5047965993946287, 0.5993793404014666, 0.636...   \n",
       "12  [0.41558388954013786, 0.5197193547434904, 0.55...   \n",
       "13  [0.40998448319504444, 0.5108277666993384, 0.54...   \n",
       "14  [0.4372664101902856, 0.5350738709604885, 0.577...   \n",
       "15  [0.45392970371457847, 0.5491465973337757, 0.58...   \n",
       "16  [0.4450651008460088, 0.544788505029302, 0.5843...   \n",
       "17  [0.4648316810018764, 0.5528975232992306, 0.592...   \n",
       "18  [0.4698239210868649, 0.5625311998850355, 0.599...   \n",
       "19  [0.46713890457779805, 0.5619240383351372, 0.60...   \n",
       "20  [0.4566956733900116, 0.5595898272287811, 0.602...   \n",
       "21  [0.4668555611530023, 0.5668623095049933, 0.605...   \n",
       "22  [0.46533090391678905, 0.5670781898762319, 0.60...   \n",
       "23  [0.49701140122114307, 0.5803278687953595, 0.62...   \n",
       "\n",
       "                                             test_acc  \n",
       "0   [0.11555806102636433, 0.17790304550170277, 0.1...  \n",
       "1   [0.21736189578119097, 0.1386696750617333, 0.16...  \n",
       "2   [0.13957158876152492, 0.14768883820807954, 0.1...  \n",
       "3   [0.15727170218641134, 0.14441939342657803, 0.1...  \n",
       "4   [0.14791431621015239, 0.15693348356372475, 0.1...  \n",
       "5   [0.15907553345828054, 0.15580608808564647, 0.1...  \n",
       "6   [0.15039458889577206, 0.18286358455212823, 0.1...  \n",
       "7   [0.12468996625750618, 0.17655016868474988, 0.1...  \n",
       "8   [0.18342728455043833, 0.17914317980389768, 0.2...  \n",
       "9   [0.18545659288570807, 0.17125140943195935, 0.1...  \n",
       "10  [0.16617812837811088, 0.18793686500749432, 0.1...  \n",
       "11  [0.21262683382356518, 0.1847801577644223, 0.17...  \n",
       "12  [0.11228861333434853, 0.17305524593004967, 0.1...  \n",
       "13  [0.1580608814268078, 0.15028184916343215, 0.17...  \n",
       "14  [0.16099210763444627, 0.15377677615423263, 0.1...  \n",
       "15  [0.1399098073821116, 0.18049605578717512, 0.14...  \n",
       "16  [0.14137542133063094, 0.1670800436607309, 0.17...  \n",
       "17  [0.19109357155443682, 0.2014656165040391, 0.19...  \n",
       "18  [0.1765501700665092, 0.16606538759842682, 0.18...  \n",
       "19  [0.15952649472277136, 0.1612175871416498, 0.15...  \n",
       "20  [0.17034949173073557, 0.17440811680470253, 0.1...  \n",
       "21  [0.15118376392526753, 0.19188274836887972, 0.1...  \n",
       "22  [0.12344983257794293, 0.14791432028718235, 0.1...  \n",
       "23  [0.16414881451370397, 0.16110484871547145, 0.1...  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_df.to_csv('../models/rnn_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load RNN Results DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dropout_pct</th>\n",
       "      <th>dense_1_nodes</th>\n",
       "      <th>dense_2_nodes</th>\n",
       "      <th>dense_3_nodes</th>\n",
       "      <th>target_nodes</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>patience</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.929674621391721, 1.4891164448469876, 1.3458...</td>\n",
       "      <td>[2.788091388872269, 2.7357079937062916, 2.7352...</td>\n",
       "      <td>[0.4183093846489654, 0.5335627086332418, 0.569...</td>\n",
       "      <td>[0.11555806102636433, 0.17790304550170277, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.807594449785664, 1.4264270857600783, 1.2847...</td>\n",
       "      <td>[2.639851927085283, 2.8233394692688676, 2.8723...</td>\n",
       "      <td>[0.4459151326554479, 0.541307428624155, 0.5821...</td>\n",
       "      <td>[0.21736189578119097, 0.1386696750617333, 0.16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>64.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.7993344856982916, 1.3989937976570672, 1.244...</td>\n",
       "      <td>[2.7024768084254043, 2.9492240141585824, 3.019...</td>\n",
       "      <td>[0.460541049611739, 0.5553126883259075, 0.6000...</td>\n",
       "      <td>[0.13957158876152492, 0.14768883820807954, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2</td>\n",
       "      <td>64.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.71986253795728, 1.3594488705780023, 1.21665...</td>\n",
       "      <td>[2.7534403096729094, 2.8281125191771324, 2.931...</td>\n",
       "      <td>[0.4694731150819956, 0.5630709044019566, 0.602...</td>\n",
       "      <td>[0.15727170218641134, 0.14441939342657803, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2</td>\n",
       "      <td>128.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[1.8578103558219237, 1.4254890923910857, 1.277...</td>\n",
       "      <td>[2.7237480103364793, 2.7852583254069057, 2.808...</td>\n",
       "      <td>[0.4363489153453843, 0.5463131596779426, 0.587...</td>\n",
       "      <td>[0.14791431621015239, 0.15693348356372475, 0.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dropout_pct  dense_1_nodes  dense_2_nodes  dense_3_nodes  target_nodes  \\\n",
       "0          0.2           64.0           64.0           32.0          20.0   \n",
       "1          0.2           64.0           64.0           64.0          20.0   \n",
       "2          0.2           64.0          128.0           32.0          20.0   \n",
       "3          0.2           64.0          128.0           64.0          20.0   \n",
       "4          0.2          128.0           64.0           32.0          20.0   \n",
       "\n",
       "   epochs  batch_size  patience  \\\n",
       "0    25.0       500.0       5.0   \n",
       "1    25.0       500.0       5.0   \n",
       "2    25.0       500.0       5.0   \n",
       "3    25.0       500.0       5.0   \n",
       "4    25.0       500.0       5.0   \n",
       "\n",
       "                                          train_loss  \\\n",
       "0  [1.929674621391721, 1.4891164448469876, 1.3458...   \n",
       "1  [1.807594449785664, 1.4264270857600783, 1.2847...   \n",
       "2  [1.7993344856982916, 1.3989937976570672, 1.244...   \n",
       "3  [1.71986253795728, 1.3594488705780023, 1.21665...   \n",
       "4  [1.8578103558219237, 1.4254890923910857, 1.277...   \n",
       "\n",
       "                                           test_loss  \\\n",
       "0  [2.788091388872269, 2.7357079937062916, 2.7352...   \n",
       "1  [2.639851927085283, 2.8233394692688676, 2.8723...   \n",
       "2  [2.7024768084254043, 2.9492240141585824, 3.019...   \n",
       "3  [2.7534403096729094, 2.8281125191771324, 2.931...   \n",
       "4  [2.7237480103364793, 2.7852583254069057, 2.808...   \n",
       "\n",
       "                                           train_acc  \\\n",
       "0  [0.4183093846489654, 0.5335627086332418, 0.569...   \n",
       "1  [0.4459151326554479, 0.541307428624155, 0.5821...   \n",
       "2  [0.460541049611739, 0.5553126883259075, 0.6000...   \n",
       "3  [0.4694731150819956, 0.5630709044019566, 0.602...   \n",
       "4  [0.4363489153453843, 0.5463131596779426, 0.587...   \n",
       "\n",
       "                                            test_acc  \n",
       "0  [0.11555806102636433, 0.17790304550170277, 0.1...  \n",
       "1  [0.21736189578119097, 0.1386696750617333, 0.16...  \n",
       "2  [0.13957158876152492, 0.14768883820807954, 0.1...  \n",
       "3  [0.15727170218641134, 0.14441939342657803, 0.1...  \n",
       "4  [0.14791431621015239, 0.15693348356372475, 0.1...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_df = pd.read_csv('../models/rnn_df.csv')\n",
    "rnn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Accuracy Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.644208\n",
       "1     0.579964\n",
       "2     0.671449\n",
       "3     0.727902\n",
       "4     0.729145\n",
       "5     0.649090\n",
       "6     0.708005\n",
       "7     0.696900\n",
       "8     0.644397\n",
       "9     0.623079\n",
       "10    0.676727\n",
       "11    0.639821\n",
       "12    0.615038\n",
       "13    0.566056\n",
       "14    0.573157\n",
       "15    0.632383\n",
       "16    0.634693\n",
       "17    0.605860\n",
       "18    0.656117\n",
       "19    0.693569\n",
       "20    0.678472\n",
       "21    0.618855\n",
       "22    0.650694\n",
       "23    0.676460\n",
       "Name: train_acc, dtype: float64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_df['train_acc'].map(lambda x: np.mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.169673\n",
       "1     0.167306\n",
       "2     0.169391\n",
       "3     0.165947\n",
       "4     0.167424\n",
       "5     0.166128\n",
       "6     0.177322\n",
       "7     0.166413\n",
       "8     0.180595\n",
       "9     0.168264\n",
       "10    0.178557\n",
       "11    0.182826\n",
       "12    0.164651\n",
       "13    0.164501\n",
       "14    0.157272\n",
       "15    0.161161\n",
       "16    0.163991\n",
       "17    0.189258\n",
       "18    0.182548\n",
       "19    0.174899\n",
       "20    0.176560\n",
       "21    0.173796\n",
       "22    0.156808\n",
       "23    0.175400\n",
       "Name: test_acc, dtype: float64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_df['test_acc'].map(lambda x: np.mean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Highest Mean Accuracy Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model number\n",
    "mn = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0.5047965993946287, 0.5993793404014666, 0.6366727388573427, 0.6718882785822661, 0.7002765946137972, 0.7259124358607667]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_df.loc[mn,'train_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0.21262683382356518, 0.1847801577644223, 0.17587373358656347, 0.17711386601981344, 0.1789177007502808, 0.16764374002142313]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_df.loc[mn,'test_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dropout_pct                                                    0.2\n",
       "dense_1_nodes                                                  256\n",
       "dense_2_nodes                                                  128\n",
       "dense_3_nodes                                                   64\n",
       "target_nodes                                                    20\n",
       "epochs                                                          25\n",
       "batch_size                                                     500\n",
       "patience                                                         5\n",
       "train_loss       [1.5830821341801726, 1.2398308042507458, 1.109...\n",
       "test_loss        [2.6746810533551515, 2.8602569264516218, 3.123...\n",
       "train_acc        [0.5047965993946287, 0.5993793404014666, 0.636...\n",
       "test_acc         [0.21262683382356518, 0.1847801577644223, 0.17...\n",
       "Name: 11, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_df.loc[mn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = rnn_df.loc[mn,'dropout_pct']\n",
    "d1 = rnn_df.loc[mn,'dense_1_nodes'].astype(int)\n",
    "d2 = rnn_df.loc[mn,'dense_2_nodes'].astype(int)\n",
    "d3 = rnn_df.loc[mn,'dense_3_nodes'].astype(int)\n",
    "tn = rnn_df.loc[mn,'target_nodes'].astype(int)\n",
    "e = 3 # rnn_df.loc[mn,'epochs'].astype(int)\n",
    "bs = rnn_df.loc[mn,'batch_size'].astype(int)\n",
    "p = 3 # rnn_df.loc[mn,'patience'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2, 256, 128, 64, 20, 3, 500, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp, d1, d2, d3, tn, e, bs, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 74115 samples, validate on 8870 samples\n",
      "Epoch 1/3\n",
      "74115/74115 [==============================] - 19s 251us/step - loss: 1.6154 - acc: 0.4992 - val_loss: 2.8453 - val_acc: 0.1508\n",
      "Epoch 2/3\n",
      "74115/74115 [==============================] - 16s 222us/step - loss: 1.2685 - acc: 0.5911 - val_loss: 2.8731 - val_acc: 0.1785\n",
      "Epoch 3/3\n",
      "74115/74115 [==============================] - 16s 222us/step - loss: 1.1146 - acc: 0.6349 - val_loss: 2.9941 - val_acc: 0.2082\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(X_train_sc.shape[0], input_dim=X_train_sc.shape[1], activation='relu'))\n",
    "model.add(Dropout(dp))\n",
    "model.add(Dense(d1, activation='relu'))\n",
    "model.add(Dropout(dp))\n",
    "model.add(Dense(d2, activation='relu'))\n",
    "model.add(Dropout(dp))\n",
    "model.add(Dense(d3, activation='relu'))\n",
    "model.add(Dense(y_test_d.shape[1], activation=None))\n",
    "model.add(Activation(tf.nn.softmax))\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "                   optimizer='adam', metrics=['accuracy'])\n",
    "early_stop = EarlyStopping(monitor='val_acc', \n",
    "                                min_delta=0, patience=p, \n",
    "                                verbose=1, mode='auto')\n",
    "history = model.fit(X_train_sc, \n",
    "                    y_train_d, \n",
    "                    validation_data=(X_test_sc, y_test_d), \n",
    "                    epochs=e, \n",
    "                    batch_size=bs, \n",
    "                    callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2d786187b8>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "plt.plot(train_loss, label='Training loss')\n",
    "plt.plot(test_loss, label='Testing loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2d785487b8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_acc = history.history['acc']\n",
    "test_acc = history.history['val_acc']\n",
    "plt.plot(train_acc, label='Training Accuracy')\n",
    "plt.plot(test_acc, label='Testing Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Final Keras FFRNN to Disk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/rnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorizing Text Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using medium English library which does not include vectors\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_to_df(par_file, nlp=nlp, sw=['the','a','but','like','for'], to_stem=False):\n",
    "#     # Run spaCy process on each paragraph and store docs in list\n",
    "#     print('nlp of paragraphs...')\n",
    "#     par_nlp = []\n",
    "#     for par in tqdm(par_file.paragraph):\n",
    "#         par_nlp.append(nlp(par))\n",
    "    \n",
    "#     # Store paragraph lemma from spaCy docs\n",
    "#     print('nlp lemmatizing, part-of-speech, stopwords...')\n",
    "#     par_lemma = []\n",
    "#     for par in tqdm(par_nlp):\n",
    "#         par_lemma.append([token.lemma_ for token in par     # List comprehension\n",
    "#                            if token.lemma_ != '-PRON-'           # Pronouns are excluded\n",
    "#                            and token.pos_ != 'PUNCT'             # Punctionation is excluded\n",
    "#                            and token.is_alpha                    # Numbers are excluded\n",
    "#                            and not token.is_stop                 # Stop words are excluded\n",
    "#                           and len(token.lemma_) > 1])               \n",
    "    \n",
    "#     # Stem lemma with NLTK PorterStemmer and remove stop words\n",
    "#     print('additional stopwords...')\n",
    "#     if to_stem: ps = PorterStemmer()\n",
    "#     par_lemma_sw = []\n",
    "#     for vec_list in tqdm(par_lemma):    \n",
    "#         update_list = []\n",
    "#         for token in vec_list:\n",
    "#             if token in sw: continue\n",
    "#             if to_stem: update_list.append(ps.stem(token))\n",
    "#             else: update_list.append(token)\n",
    "#         par_lemma_sw.append(update_list)\n",
    "    \n",
    "#     # Run spaCy on each sentence doc: Text\n",
    "#     print('saving sentence text...')\n",
    "#     sent_text = []\n",
    "#     for par in tqdm(par_nlp):\n",
    "#         sent_list = []\n",
    "#         for s in par.sents:\n",
    "#             sent_list.append(s.text)\n",
    "#         sent_text.append(sent_list)\n",
    "    \n",
    "#     # Run spaCy on each sentence doc: NLP\n",
    "#     print('nlp of sentences...')\n",
    "#     sent_nlp = []\n",
    "#     for par in tqdm(par_nlp):\n",
    "#         sent_list = []\n",
    "#         for s in par.sents:\n",
    "#             sent_list.append(nlp(s.text))\n",
    "#         sent_nlp.append(sent_list)\n",
    "    \n",
    "#     # Store lemma from spaCy docs\n",
    "#     print('nlp lemmatizing, part-of-speech, stopwords...')\n",
    "#     sent_lemma = []\n",
    "#     for par in tqdm(sent_nlp):\n",
    "#         for sent in par:\n",
    "#             sent_lemma.append([token.lemma_ for token in sent     # List comprehension\n",
    "#                                if token.lemma_ != '-PRON-'           # Pronouns are excluded\n",
    "#                                and token.pos_ != 'PUNCT'             # Punctionation is excluded\n",
    "#                                and token.is_alpha                    # Numbers are excluded\n",
    "#                                and not token.is_stop                 # Stop words are excluded\n",
    "#                               and len(token.lemma_) > 1])               \n",
    "    \n",
    "#     # Stem lemma with NLTK PorterStemmer and remove stop words\n",
    "#     print('additional stopwords...')\n",
    "#     if to_stem: ps = PorterStemmer()\n",
    "#     sent_lemma_sw = []\n",
    "#     for vec_list in tqdm(sent_lemma):    \n",
    "#         update_list = []\n",
    "#         for token in vec_list:\n",
    "#             if token in sw: continue\n",
    "#             if to_stem: update_list.append(ps.stem(token))\n",
    "#             else: update_list.append(token)\n",
    "#         sent_lemma_sw.append(update_list)\n",
    "    \n",
    "#     print('constructing dataframe...')\n",
    "#     nlp_df = pd.DataFrame(columns=['author','work','a_num','w_num','p_num','s_num',\n",
    "#                                     'sent_text','sent_lemma','par_text','par_lemma'])\n",
    "\n",
    "#     a_num = 0\n",
    "#     w_num = 0\n",
    "#     p_num = 0\n",
    "\n",
    "#     for p, sents_in_par in enumerate(tqdm(sent_text)):\n",
    "#         for s, sent in enumerate(sents_in_par):\n",
    "#             nlp_df = nlp_df.append({'author':par_file.loc[p, 'author'], \n",
    "#                                     'work':par_file.loc[p, 'work'], \n",
    "#                                     'a_num':a_num,\n",
    "#                                     'w_num':w_num,\n",
    "#                                     'p_num':p_num,\n",
    "#                                     's_num':s,\n",
    "#                                     'sent_text':sent,\n",
    "#                                     'par_text':par_file.loc[p, 'paragraph'],\n",
    "#                                     'par_lemma':par_lemma_sw[p]\n",
    "#                                     }, ignore_index=True)\n",
    "#         p_num += 1\n",
    "#         if p == par_file.shape[0]-1: continue\n",
    "#         if par_file.loc[p,'work'] != par_file.loc[p+1,'work']:\n",
    "#             p_num = 0\n",
    "#             w_num += 1\n",
    "#             if par_file.loc[p,'author'] != par_file.loc[p+1,'author']:\n",
    "#                 a_num += 1\n",
    "\n",
    "#     nlp_df['sent_lemma'] = sent_lemma_sw\n",
    "#     print('complete')\n",
    "#     return nlp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_to_df(par_file, nlp=nlp, sw=['the','a','but','like','for'], to_stem=False):\n",
    "    # Run spaCy process on each paragraph and store docs in list\n",
    "    print('1/8: nlp of paragraphs...')\n",
    "    par_nlp = []\n",
    "    for par in tqdm(par_file.paragraph):\n",
    "        par_nlp.append(nlp(par))\n",
    "    \n",
    "    # Store paragraph lemma from spaCy docs\n",
    "    print('2/8: nlp lemmatizing, part-of-speech, stopwords...')\n",
    "    par_lemma = []\n",
    "    for par in tqdm(par_nlp):\n",
    "        par_lemma.append([token.lemma_ for token in par     # List comprehension\n",
    "                           if token.lemma_ != '-PRON-'           # Pronouns are excluded\n",
    "                           and token.pos_ != 'PUNCT'             # Punctuation is excluded\n",
    "                           and token.is_alpha                    # Numbers are excluded\n",
    "                           and not token.is_stop                 # Stop words are excluded\n",
    "                          and len(token.lemma_) > 1])\n",
    "    par_lemma = [[pl[i].lower() for i in range(len(pl))] for pl in par_lemma]\n",
    "    \n",
    "    # Stem lemma with NLTK PorterStemmer and remove stop words\n",
    "    print('3/8: additional stopwords...')\n",
    "    if to_stem: ps = PorterStemmer()\n",
    "    par_lemma_sw = []\n",
    "    for vec_list in tqdm(par_lemma):    \n",
    "        update_list = []\n",
    "        for token in vec_list:\n",
    "            if token in sw: continue\n",
    "            if to_stem: update_list.append(ps.stem(token))\n",
    "            else: update_list.append(token)\n",
    "        par_lemma_sw.append(update_list)\n",
    "    \n",
    "    # Run spaCy on each sentence doc: Text\n",
    "    print('4/8: saving sentence text...')\n",
    "    sent_text = []\n",
    "    for par in tqdm(par_nlp):\n",
    "        sent_list = []\n",
    "        for s in par.sents:\n",
    "            sent_list.append(s.text)\n",
    "        sent_text.append(sent_list)\n",
    "    \n",
    "    # Run spaCy on each sentence doc: NLP\n",
    "    print('5/8: nlp of sentences...')\n",
    "    sent_nlp = []\n",
    "    for par in tqdm(par_nlp):\n",
    "        sent_list = []\n",
    "        for s in par.sents:\n",
    "            sent_list.append(nlp(s.text))\n",
    "        sent_nlp.append(sent_list)\n",
    "    \n",
    "    # Store lemma from spaCy docs\n",
    "    print('6/8: nlp lemmatizing, part-of-speech, stopwords...')\n",
    "    sent_lemma = []\n",
    "    for par in tqdm(sent_nlp):\n",
    "        for sent in par:\n",
    "            sent_lemma.append([token.lemma_ for token in sent     # List comprehension\n",
    "                               if token.lemma_ != '-PRON-'           # Pronouns are excluded\n",
    "                               and token.pos_ != 'PUNCT'             # Punctuation is excluded\n",
    "                               and token.is_alpha                    # Numbers are excluded\n",
    "                               and not token.is_stop                 # Stop words are excluded\n",
    "                              and len(token.lemma_) > 1])\n",
    "    sent_lemma = [[sl[j].lower() for j in range(len(sl))] for sl in sent_lemma]\n",
    "    \n",
    "    # Stem lemma with NLTK PorterStemmer and remove stop words\n",
    "    print('7/8: additional stopwords...')\n",
    "    if to_stem: ps = PorterStemmer()\n",
    "    sent_lemma_sw = []\n",
    "    for vec_list in tqdm(sent_lemma):    \n",
    "        update_list = []\n",
    "        for token in vec_list:\n",
    "            if token in sw: continue\n",
    "            if to_stem: update_list.append(ps.stem(token))\n",
    "            else: update_list.append(token)\n",
    "        sent_lemma_sw.append(update_list)\n",
    "    \n",
    "    print('8/8: constructing dataframe...')\n",
    "    nlp_df = pd.DataFrame(columns=['author','work','a_num','w_num','p_num','s_num',\n",
    "                                    'sent_text','sent_lemma','par_text','par_lemma'])\n",
    "\n",
    "    a_num = 0\n",
    "    w_num = 0\n",
    "    p_num = 0\n",
    "\n",
    "    for p, sents_in_par in enumerate(tqdm(sent_text)):\n",
    "        for s, sent in enumerate(sents_in_par):\n",
    "            nlp_df = nlp_df.append({'author':par_file.loc[p, 'author'], \n",
    "                                    'work':par_file.loc[p, 'work'], \n",
    "                                    'a_num':a_num,\n",
    "                                    'w_num':w_num,\n",
    "                                    'p_num':p_num,\n",
    "                                    's_num':s,\n",
    "                                    'sent_text':sent,\n",
    "                                    'par_text':par_file.loc[p, 'paragraph'],\n",
    "                                    'par_lemma':par_lemma_sw[p]\n",
    "                                    }, ignore_index=True)\n",
    "        p_num += 1\n",
    "        if p == par_file.shape[0]-1: continue\n",
    "        if par_file.loc[p,'work'] != par_file.loc[p+1,'work']:\n",
    "            p_num = 0\n",
    "            w_num += 1\n",
    "            if par_file.loc[p,'author'] != par_file.loc[p+1,'author']:\n",
    "                a_num += 1\n",
    "\n",
    "    nlp_df['sent_lemma'] = sent_lemma_sw\n",
    "    print('complete')\n",
    "    return nlp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_factorize(lda_df, model=model_full, ss=ss):\n",
    "    lda_sc = ss.transform(lda_df.values)    \n",
    "    preds = model.predict_proba(lda_sc)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_nlp_factors(preds, raw_text, doc_number=0, thresh=0.01, target_authors=None):\n",
    "    print('-'*30)\n",
    "    print('Doc #:', doc_number)\n",
    "    if target_authors: print('Target Author:', target_authors[doc_number])\n",
    "    print('Text:\\n', raw_text[doc_number])\n",
    "    print('\\nPhilosophical Factors:\\n')\n",
    "    \n",
    "    result_list = [(text_data.Author.unique()[i], preds[doc_number][i]) for i in range(len(text_data.Author.unique()))]\n",
    "    result_list.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    \n",
    "    for j in range(len(text_data.Author.unique())):\n",
    "        n_sp = 35 - len(result_list[j][0])\n",
    "        if result_list[j][1] >= thresh:\n",
    "            print('\\t{}{}{}'.format(result_list[j][0],str(' '*n_sp),str(round(result_list[j][1], 3))))\n",
    "    print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "babbage = \"\"\"\n",
    "There are few circumstances which so strongly distinguish the philosopher, \n",
    "as the calmness with which he can reply to criticisms he may think...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue to Notebook 5: Factorizing Unseen Text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
